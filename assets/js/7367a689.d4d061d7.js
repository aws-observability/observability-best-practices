"use strict";(globalThis.webpackChunkobservability_best_practices=globalThis.webpackChunkobservability_best_practices||[]).push([[2784],{6233(e,t,o){o.r(t),o.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"guides/containers/oss/eks/resource-optimization","title":"Resource Optimization best practices for Kubernetes workloads","description":"Kubernetes adoption continues to accelerate, as many move to microservice based architectures. A lot of the initial focus was on designing and building new cloud native architectures to support the applications. As environments grow, we are starting to see the focus to optimize resource allocation from customers. Resource optimization is the second most important  question operations team ask for after security.","source":"@site/docs/guides/containers/oss/eks/resource-optimization.md","sourceDirName":"guides/containers/oss/eks","slug":"/guides/containers/oss/eks/resource-optimization","permalink":"/observability-best-practices/guides/containers/oss/eks/resource-optimization","draft":false,"unlisted":false,"editUrl":"https://github.com/aws-observability/observability-best-practices/blob/main/docusaurus/docs/guides/containers/oss/eks/resource-optimization.md","tags":[],"version":"current","frontMatter":{}}');var i=o(74848),n=o(28453);const r={},a="Resource Optimization best practices for Kubernetes workloads",c={},d=[{value:"Reasons for Right-sizing applications on Kubernetes",id:"reasons-for-right-sizing-applications-on-kubernetes",level:2},{value:"Requests and Limits",id:"requests-and-limits",level:2},{value:"Recommendations",id:"recommendations",level:2},{value:"Vertical Pod Autoscaler (VPA)",id:"vertical-pod-autoscaler-vpa",level:3},{value:"Goldilocks",id:"goldilocks",level:3},{value:"Understand throttling using cAdvisor metric and configuring the resource appropriately",id:"understand-throttling-using-cadvisor-metric-and-configuring-the-resource-appropriately",level:3}];function l(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,n.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"resource-optimization-best-practices-for-kubernetes-workloads",children:"Resource Optimization best practices for Kubernetes workloads"})}),"\n",(0,i.jsx)(t.p,{children:"Kubernetes adoption continues to accelerate, as many move to microservice based architectures. A lot of the initial focus was on designing and building new cloud native architectures to support the applications. As environments grow, we are starting to see the focus to optimize resource allocation from customers. Resource optimization is the second most important  question operations team ask for after security.\nLet's talk about guidance on how to optimize resource allocation and right-size applications on Kubernetes environments. This includes applications running on Amazon EKS deployed with managed node groups, self-managed node groups, and AWS Fargate."}),"\n",(0,i.jsx)(t.h2,{id:"reasons-for-right-sizing-applications-on-kubernetes",children:"Reasons for Right-sizing applications on Kubernetes"}),"\n",(0,i.jsx)(t.p,{children:"In Kubernetes, resource right-sizing is done through setting resource specifications on applications. These settings directly impact:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Performance \u2014 Kubernetes applications will arbitrarily compete for resources without proper resource specifications, This can adversely impact application performance."}),"\n",(0,i.jsx)(t.li,{children:"Cost Optimization \u2014 Applications deployed with oversized resource specifications will result in increased costs and under utilized infrastructure."}),"\n",(0,i.jsx)(t.li,{children:"Autoscaling \u2014 The Kubernetes Cluster Autoscaler and Horizontal Pod Autoscaling require resource specifications to function."}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["The most common resource specifications in Kubernetes are for ",(0,i.jsx)(t.a,{href:"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits",children:"CPU and memory requests and limits"}),"."]}),"\n",(0,i.jsx)(t.h2,{id:"requests-and-limits",children:"Requests and Limits"}),"\n",(0,i.jsxs)(t.p,{children:["Containerized applications are deployed on Kubernetes as Pods. CPU and memory requests and limits are an optional part of the Pod definition. CPU is specified in units of ",(0,i.jsx)(t.a,{href:"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu",children:"Kubernetes CPUs"})," while memory is specified in bytes, usually as ",(0,i.jsx)(t.a,{href:"https://simple.wikipedia.org/wiki/Mebibyte",children:"mebibytes (Mi)"}),"."]}),"\n",(0,i.jsx)(t.p,{children:"Request and limits each serve different functions in Kubernetes and impact scheduling and resource enforcement differently."}),"\n",(0,i.jsx)(t.h2,{id:"recommendations",children:"Recommendations"}),"\n",(0,i.jsx)(t.p,{children:'An application owner needs to choose the "right" values  for their CPU and memory resource requests. An ideal way  is to load test the application in a development environment and measure resource usage using observability tooling. While that might make sense for your organization\u2019s most critical applications, it\u2019s likely not feasible for every containerized application deployed in your cluster. Let\'s talk about the tools that can help us optimize and right-size the workloads:'}),"\n",(0,i.jsx)(t.h3,{id:"vertical-pod-autoscaler-vpa",children:"Vertical Pod Autoscaler (VPA)"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.a,{href:"https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler",children:"VPA"})," is Kubernetes sub-project owned by the Autoscaling special interest group (SIG). It\u2019s designed to automatically set Pod requests based on observed application performance. VPA collects resource usage using the ",(0,i.jsx)(t.a,{href:"https://github.com/kubernetes-sigs/metrics-server",children:"Kubernetes Metric Server"})," by default but can be optionally configured to use Prometheus as a data source.\nVPA has a recommendation engine  that measures application performance and makes sizing recommendations. The VPA recommendation engine can be deployed stand-alone so VPA will not perform any autoscaling actions. It\u2019s configured by creating a VerticalPodAutoscaler custom resource for each application and VPA updates the object\u2019s status field with resource sizing recommendations.\nCreating VerticalPodAutoscaler objects for every application in your cluster and trying to read and interpret the JSON results is challenging at scale. ",(0,i.jsx)(t.a,{href:"https://github.com/FairwindsOps/goldilocks",children:"Goldilocks"})," is an open source project that makes this easy."]}),"\n",(0,i.jsx)(t.h3,{id:"goldilocks",children:"Goldilocks"}),"\n",(0,i.jsx)(t.p,{children:'Goldilocks is an open source project from Fairwinds that is designed to help organizations get their Kubernetes application resource requests \u201cjust right". The default configuration of Goldilocks is an opt-in model. You choose which workloads are monitored by adding the goldilocks.fairwinds.com/enabled: true label to a namespace.'}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Goldilocks-Architecture",src:o(37191).A+"",width:"1291",height:"772"})}),"\n",(0,i.jsx)(t.p,{children:"The Metrics Server collects resource metrics from the Kubelet running on worker nodes and exposes them through Metrics API for use by the Vertical Pod Autoscaler. The Goldilocks controller watches for namespaces with the goldilocks.fairwinds.com/enabled: true label and creates VerticalPodAutoscaler objects for each workload in those namespaces."}),"\n",(0,i.jsx)(t.p,{children:"To enable namespaces for resource recommendation, run the below command:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"kubectl create ns javajmx-sample\nkubectl label ns javajmx-sample goldilocks.fairwinds.com/enabled=true\n"})}),"\n",(0,i.jsx)(t.p,{children:"To deploy goldilocks in the Amazon EKS Cluster, run the below command:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"helm repo add fairwinds-stable https://charts.fairwinds.com/stable\nhelm upgrade --install goldilocks fairwinds-stable/goldilocks --namespace goldilocks --create-namespace --set vpa.enabled=true\n"})}),"\n",(0,i.jsx)(t.p,{children:"Goldilocks-dashboard will expose the dashboard in the port 8080 and we can access it to get the resource recommendation.  Let\u2019s run the below command to access the dashboard:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"kubectl -n goldilocks port-forward svc/goldilocks-dashboard 8080:80\n"})}),"\n",(0,i.jsxs)(t.p,{children:["Then open your browser to ",(0,i.jsx)(t.a,{href:"http://localhost:8080",children:"http://localhost:8080"})]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Goldilocks-Dashboard",src:o(29776).A+"",width:"1128",height:"326"})}),"\n",(0,i.jsxs)(t.p,{children:["Let\u2019s analyze the sample namespace to see the recommendations provided by Goldilocks. We should be able to see the recommendations for the deployment.\n",(0,i.jsx)(t.img,{alt:"Goldilocks-Recommendation",src:o(35851).A+"",width:"1569",height:"776"})]}),"\n",(0,i.jsx)(t.p,{children:"We could see the request & limit recommendations for the javajmx-sample workload. The Current column under each Quality of Service (Qos) indicates the currently configured CPU and Memory request and limits. The Guranteed and Burstable column indicates the recommended CPU and Memory request limits for the respective QoS."}),"\n",(0,i.jsx)(t.p,{children:"We can clearly notice that we have over provisioned the resources and goldilocks has made the recommendations to optimize the CPU and Memory request. The CPU request and limits has been recommended to be 15m and 15m compared to 100m and 300m for Guranteed QoS and Memory request and limits to be 105M and 105M, compared to 180Mi and 300 Mi.\nYou can simply copy the respective manifest file for the QoS class, they are interested in and deploy the workloads which is right-sized and optimized."}),"\n",(0,i.jsx)(t.h3,{id:"understand-throttling-using-cadvisor-metric-and-configuring-the-resource-appropriately",children:"Understand throttling using cAdvisor metric and configuring the resource appropriately"}),"\n",(0,i.jsx)(t.p,{children:"When we configure limits, we are telling the Linux node how long a specific containerized application can run during a specific period of time. We do this to protect the rest of the workloads on a node from a wayward set of processes from taking an unreasonable amount of CPU cycles. We are not defining several physical \u201ccores\u201d sitting on a motherboard; however, we are configuring how much time a grouping of processes or threads in a single container can run before we want to temporarily pause the container to avoid overwhelming other applications."}),"\n",(0,i.jsxs)(t.p,{children:["There is a handy cAdvisor metrics called ",(0,i.jsx)(t.code,{children:"container_cpu_cfs_throttled_seconds_total"})," which adds up all the throttled 5 ms slices and gives us an idea how far over the quota the process is. This metric is in seconds, so we divide the value by 10 to get 100 ms, which is the real period of time associated with the container."]}),"\n",(0,i.jsx)(t.p,{children:"PromQl query to understand the top three pods CPU usage over a 100 ms time."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:'topk(3, max by (pod, container)(rate(container_cpu_usage_seconds_total{image!="", instance="$instance"}[$__rate_interval]))) / 10\n'})}),"\n",(0,i.jsx)(t.p,{children:"A value of 400 ms of vCPU usage is observed."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Throttled-Period",src:o(12124).A+"",width:"1000",height:"198"})}),"\n",(0,i.jsx)(t.p,{children:"PromQL gives us a per second throttling, with 10 periods in a second. To get the per period throttling, we divide by 10. If we want to know how much to increase the limits setting, then we can multiple by 10 (e.g., 400 ms * 10 = 4000 m)."}),"\n",(0,i.jsx)(t.p,{children:"While the above tools provide ways to identify opportunities for resource optimization, applications team should spend time in identifying whether a given application is CPU / Memory intensive and allocate resources to prevent throttling / over-provisioning."})]})}function h(e={}){const{wrapper:t}={...(0,n.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},37191(e,t,o){o.d(t,{A:()=>s});const s=o.p+"assets/images/goldilocks-architecture-879084217dd0813e22ad3fda88f1b9a0.png"},29776(e,t,o){o.d(t,{A:()=>s});const s=o.p+"assets/images/goldilocks-dashboard-94a18829a8f83117496d2bc74b32909f.png"},35851(e,t,o){o.d(t,{A:()=>s});const s=o.p+"assets/images/goldilocks-recommendation-aa45dfaf808e1d9276f2f607d0064e56.png"},12124(e,t,o){o.d(t,{A:()=>s});const s=o.p+"assets/images/throttled-period-bf33f6b78e32c8b630b1c70e67b02d92.png"},28453(e,t,o){o.d(t,{R:()=>r,x:()=>a});var s=o(96540);const i={},n=s.createContext(i);function r(e){const t=s.useContext(n);return s.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(n.Provider,{value:t},e.children)}}}]);