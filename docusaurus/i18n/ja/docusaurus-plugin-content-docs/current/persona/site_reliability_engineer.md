# Site Reliability Engineers

Site Reliability Engineering (SRE) は、ソフトウェアシステムの信頼性とパフォーマンスの向上に焦点を当てたソフトウェアエンジニアリングの実践手法です。
SRE の主要な目標の 1 つは、可用性、パフォーマンス、レイテンシー、効率性、キャパシティ、インシデント対応などの分野でソフトウェアシステムの信頼性を向上させることです。
SRE チームが目標の達成を検証するために測定する指標には、Service Level Agreement (SLA)、Service Level Objective (SLO)、Service Level Indicator (SLI)、エラーバジェットなどがあります。

以下は、オブザーバビリティ戦略を導くための SRE の重点分野とベストプラクティスです。



## インシデント対応とクライシス管理
インシデント対応には、インシデント解決までの平均時間 (MTTR) を最小限に抑え、Service Level Agreement (SLA) を満たすことを目的として、計画外のイベントや中断を監視、検出、対応することが含まれます。




### インシデント対応とクライシス管理に関するベストプラクティス：
- 迅速な検知、対応、封じ込めは、最小限の時間でインシデントを緩和し、さらなる影響を回避するために重要です。

- オンコールスケジュールを効率化し、効果的なインシデント緩和のための運用手順書を組み込んだ、堅牢なオンコールシステムを構築します。

- 効果的なインシデント後の分析プロセスを構築します。根本原因分析には通常、以下の項目が含まれます。

    - 影響分析 - このインシデントによって影響を受けたシステム、内部プロセス、エンドユーザー、および発生した可能性のある財務的影響を特定します。

    - 根本原因と解決策 - イベントの根本原因分析を実施し、将来同様のシナリオが再発することを防ぐためのガードレールを実装する機会を特定します。
    
    - モニタリングとアラーム - 設定されたメトリクスとアラームのしきい値が正しいシグナルを報告しているか、潜在的なインシデントを防止する機会があるかを特定します。
    
    - アクションアイテムと学習 - アクションアイテムに担当者を割り当て、フォローアップを行います。インシデントからの学びを製品ライフサイクルに組み込み、将来の障害を防ぐためのフィードバックメカニズムを確立することが重要です。




## サービスレベル目標と主要メトリクス

SLI (Service Level Indicator) は、実際の測定値やメトリクスです。例として、ミリ秒単位のレスポンス時間、システムの稼働率、100 万リクエストあたりのエラー率、1 秒あたりのリクエスト数によるスループット、リソース使用率（CPU、メモリなど）があります。

SLO (Service Level Objective) は、SLI を使用して設定される目標値です。これは「良好なサービス」とは何かを定義します。例として、95% のリクエストが 200 ミリ秒以内に完了すること、月間稼働率 99.9%、30 日間のエラー率が 0.1% 未満などがあります。SLI と SLO の関係は以下のように定義できます。

         SLI（メトリクス）+ 目標値 + 時間枠 = SLO
         例：レスポンス時間（SLI）+ 200 ミリ秒未満 + 30 日間の測定 = SLO



#### SLI と SLO のベストプラクティスは以下の通りです
- SLO に SMART フレームワークを確立する
    - Specific (具体的): 明確なメトリクスとしきい値 (「応答時間が 200ms 未満」)。
    - Measurable (測定可能): モニタリングツールで追跡可能。
    - Achievable (達成可能): システム能力に応じた現実的な目標。
    - Relevant (関連性): ユーザーエクスペリエンスに関連している。
    - Time-bound (期限付き): 定義された期間で測定 (例: 30 日間)。

- ユーザーエクスペリエンスに直接影響する SLI を選択する。

- ビジネスニーズに基づいた現実的な SLO 目標を設定する。

- 定期的なモニタリングと調整を行う。

- 明確な文書化とコミュニケーションを行う。

- 必要に応じて、サービスレベルごとに異なる SLO を設定する。

- 以下のような主要メトリクスを特定する

    - レイテンシー: システムがリクエストに応答するまでの時間を測定し、成功時とエラー時の両方のレイテンシーを追跡。
    - トラフィック: システムを通過するリクエストやデータの量を監視し、使用パターンとスケール要件を理解。
    - エラー: システム内で発生するエラーの頻度とタイプを追跡。
    - 飽和度: CPU やメモリなどの重要なリソースの使用率を監視し、潜在的なボトルネックを特定。


以下は SLO ドキュメントの例です

    サービス: ユーザー認証 API
    SLO: 認証リクエストの 99.9% が 500ms 以内に完了
    測定期間: 30 日間の移動期間
    SLI: サーバーで測定された応答時間
    除外事項: 計画されたメンテナンス期間




## キャパシティプランニングとスケーリング
キャパシティプランニングとイベントレディネスは、システムの信頼性を確保するための重要な要素です。




#### ベストプラクティスの例

- 予想されるユーザートラフィックパターン、ユーザーの地理的分布、対象となる AWS リージョン、イベントのピーク時間などの主要コンポーネントを含む、包括的なイベントカレンダーを実装します。

- システムのスケーリング検証、パフォーマンスベンチマーク、容量しきい値のテストを含むイベントレディネステストを実施します。

- バックアップと復元手順、リージョン切り替えの実行手順、インシデント対応プロトコル、緩和手順などのフェイルオーバーメカニズムを検証します。




## インフラストラクチャ管理のための自動化とスクリプティング
インフラストラクチャの効率的な運用には自動化が重要です。自動化により、より信頼性が高く、スケーラブルで効率的なインフラストラクチャを実現し、チームは日常的なメンテナンスではなく戦略的な取り組みに集中できるようになります。自動化の利点には以下のようなものがあります。

* 人的介入をほとんど、またはまったく必要としないシステムの信頼性向上。

* トラフィックと需要に応じてアプリケーションが自動的にスケールできる、改善されたスケーラビリティ。

* 迅速かつ自動化されたインシデント解決、エラー率の低減、MTBF（平均故障間隔）の改善。

* 運用コストの削減とリソース使用率の向上。




#### 主要な自動化戦略には以下が含まれます

* バージョン管理されたインフラストラクチャの変更と共に、Infrastructure as Code (IaC) を実装。

* 自動テストとロールバック機能を含む、継続的インテグレーション/継続的デプロイメント (CI/CD)。

* 統合されたヘルスチェックと自動復旧機能を備えた自己修復システム。





## SRE チームのためのモニタリングとアラート戦略
効果的なモニタリングとアラートは、分散型のマイクロサービスベースのアプリケーションの信頼性とパフォーマンスを積極的に確保するために、Site Reliability Engineering (SRE) チームにとって重要です。
数百のマイクロサービスを持つ可能性のある分散システムのモニタリングは課題となる場合があります。
アーキテクチャの複雑さに関係なく、重要なメトリクスを特定し、それらがアプリケーションのパフォーマンスとユーザーエクスペリエンスに与える影響から Working Backwards する必要があります。



#### 包括的なテレメトリの収集
- アーキテクチャの各コンポーネントの健全性とパフォーマンスについて、十分な洞察を提供するテレメトリデータを収集していることを確認します。
収集したデータの関連性とアクション可能性を継続的に評価します。




#### アラート戦略

- アクションにつながるアラートを定義する - テレメトリデータから生成されるアラートは、SRE チームが問題を素早く特定し対応できるよう、アクションにつながるものである必要があります。アラートは、意味のある、かつ潜在的な問題を予測できる閾値とパターンに基づいて設定する必要があります。

- アラートのルーティングとエスカレーションを最適化する - 重要な問題について適切なチームや個人に通知されるよう、明確に定義されたアラートのルーティングとエスカレーションのプロセスを実装します。対応性を向上させアラート疲れを最小限に抑えるため、アラートのルーティングを継続的にレビューし改善します。




#### ダッシュボードと可視化

- 包括的なダッシュボードの作成 - アプリケーションの状態を全体的に把握できるダッシュボードを作成します。これには、主要な運用メトリクス、コストと容量計画のデータ、インフラストラクチャの健全性が含まれます。問題を効果的に予測し防止できるよう、ダッシュボードにはしきい値とインジケータを含めるようにします。

- データ駆動型の意思決定の実現 - ダッシュボードから得られたインサイトを活用して、容量計画、パフォーマンスの最適化、インシデント対応戦略などのデータ駆動型の意思決定プロセスを実現します。




## カオスエンジニアリングと実験のガイドライン

カオスエンジニアリングの目的は、アプリケーションの信頼性をテストし、停止、トラフィックの急増、その他の外部イベントなどの破壊的なイベントに対してアプリケーションがどのように応答するかを理解することです。

カオスエンジニアリングは、チームがパフォーマンスのボトルネックやアプリケーションの動作を評価し、実際のシナリオで障害を修復するための戦略を実装するのに役立ちます。



### カオスエンジニアリングのベストプラクティス

- 小規模から始めて徐々に複雑さを増やす - これには仮説の構築が含まれます（例えば、アプリケーションのトラフィックが 30% 増加した場合、どのように動作するか）。

- 定常状態を定義する。

- 実験を通して障害を導入する。

- システムの動作を観察し、回復力を高めるための是正措置を講じる。

- 堅牢なモニタリングを実装する - 効果的なカオスエンジニアリングのために、ログ、メトリクス、トレースなどの関連するテレメトリデータを収集していることを確認する。

- 常にロールバック計画を持つ - カオスエンジニアリングを CI/CD パイプラインに統合することで、ロールバック計画の自動化とテストが可能になる。

- 各実験から学び、結果を文書化し、システムの回復力を向上させ、カオスエンジニアリングを開発ライフサイクルに統合する。

これらのカオスエンジニアリングのプラクティスを体系的に実装することで、組織はシステムの回復力を大幅に向上させ、予期せないダウンタイムを削減し、より信頼性の高いサービスを構築できます。
目的はカオスを引き起こすことではなく、カオスな状況に耐えられるシステムを構築することであることを忘れないでください。



## 参考資料
- [AWS Observability Workshop](https://catalog.workshops.aws/observability/en-US)
- [AWS Observability Best Practices](https://aws-observability.github.io/observability-best-practices/)
- [Amazon CloudWatch Intelligent Operations](https://aws.amazon.com/jp/cloudwatch/features/intelligent-operations/)
- [Resilience analysis framework](https://docs.aws.amazon.com/ja_jp/prescriptive-guidance/latest/resilience-analysis-framework/introduction.html)
- [Chaos Engineering with AWS Fault Injection Simulator](https://aws.amazon.com/jp/blogs/news/chaos-testing-with-aws-fault-injection-simulator-and-aws-codepipeline/)
