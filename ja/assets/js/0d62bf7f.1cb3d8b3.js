"use strict";(self.webpackChunkobservability_best_practices=self.webpackChunkobservability_best_practices||[]).push([[2348],{63488:(e,s,a)=>{a.r(s),a.d(s,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>d});var n=a(74848),t=a(28453);const i={},o="Autoscaling applications using KEDA on AMP and EKS",r={id:"guides/containers/oss/eks/keda-amp-eks",title:"Autoscaling applications using KEDA on AMP and EKS",description:"Handling increased traffic on Amazon EKS applications is challenging, with manual scaling being inefficient and error-prone. Autoscaling offers a better solution for resource allocation. KEDA enables Kubernetes autoscaling based on various metrics and events, while Amazon Managed Service for Prometheus provides secure metric monitoring for EKS clusters. This solution combines KEDA with Amazon Managed Service for Prometheus, demonstrating autoscaling based on Requests Per Second (RPS) metrics. The approach delivers automated scaling tailored to workload demands, which users can apply to their own EKS workloads. Amazon Managed Grafana is used for monitoring and visualizing scaling patterns, allowing users to gain insights into autoscaling behaviors and correlate them with business events.",source:"@site/docs/guides/containers/oss/eks/keda-amp-eks.md",sourceDirName:"guides/containers/oss/eks",slug:"/guides/containers/oss/eks/keda-amp-eks",permalink:"/observability-best-practices/ja/docs/guides/containers/oss/eks/keda-amp-eks",draft:!1,unlisted:!1,editUrl:"https://github.com/aws-observability/observability-best-practices/docs/guides/containers/oss/eks/keda-amp-eks.md",tags:[],version:"current",frontMatter:{},sidebar:"guides",previous:{title:"EKS \u306e\u30aa\u30d6\u30b6\u30fc\u30d0\u30d3\u30ea\u30c6\u30a3: \u5fc5\u9808\u30e1\u30c8\u30ea\u30af\u30b9",permalink:"/observability-best-practices/ja/docs/guides/containers/oss/eks/best-practices-metrics-collection"},next:{title:"Lambda \u30d9\u30fc\u30b9\u306e\u30b5\u30fc\u30d0\u30fc\u30ec\u30b9\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u305f\u3081\u306e\u30aa\u30d6\u30b6\u30fc\u30d0\u30d3\u30ea\u30c6\u30a3",permalink:"/observability-best-practices/ja/docs/guides/serverless/aws-native/lambda-based-observability"}},c={},d=[{value:"KEDA configuration",id:"keda-configuration",level:2},{value:"Blogs",id:"blogs",level:2}];function l(e){const s={a:"a",h1:"h1",h2:"h2",img:"img",p:"p",...(0,t.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(s.h1,{id:"autoscaling-applications-using-keda-on-amp-and-eks",children:"Autoscaling applications using KEDA on AMP and EKS"}),"\n",(0,n.jsx)(s.h1,{id:"current-landscape",children:"Current Landscape"}),"\n",(0,n.jsx)(s.p,{children:"Handling increased traffic on Amazon EKS applications is challenging, with manual scaling being inefficient and error-prone. Autoscaling offers a better solution for resource allocation. KEDA enables Kubernetes autoscaling based on various metrics and events, while Amazon Managed Service for Prometheus provides secure metric monitoring for EKS clusters. This solution combines KEDA with Amazon Managed Service for Prometheus, demonstrating autoscaling based on Requests Per Second (RPS) metrics. The approach delivers automated scaling tailored to workload demands, which users can apply to their own EKS workloads. Amazon Managed Grafana is used for monitoring and visualizing scaling patterns, allowing users to gain insights into autoscaling behaviors and correlate them with business events."}),"\n",(0,n.jsx)(s.h1,{id:"autoscaling-application-based-on-amp-metrics-with-keda",children:"Autoscaling application based on AMP metrics with KEDA"}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"keda-arch",src:a(32889).A+"",width:"559",height:"340"})}),"\n",(0,n.jsx)(s.p,{children:"This solution demonstrates AWS integration with open-source software to create an automated scaling pipeline. It combines Amazon EKS for managed Kubernetes, AWS Distro for Open Telemetry (ADOT) for metric collection, KEDA for event-driven autoscaling, Amazon Managed Service for Prometheus for metric storage, and Amazon Managed Grafana for visualization. The architecture involves deploying KEDA on EKS, configuring ADOT to scrape metrics, defining autoscaling rules with KEDA ScaledObject, and using Grafana dashboards to monitor scaling. The autoscaling process begins with user requests to the microservice, ADOT collecting metrics, and sending them to Prometheus. KEDA queries these metrics at regular intervals, determines scaling needs, and interacts with the Horizontal Pod Autoscaler (HPA) to adjust pod replicas. This setup enables metrics-driven autoscaling for Kubernetes microservices, providing a flexible, cloud-native architecture that can scale based on various utilization indicators."}),"\n",(0,n.jsx)(s.h1,{id:"cross-account-eks-application-scaling-with-keda-on-amp-metrics",children:"Cross account EKS application scaling with KEDA on AMP metrics"}),"\n",(0,n.jsx)(s.p,{children:"In this case, lets assume KEDA EKS is running on AWS Account ending with ID 117 and central AMP Account ID is ending with 814. In the KEDA EKS account, setup the cross account IAM role as below:"}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"keda1",src:a(14087).A+"",width:"1654",height:"1048"})}),"\n",(0,n.jsxs)(s.p,{children:["Also the trust relationship to be updated as below:\n",(0,n.jsx)(s.img,{alt:"keda2",src:a(11436).A+"",width:"1674",height:"990"})]}),"\n",(0,n.jsxs)(s.p,{children:["In the EKS cluster, you could see we dont use Pod identity since IRSA is being used here\n",(0,n.jsx)(s.img,{alt:"keda3",src:a(47349).A+"",width:"1674",height:"1152"})]}),"\n",(0,n.jsxs)(s.p,{children:["Whilst the central AMP account we have the AMP access set up as below\n",(0,n.jsx)(s.img,{alt:"keda4",src:a(23578).A+"",width:"1662",height:"780"})]}),"\n",(0,n.jsxs)(s.p,{children:["The trust relationship has the access as well\n",(0,n.jsx)(s.img,{alt:"keda5",src:a(71331).A+"",width:"1680",height:"924"})]}),"\n",(0,n.jsxs)(s.p,{children:["And take a note of the workspace ID as below\n",(0,n.jsx)(s.img,{alt:"keda6",src:a(34472).A+"",width:"1674",height:"682"})]}),"\n",(0,n.jsx)(s.h2,{id:"keda-configuration",children:"KEDA configuration"}),"\n",(0,n.jsx)(s.p,{children:"With the setup in place, lets ensure keda is running as below. For setup instructions refer to the blog link shared below"}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.img,{alt:"keda7",src:a(44689).A+"",width:"1674",height:"554"})}),"\n",(0,n.jsxs)(s.p,{children:["Ensure to use the central AMP role defined above in the configuration\n",(0,n.jsx)(s.img,{alt:"keda8",src:a(67382).A+"",width:"1668",height:"326"})]}),"\n",(0,n.jsxs)(s.p,{children:["In the KEDA scaler configuration, point to the central AMP account as below\n",(0,n.jsx)(s.img,{alt:"keda9",src:a(64319).A+"",width:"1552",height:"862"})]}),"\n",(0,n.jsxs)(s.p,{children:["And now you can see that the pods are scaled appropriately\n",(0,n.jsx)(s.img,{alt:"keda10",src:a(68327).A+"",width:"1360",height:"864"})]}),"\n",(0,n.jsx)(s.h2,{id:"blogs",children:"Blogs"}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.a,{href:"https://aws.amazon.com/blogs/mt/autoscaling-kubernetes-workloads-with-keda-using-amazon-managed-service-for-prometheus-metrics/",children:"https://aws.amazon.com/blogs/mt/autoscaling-kubernetes-workloads-with-keda-using-amazon-managed-service-for-prometheus-metrics/"})})]})}function h(e={}){const{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,n.jsx)(s,{...e,children:(0,n.jsx)(l,{...e})}):l(e)}},32889:(e,s,a)=>{a.d(s,{A:()=>n});const n=a.p+"assets/images/arch-ba3f06a9d099b8a55247d888269f65ae.png"},14087:(e,s,a)=>{a.d(s,{A:()=>n});const n=a.p+"assets/images/keda1-d8a375bf73aad401e33ce229cc307e78.png"},68327:(e,s,a)=>{a.d(s,{A:()=>n});const n=a.p+"assets/images/keda10-82e65c546a6bbed8f1852c26f7f06b78.png"},11436:(e,s,a)=>{a.d(s,{A:()=>n});const n=a.p+"assets/images/keda2-16022f1d3dd486f14a565662c8ebee83.png"},47349:(e,s,a)=>{a.d(s,{A:()=>n});const n=a.p+"assets/images/keda3-ef4da21377989f4c7820c015b7a126f1.png"},23578:(e,s,a)=>{a.d(s,{A:()=>n});const n=a.p+"assets/images/keda4-fdf35c9439598ec86650c83b44df6cf3.png"},71331:(e,s,a)=>{a.d(s,{A:()=>n});const n=a.p+"assets/images/keda5-fdf9c73abc77c8ec9402e01ebb07e707.png"},34472:(e,s,a)=>{a.d(s,{A:()=>n});const n=a.p+"assets/images/keda6-1bed5794e7c96191e530dbc5dda8bfa6.png"},44689:(e,s,a)=>{a.d(s,{A:()=>n});const n=a.p+"assets/images/keda7-e069b701309fe2112637b73428faae19.png"},67382:(e,s,a)=>{a.d(s,{A:()=>n});const n=a.p+"assets/images/keda8-80b49c92e8874797ba0908a64b7c34b4.png"},64319:(e,s,a)=>{a.d(s,{A:()=>n});const n=a.p+"assets/images/keda9-d92387e98158e8b37afa666a20468172.png"},28453:(e,s,a)=>{a.d(s,{R:()=>o,x:()=>r});var n=a(96540);const t={},i=n.createContext(t);function o(e){const s=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function r(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),n.createElement(i.Provider,{value:s},e.children)}}}]);