{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What is observability","text":""},{"location":"#what-it-is","title":"What it is","text":"<p>Observability is the capability to continuously generate and discover actionable insights based on signals from the system under observation. In other words, observability allows users to understand a system\u2019s state from its external output and take (corrective) action.</p>"},{"location":"#problem-it-addresses","title":"Problem it addresses","text":"<p>Computer systems are measured by observing low-level signals such as CPU time, memory, disk space, and higher-level and business signals, including API response times, errors, transactions per second, etc.</p> <p>The observability of a system has a significant impact on its operating and development costs. Observable systems yield meaningful, actionable data to their operators, allowing them to achieve favorable outcomes (faster incident response, increased developer productivity) and less toil and downtime.</p>"},{"location":"#how-it-helps","title":"How it helps","text":"<p>Understanding that more information does not necessarily translate into a more observable system is pivotal. In fact, sometimes, the amount of information generated by a system can make it harder to identify valuable health signals from the noise generated by the application. Observability requires the right data at the right time for the right consumer (human or piece of software) to make the right decisions.</p>"},{"location":"#what-you-will-find-here","title":"What you will find here","text":"<p>This site contains our best practices for observability: what do to, what not to do, and a collection of recipes on how to do them. Most of the content here is vendor agnostic and represents what any good observability solution will provide.</p> <p>It is important that you consider observability as a solution though and not a product. Observability comes from your practices, and is integral to strong development and DevOps leadership. A well-observed application is one that places observability as a principal of operations, similar to how security must be at the forefront of how you organize a project. Attempting to \u201cbolt-on\u201d observability after the fact is an anti-pattern and meets with less success.</p> <p>This site is organized into four categories:</p> <ol> <li>Best practices by solution, such as for dashboarding, application performance monitoring, or containers</li> <li>Best practices for the use of different data types, such as for logs or traces</li> <li>Best practices for specific AWS tools (though these are largely fungible to other vendor products as well)</li> <li>Curated recipes for observability with AWS</li> </ol> <p>Success</p> <p>This site is based on real world use cases that AWS and our customers have solved for.</p> <p>Observability is at the heart of modern application development, and a critical consideration when operating distributed systems, such as microservices, or complex applications with many external integrations. We consider it to be a leading indicator of a healthy workload, and we are pleased to share our experiences with you here!</p>"},{"location":"contributors/","title":"Contributors","text":"<p>The content on this site is maintained by Solution Architects, AWS Observability service team members and other volunteers from across the organization. Our goal is to improve the discovery of relevant best practices on how to set up and use AWS services and open source projects in the observability space.</p> <p>Recipes and content contributions in general so far are from the following people:</p> Authors Authors Authors Authors Alolita Sharma Aly Shah Imtiaz Helen Ashton Elamaran Shanmugam Dinesh Boddula Imaya Kumar Jagannathan Dieter Adant Eric Hsueh Jason Derrett Kevin Lewin Mahesh Biradar Michael Hausenblas Munish Dabra Rich McDonough Rob Sable Rodrigue Koffi Sheetal Joshi Tomasz Wrzonski Tyler Lynch Vijayan Sarathy Vikram Venkataraman Yiming Peng Arun Chandapillai Alex Livingstone Kiran Prakash Bobby Hallahan Toshal Dudhwala Franklin Aguinaldo Nirmal Mehta Lucas Vieira Souza da Silva William Armiros Abhi Khanna Arvind Raghunathan Doyita Mitra Rahul Popat Taiki Hibira <p>Note that all recipes published on this site are available via the MIT-0 license, a modification to the usual MIT license that removes the requirement for attribution.</p>"},{"location":"faq/adot/","title":"AWS Distro for Open Telemetry (ADOT) -  FAQ","text":"<ol> <li>Can I use the ADOT collector to ingest metrics into AMP?     Yes, this functionality was introduced with the GA launch for metrics support in May 2022 and you can use the ADOT collector from EC2, via our EKS add-on, via our ECS side-car integration, and/or via our Lambda layers.</li> <li>Can I use the ADOT collector to collect logs and ingest them into Amazon CloudWatch or Amazon OpenSearch?     Not yet but we\u2019re working on stabilizing logs upstream in OpenTelemetry and when the time comes, potentially later in 2023 or early 2024 we will support logs in ADOT, see also the public roadmap entry</li> <li>Where can I find resource usage and performance details on the ADOT collector?     We have a Performance Report online that we keep up to date as we release collectors.</li> <li>Is it possible to use ADOT with Apache Kafka?     Yes, support to Kafka exporter and receiver was added in the ADOT collector v0.28.0. For more details, please check the ADOT collector documentation.</li> <li>How can I configure the ADOT collector?     The ADOT collector is configured using YAML configuration files that are stored locally. Besides that, it is possible to use configuration stored in other locations, like S3 buckets. All the supported mechanisms to configure the ADOT collector are described in detail in the ADOT collector documentation.</li> <li>Can I do advanced sampling in the ADOT collector?     We\u2019re working on it, please subscribe to the public roadmap entry to keep up to date.</li> <li>Any tips how to scale the ADOT collector?     Yes! See the upstream OpenTelemetry docs on Scaling the Collector.</li> <li>I have a fleet of ADOT collectors, how can I manage them?     This is an area of active development and  we expect that it will mature in 2023, see the upstream OpenTelemetry docs on Management for more details, specifically on the Open Agent Management Protocol (OpAMP).</li> <li>How do you monitor the health and performance of the ADOT collector?<ol> <li>Monitoring the collector - default metrics exposed on port 8080 that can be scraped by the Prometheus receiver</li> <li>Using the Node Exporter, running node exporter would also provide several performance and health metrics about the node, pod, and operating system the collector is running in.</li> <li>Kube-state-metrics (KSM), KSM can also produce interesting events about the collector.</li> <li>Prometheus <code>up</code> metric</li> <li>A simple Grafana dashboard to get started: https://grafana.com/grafana/dashboards/12553</li> </ol> </li> <li>Product FAQ - https://aws.amazon.com/otel/faqs/</li> </ol>"},{"location":"faq/amg/","title":"Amazon Managed Grafana - FAQ","text":"<p>Why should I choose Amazon Managed Grafana?</p> <p>High Availability: Amazon Managed Grafana workspaces are highly available with multi-az replication. Amazon Managed Grafana also continuously monitors for the health of workspaces and replaces unhealthy nodes, without impacting access to the workspaces. Amazon Managed Grafana manages the availability of compute and database nodes so customers don\u2019t have to manage the infrastructure resources required for the administration &amp; maintenance.</p> <p>Data Security: Amazon Managed Grafana encrypts the data at rest without any special configuration, third-party tools, or additional cost. Data in-transit area also encrypted via TLS.</p> <p>Which AWS regions are supported?</p> <p>Current list of supported Regions is available in the Supported Regions section in the documentation.</p> <p>We have multiple AWS accounts in multiple regions in our Organization, does Amazon Managed Grafana work for these scenarios</p> <p>Amazon Managed Grafana integrates with AWS Organizations to discover AWS accounts and resources in Organizational Units (OUs). With AWS Organizations customers can centrally manage data source configuration and permission settings for multiple AWS accounts.</p> <p>What data sources are supported in Amazon Managed Grafana?</p> <p>Data sources are storage backends that customers can query in Grafana to build dashboards in Amazon Managed Grafana. Amazon Managed Grafana supports about 30+ built-in data sources including AWS native services like Amazon CloudWatch, Amazon OpenSearch Service, AWS IoT SiteWise, AWS IoT TwinMaker, Amazon Managed Service for Prometheus, Amazon Timestream, Amazon Athena, Amazon Redshift, AWS X-Ray and many others. Additionally, about 15+ other data sources are also available for upgraded workspaces in Grafana Enterprise.</p> <p>Data sources of my workloads are in private VPCs. How do I connect them to Amazon Managed Grafana securely?</p> <p>Private data sources within a VPC can be connected to Amazon Managed Grafana through AWS PrivateLink to keep the traffic secure. Further access control to Amazon Managed Grafana service from the VPC endpoints can be restricted by attaching an IAM resource policy for Amazon VPC endpoints.</p> <p>What User Authentication mechanism is available in Amazon Managed Grafana?</p> <p>In Amazon Managed Grafana workspace, users are authenticated to the Grafana console by single sign-on using any IDP that supports Security Assertion Markup Language 2.0 (SAML 2.0) or AWS IAM Identity Center (successor to AWS Single Sign-On).</p> <p>Related blog: Fine-grained access control in Amazon Managed Grafana using Grafana Teams</p> <p>What kind of automation support is available for Amazon Managed Grafana?</p> <p>Amazon Managed Grafana is integrated with AWS CloudFormation, which helps customers in modeling and setting up AWS resources so that customers can spend less time creating and managing resources and infrastructure in AWS. With AWS CloudFormation customers can reuse templates to set up Amazon Managed Grafana resources consistently and repeatedly. Amazon Managed Grafana also has APIavailable which supports customers in automating through AWS CLI or integrating with software/products. Amazon Managed Grafana workspaces has HTTP APIs for automation and integration support.</p> <p>Related blog: Announcing Private VPC data source support for Amazon Managed Grafana</p> <p>My Organization uses Terraform for automation. Does Amazon Managed Grafana support Terraform? Yes, Amazon Managed Grafana supports Terraform for automation</p> <p>Example: Reference implementation for Terraform support</p> <p>I am using commonly used Dashboards in my current Grafana setup. Is there a way to use them on Amazon Managed Grafana rather than re-creating again?</p> <p>Amazon Managed Grafana supports HTTP APIs that allow you to easily automate deployment and management of Dashboards, users and much more. You can use these APIs in your GitOps/CICD processes to automate management of these resources.</p> <p>Does Amazon Managed Grafana support Alerts?</p> <p>Amazon Managed Grafana alerting provides customers with robust and actionable alerts that help learn about problems in the systems in near real time, minimizing disruption to services. Grafana includes access to an updated alerting system, Grafana alerting, that centralizes alerting information in a single, searchable view.</p> <p>My Organization requires all actions be recorded for audits. Can Amazon Managed Grafana events be recorded?</p> <p>Amazon Managed Grafana is integrated with AWS CloudTrail, which provides a record of actions taken by a user, a role, or an AWS service in Amazon Managed Grafana. CloudTrail captures all API calls for Amazon Managed Grafanaas events. The calls that are captured include calls from the Amazon Managed Grafana console and code calls to the Amazon Managed Grafana API operations.</p> <p>What more information is available?</p> <p>For additional information on Amazon Managed Grafana customers can read the AWS Documentation, go through the AWS Observability Workshop on Amazon Managed Grafana and also check the product page to know the features, pricing details, latest blog posts and videos.</p> <p>Product FAQ https://aws.amazon.com/grafana/faqs/</p>"},{"location":"faq/amp/","title":"Amazon Managed Service for Prometheus - FAQ","text":"<ol> <li>Which AWS Regions are supported currently and is it possible to collect metrics from other regions? See our documentation for updated list of Regions that we support. We plan to support all commercial regions in 2023. Please let us know which regions you would like so that we can better prioritize our existing Product Feature Requests (PFRs). You can always collect data from any regions and send it to a specific region that we support. Here\u2019s a blog for more details: Cross-region metrics collection for Amazon Managed Service for Prometheus.</li> <li>How long does it take to see metering and/or billing in Cost Explorer or  CloudWatch as AWS billing charges?     We meter blocks of ingested metric samples as soon as they are uploaded to S3 every 2 hours. It can take up to 3 hours to see metering and charges reported for Amazon Managed Service for Prometheus.</li> <li>As far as I can see the Prometheus Service is only able to scrape metrics from a cluster (EKS/ECS) Is that correct?     We apologize for the lack of documentation for other compute environments. You can use Prometheus server to scrape Prometheus metrics from EC2 and any other compute environments where you can install a Prometheus server today as long as you configure the remote write and setup the AWS SigV4 proxy. The link to the EC2 blog has a section \u201cRunning aws-sigv4-proxy\u201d that can show you how to run it. We do need to add more documentation to help our customers simplify how to run AWS SigV4 on other compute environments.</li> <li>How would one connect this service to Grafana? Is there some documentation about this?     We use the default Prometheus data source available in Grafana to query Amazon Managed Service for Prometheus using PromQL. Here\u2019s some documentation and a blog that will help you get started:<ol> <li>Service docs</li> <li>Grafana setup on EC2</li> </ol> </li> <li>What are some of the best practices to reduce the number of samples being sent to Amazon Managed Service for Prometheus?     To reduce the number of samples being ingested into Amazon Managed Service for Prometheus, customers can extend their scrape interval (e.g., change from 30s to 1min) or decrease the number of series they are scraping. Changing the scrape interval will have a more dramatic impact on the number of samples than decreasing the number of series, with doubling the scrape interval halving the volume of samples ingested.</li> <li> <p>How to send CloudWatch metrics to Amazon Managed Service for Prometheus?     We recommend utilizing CloudWatch metric streams to send CloudWatch metrics to Amazon Managed Service for Prometheus. Some possible shortcomings of this integration are,</p> <ol> <li>A Lambda function is required to call the Amazon Managed Service for Prometheus APIs,</li> <li>No ability to enrich CloudWatch metrics with metadata (e.g., with AWS tags) before ingesting them to Amazon Managed Service for Prometheus,</li> <li>Metrics can only be filtered by namespace (not granular enough). As an alternative, customers can utilize Prometheus Exporters to send CloudWatch metrics data to Amazon Managed Service for Prometheus: (1) CloudWatch  Exporter: Java based scraping that uses CW ListMetrics and  GetMetricStatistics (GMS) APIs.</li> </ol> <p>Yet Another CloudWatch Exporter (YACE) is another option to get metrics from CloudWatch into Amazon Managed Service for Prometheus. This is a Go based tool that uses the CW ListMetrics, GetMetricData (GMD), and  GetMetricStatistics (GMS) APIs. Some disadvantages in using this could be that you will have to deploy the agent and have to manage the life-cycle yourself which has to be done thoughtfully.</p> </li> <li> <p>What version of Prometheus are you compatible with?     Amazon Managed Service for Prometheus is compatible with Prometheus 2.x. Amazon Managed Service for Prometheus is based on the open source CNCF Cortex project as its data plane. Cortex strives to be 100% API compatible with Prometheus (under /prometheus/ and /api/prom/). Amazon Managed Service for Prometheus supports Prometheus-compatible PromQL queries and Remote write metric ingestion and the Prometheus data model for existing metric types including Gauge, Counters, Summary, and Histogram. We do not currently expose all Cortex APIs. The list of compatible APIs we support can be found here. Customers can work with their account team to open new or influence existing Product Features Requests (PFRs) if we are missing any features required from Amazon Managed Service for Prometheus.</p> </li> <li>What collector do you recommend for ingesting metrics into Amazon Managed Service for Prometheus? Should I utilize Prometheus in Agent mode?     We support the usage of Prometheus servers inclusive of agent mode, the OpenTelemetry agent, and the AWS Distro for OpenTelemetry agent as agents that customers can use to send metrics data to Amazon Managed Service for Prometheus. The AWS Distro for OpenTelemetry is a downstream distribution of the OpenTelemetry project packaged and secured by AWS. Any of the three should be fine, and you\u2019re welcome to pick whichever best suits your individual team\u2019s needs and preferences.</li> <li>How does Amazon Managed Service for Prometheus\u2019s performance scale with the size of a workspace?     Currently, Amazon Managed Service for Prometheus supports up to 200M active time series in a single workspace. When we announce a new max limit, we\u2019re ensuring that the performance and reliability properties of the service continue to be maintained across ingest and query. Queries across the same size dataset should not see a performance degradation regardless of the number of active series in a workspace.</li> <li>Product FAQ https://aws.amazon.com/prometheus/faqs/</li> </ol>"},{"location":"faq/cloudwatch/","title":"Amazon CloudWatch - FAQ","text":"<p>Why should I choose Amazon CloudWatch?</p> <p>Amazon CloudWatch is an AWS cloud native service which provides unified observability on a single platform for monitoring AWS cloud resources and the applications you run on AWS. Amazon CloudWatch can be used to collect monitoring and operational data in the form of logs, track metrics, eventsand set alarms. It also provides a unified view of AWS resources, applications, and services that run on AWS and on-premises servers. Amazon CloudWatch helps you gain system-wide visibility into resource utilization, application performance, and operational health of your workloads. Amazon CloudWatch provides actionable insights for AWS, hybrid, and on-premises applications and infrastructure resources. Cross-account observability is an addition to CloudWatch\u2019s unified observability capability.</p> <p>Which AWS Services are natively integrated with Amazon CloudWatch and Amazon CloudWatch Logs?</p> <p>Amazon CloudWatch natively integrates with more than 70+ AWS services allowing customers to collect infrastructure metrics for simplified monitoring and scalability with no action. Please check the documentation for a complete list of supported AWS services that publish CloudWatch metrics. Currently, more than 30 AWS services publish logs to CloudWatch. Please check the documentation for a complete list of supported AWS services that publish logs to CloudWatch Logs.</p> <p>Where do I get the list of all the published metrics from all AWS Services to Amazon CloudWatch?</p> <p>The list of all the AWS Services that publish metrics to Amazon CloudWatch is in AWS documentation.</p> <p>Where do I get started for collecting &amp; monitoring metrics to Amazon CloudWatch?</p> <p>Amazon CloudWatch collects metrics from various AWS Services which can be viewed through AWS Management Console, AWS CLI, or an API. Amazon CloudWatch collects available metrics for Amazon EC2 Instances. For additional custom metrics customers can make use of unified CloudWatch agent to collect and monitor.</p> <p>Related AWS Observability Workshop: Metrics</p> <p>My Amazon EC2 Instance requires very granular level of monitoring, what do I do?</p> <p>By default, Amazon EC2 sends metric data to CloudWatch in 5-minute periods as Basic Monitoring for an instance. To send metric data for your instance to CloudWatch in 1-minute periods, detailed monitoring can be enabled on the instance.</p> <p>I want to publish own metrics for my application. Is there an option?</p> <p>Customers can also publish their own custom metrics to CloudWatch using the API or CLI through standard resolution of 1 minute granularity or high resolution granularity down to 1 sec interval.</p> <p>The CloudWatch agent also supports collecting custom metrics from EC2 instances in specialized scenarios like Network performance metrics for EC2 instances running on Linux that use the Elastic Network Adapter (ENA), NVIDIA GPU metrics from Linux servers and Process metrics using procstat plugin from individual processes on Linux &amp; Windows servers.</p> <p>Related AWS Observability Workshop: Public custom metrics</p> <p>What more support is available for collecting custom metrics through Amazon CloudWatch agent?</p> <p>Custom metrics from applications or services can be retrieved using the unified CloudWatch agent with support for StatsDor collectdprotocols. StatsD is a popular open-source solution that can gather metrics from a wide variety of applications. StatsD is especially useful for instrumenting own metrics, which supports both Linux and Windows based servers. collectd protocol is a popular open-source solution supported only on Linux Servers with plugins that can gather system statistics for a wide variety of applications.</p> <p>My workload contains lot of ephemeral resources and generates logs in high-cardinality, what is the recommended approach collecting and measuring the metrics and logs?</p> <p>CloudWatch embedded metric format enables customers to ingest complex high-cardinality application data in the form of logs and to generate actionable metrics from ephemeral resources such as Lambda functions and containers. By doing so, customers can embed custom metrics alongside detailed log event data without having to instrument or maintain separate code, while gaining powerful analytical capabilities on your log data and CloudWatch can automatically extract the custom metrics to help visualize the data and set alarm on them for real-time incident detection.</p> <p>Related AWS Observability Workshop: Embedded Metric Format</p> <p>Where do I get started for collecting &amp; monitoring logs to Amazon CloudWatch?</p> <p>Amazon CloudWatch Logs helps customers monitor and troubleshoot systems and applications in near real time using existing system, application and custom log files. Customers can install the unified CloudWatch Agent to collect logs from Amazon EC2 Instances and on-premise servers to CloudWatch.</p> <p>Related AWS Observability Workshop: Log Insights</p> <p>What is CloudWatch agent and why should I use that?</p> <p>The Unified CloudWatch Agent is an open-source software under the MIT license which supports most operating systems utilizing x86-64 and ARM64 architectures. The CloudWatch Agent helps collect system-level metrics from Amazon EC2 Instances &amp; on-premise servers in a hybrid environment across operating systems, retrieve custom metrics from applications or services and collect logs from Amazon EC2 instances and on-premises servers.</p> <p>I\u2019ve all scales of installation required in my environment, so how can the CloudWatch agent be installed normally and using automation?</p> <p>On all the supported operating systems including Linux and Windows Servers, customers can download and install the CloudWatch agent using the command line, using AWS Systems Manager, or using an AWS CloudFormation template. You can also install the CloudWatch agent on on-premise servers for monitoring.</p> <p>We have multiple AWS accounts in multiple regions in our Organization, does Amazon CloudWatch work for these scenarios.</p> <p>Amazon CloudWatch provides cross-account observabilitywhich helps customers monitor and troubleshoot health of resources and applications that span multiple accounts within a region. Amazon CloudWatch also provides a cross-account, cross-region dashboard. With this functionality customers can gain visibility and insights of their multi-account, multi-region resources and workloads.</p> <p>What kind of automation support is available for Amazon CloudWatch?</p> <p>Apart from accessing Amazon CloudWatch through the AWS Management Console customers can also access the service via API, AWS command-line interface (CLI) and AWS SDKs. CloudWatch API for metrics &amp; dashboards help in automating through AWS CLIor integrating with software/products so that you can spend less time managing or administering the resources and applications. CloudWatch API for logs along with AWS CLI are also available separately. Code examples for CloudWatch using AWS SDKs are available for customers for additional reference.</p> <p>I want to get started with monitoring resources quickly, what is the recommended approach?</p> <p>Automatic Dashboards in CloudWatch are available in all AWS public regions which provides an aggregated view of the health and performance of all AWS resources. This helps customers quickly get started with monitoring, resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues. Automatic Dashboards are pre-built with AWS service recommended best practices, remain resource aware, and dynamically update to reflect the latest state of important performance metrics.</p> <p>Related AWS Observability Workshop: Automatic Dashboards</p> <p>I want to customize what I want to monitor in CloudWatch, what is the recommended approach?</p> <p>With Custom Dashboard customers can create as many additional dashboards as they want with different widgets and customize it accordingly. When creating a custom dashboard, there are a variety of widget types that are available to pick and choose for customization.</p> <p>Related AWS Observability Workshop: Dashboarding</p> <p>I\u2019ve built few custom dashboards , is there a way to share it?</p> <p>Yes, sharing of CloudWatch dashboards is possible. There are three ways to share. Sharing a single dashboard publicly by allowing anyone with access to the link to view the dashboard. Sharing a single dashboard privately by specifying the email addresses of the people who are allowed to view the dashboard. Sharing all of the CloudWatch dashboards in the account by specifying a third-party single sign-on (SSO) provider for dashboard access.</p> <p>Related AWS Observability Workshop: Sharing CloudWatch Dashboards</p> <p>I want to improve the observability of my application including the aws resources underneath, how can I accomplish?</p> <p>Amazon CloudWatch Application Insights facilitates observability for your applications along with the underlying AWS resources like SQL Server database, .Net based web (IIS) stack, application servers, OS, load balancers, queues, etc. It helps customers identify and set up key metrics and logs across application resources &amp; technology stack. By doing so, it reduces mean time to repair (MTTR) &amp; troubleshoot application issues faster.</p> <p>Additional details in FAQ: AWS resource &amp; custom metrics monitoring</p> <p>My Organization is open-source centric, does Amazon CloudWatch support monitoring &amp; observability through open-source technologies.</p> <p>For collecting metrics and traces, AWS Distro for OpenTelemetry (ADOT) Collector along with the CloudWatch agent can be installed side-by-side on Amazon EC2 Instance and OpenTelemetry SDKs can be used to collect application traces &amp; metrics from your workloads running on Amazon EC2 Instances.</p> <p>To support OpenTelemetry metrics in Amazon CloudWatch, AWS EMF Exporter for OpenTelemetry Collector converts OpenTelemetry format metrics to CloudWatch Embedded Metric Format(EMF) which enables applications integrated in OpenTelemetry metrics to be able to send high-cardinality application metrics to CloudWatch.</p> <p>For logs, Fluent Bit helps create an easy extension point for streaming logs from Amazon EC2 to AWS services including Amazon CloudWatch for log retention and analytics. The newly-launched Fluent Bit plugin can route logs to Amazon CloudWatch.</p> <p>For Dashboards, Amazon Managed Grafana can be added with Amazon CloudWatch as a data source by using the AWS data source configuration option in the Grafana workspace console. This feature simplifies adding CloudWatch as a data source by discovering existing CloudWatch accounts and manage the configuration of the authentication credentials that are required to access CloudWatch.</p> <p>Our workload is already built to collect metrics using Prometheus from the environment. Can I continue using the same methodology.</p> <p>Customers can choose to have an all open-source setup for their observability needs. For which, AWS Distro for OpenTelemetry (ADOT) Collector can be configured to scrape from a Prometheus-instrumented application and send the metrics to Prometheus Server or Amazon Managed Prometheus.</p> <p>The CloudWatch agent on EC2 instances can be installed &amp; configured with Prometheus to scrape metrics for monitoring in CloudWatch. This can be helpful to customers who prefer container workloads on EC2 and require custom metrics that are compatible with open source Prometheus monitoring.</p> <p>CloudWatch Container Insights monitoring for Prometheus automates the discovery of Prometheus metrics from containerized systems and workloads. Discovering Prometheus metrics is supported for Amazon Elastic Container Service (ECS), Amazon Elastic Kubernetes Service (EKS) and Kubernetes clusters running on Amazon EC2 instances.</p> <p>My workloads contain microservices compute, especially EKS/Kubernetes related containers, how do I use Amazon CloudWatch to gain insights into the environment?</p> <p>Customers can use CloudWatch Container Insights to collect, aggregate, and summarize metrics &amp; logs from containerized applications and microservices running on Amazon Elastic Kubernetes Service (Amazon EKS) or Kubernetes platforms on Amazon EC2. Container Insights also supports collecting metrics from clusters deployed on Fargate for Amazon EKS. CloudWatch automatically collects metrics for many resources, such as CPU, memory, disk &amp; network and also provides diagnostic information, such as container restart failures, to help isolate issues and resolve them quickly.</p> <p>Related AWS Observability Workshop: Container Insights on EKS</p> <p>My workloads contain microservices compute, especially ECS related containers, how do I use Amazon CloudWatch to gain insights into the environment?</p> <p>Customers can use CloudWatch Container Insights to collect, aggregate, and summarize metrics &amp; logs from containerized applications and microservices running on Amazon Elastic Container Service (Amazon ECS) or container platforms on Amazon EC2. Container Insights also supports collecting metrics from clusters deployed on Fargate for Amazon ECS. CloudWatch automatically collects metrics for many resources, such as CPU, memory, disk &amp; network and also provides diagnostic information, such as container restart failures, to help isolate issues and resolve them quickly.</p> <p>Related AWS Observability Workshop: Container Insights on ECS</p> <p>My workloads contain serverless compute, especially AWS Lambda, how do I use Amazon CloudWatch to gain insights into the environment?</p> <p>Customers can use CloudWatch Lambda Insights for monitoring and troubleshooting serverless applications running on AWS Lambda. CloudWatch Lambda Insights collects, aggregates, and summarizes system-level metrics including CPU time, memory, disk, and network &amp; also collects, aggregates, and summarizes diagnostic information such as cold starts and Lambda worker shutdowns to help customers isolate issues with Lambda functions and resolve them quickly.</p> <p>Related AWS Observability Workshop: Lambda Insights</p> <p>I aggregate lot of logs into Amazon CloudWatch logs, how do I gain observability into those data?</p> <p>CloudWatch Logs Insights enables customers to interactively search, analyze log data and have customers perform queries to efficiently and effectively respond to operational issues in Amazon CloudWatch Logs. If an issue occurs, customers can use CloudWatch Logs Insights to identify potential causes and validate deployed fixes.</p> <p>How do I query logs in Amazon CloudWatch Logs?</p> <p>CloudWatch Logs Insights in Amazon CloudWatch Logs use a query language to query log groups.</p> <p>How do I manage logs stored in Amazon CloudWatch Logs for cost optimization, compliance retention or for additional processing?</p> <p>By default, LogGroupsAmazon CloudWatch Logs arekept indefinitely and never expire. Customers can adjust the retention policy of each log group to choose a retention period between one day and 10 years, depending up on how long they want to retain the logs to optimize cost or for compliance purposes.</p> <p>Customers can export log data from log groups to Amazon S3 bucket and use this data in custom processing and analysis, or to load onto other systems.</p> <p>Customers can also configure log groups in CloudWatch Logs to stream data to your Amazon OpenSearch Service cluster in near real-time through a CloudWatch Logs subscription. By doing so, it helps customers to perform interactive log analytics, real-time application monitoring, search, and more.</p> <p>My workloads generate logs which could have sensitive data, is there a way to protect them in Amazon CloudWatch?</p> <p>Customers can make use of Log data protection feature in CloudWatch Logs that helps customers define own rules and policies to automatically detect and mask sensitive data within logs that are collected from systems and applications.</p> <p>Related AWS Observability Workshop: Data Protection</p> <p>I would like to know anomaly bands or unexpected changes when it happens to my systems &amp; applications. How can Amazon CloudWatch alert me when it occurs.</p> <p>Amazon CloudWatch Anomaly Detection applies statistical and machine learning algorithms to continuously analyze single time series of systems and applications, determine normal baselines, and surface anomalies with minimal user intervention. The algorithms create an anomaly detection model that generates a range of expected values that represent normal metric behavior. Customers can create alarm based on the analysis of past metric data and a value set for the anomaly threshold.</p> <p>Related AWS Observability Workshop: Anomaly Detection</p> <p>I\u2019ve setup metric alarm in Amazon CloudWatch, however I\u2019m getting frequent alarm noises. How can I control and fine tune this?</p> <p>Customers can combine multiple alarms into alarm hierarchies as composite alarm to reduce alarm noise by triggering just once when multiple alarms fire simultaneously. Composite alarms support an overall state by helping customers in grouping resources like an application, AWS Region, or AZ.</p> <p>Related AWS Observability Workshop: Alarms</p> <p>My workload facing the internet is experiencing performance and availability issues, how do I troubleshoot?</p> <p>Amazon CloudWatch Internet Monitor provides visibility into how internet issues impact the performance and availability between your applications hosted on AWS and your end users. With Internet Monitor, you can quickly identify what's impacting your application's performance and availability, so that you can track down and address issues which can significantly reduce the time it takes to diagnose internet issues.</p> <p>I've my workload on AWS and I want to get notified even before the end users experience an impact or latency in accessing the application. How do I get better visibility and improve the observability of my customer facing workload?</p> <p>Customers can use Amazon CloudWatch Synthetics to create canaries, configurable scripts that run on a schedule, to monitor your endpoints and APIs. Canaries follow the same routes and perform the same actions as a customer, which makes it possible to continually verify end user experience even when there are no live traffic to your applications. Canaries help you discover issues even before your customers do. Canaries check the availability and latency of endpoints and can store load time data and screenshots of the UI as rendered by a headless Chromium browser.</p> <p>Related AWS Observability Workshop: CloudWatch Synthetics</p> <p>I've my workload on AWS and I want to observe end user experience by identifying client-side performance issues and action a faster resolution if there are any real-time issues.</p> <p>CloudWatch RUM can perform real user monitoring to collect and view client-side data about your web application performance from actual user sessions in near real time. This collected data helps quickly identify and debug client-side performance issues and also helps to visualize and analyze page load times, client-side errors, and user behavior. When viewing this data, customers can see it all aggregated together and also see breakdowns by the browsers and devices that your customers use. CloudWatch RUM helps visualize anomalies in your application performance and find relevant debugging data such as error messages, stack traces, and user sessions.</p> <p>Related AWS Observability Workshop: CloudWatch RUM</p> <p>My Organization requires all actions be recorded for audits. Can Amazon CloudWatch events be recorded?</p> <p>Amazon CloudWatch is integrated with AWS CloudTrail, which provides a record of actions taken by a user, a role, or an AWS service in Amazon CloudWatch. CloudTrail captures all API calls for Amazon CloudWatch as events that include calls from the console and code calls to API operations.</p> <p>What more information is available?</p> <p>For additional information customers can read the AWS Documentation for CloudWatch, CloudWatch Events and CloudWatch Logs, go through the AWS Observability Workshop on AWS Native Observability and also check the product page to know the features, and pricing details. Additional tutorials on CloudWatch illustrating customer use case scenarios.</p> <p>Product FAQ: https://aws.amazon.com/cloudwatch/faqs/</p>"},{"location":"faq/faq/","title":"General - FAQ","text":""},{"location":"faq/faq/#how-are-logs-different-from-traces","title":"How are logs different from traces?","text":"<p>Logs are limited to a single application and the events that relate to it. For example, if a user logs into a web site hosted on a microservices platform, and makes a purchase on this site, there may be logs related to that user emitted from multiple applications:</p> <ol> <li>A front-end web server</li> <li>Authentication service</li> <li>The inventory service</li> <li>A payment processing backend</li> <li>Outbound mailer that sends the user a receipt</li> </ol> <p>Every one of these may log something about this user, and that data is all valuable. However, traces will present a single, cohesive view of the user's entire interaction across that single transaction, spanning all of these discrete components.</p> <p>In this way, a trace a collection of events from multiple services intended to show a single view of an activity, whereas logs are bound to the context of the application that created them.</p>"},{"location":"faq/faq/#what-signal-types-are-immutable","title":"What signal types are immutable?","text":"<p>All three of the basic signal types (metrics, logs, and traces) are immutable, though some implementations have greater or lesser assurance of this. For example, immutability of logs is a strict requirement in many governance frameworks - and many tools exist to ensure this. Metrics and traces should likewise always be immutable. </p> <p>This leads to a question as to handling \"bad data\", or data that was inaccurate. With  AWS observability services, there is no facility to delete metrics or traces that were emitted in error. CloudWatch Logs does allow for the deletion of an entire log stream, but you cannot retroactively change data once it has been collected. This is by design, and an important feature to ensure customer data is treated with the utmost care.</p>"},{"location":"faq/faq/#why-does-immutability-matter-for-observability","title":"Why does immutability matter for observability?","text":"<p>Immutability is paramount to observability! If past data can be modified then you would lose critical errors, or outliers in behaviour, that inform your choices when evolving your systems and operations. For example, a metric datapoint that shows a large gap in time does not simply show a lack of data collection, it may indicate a much larger issue in your infrastructure. Likewise, with \"null\" data - even empty timeseries are valuable.</p> <p>From a governance perspective, changing application logs or tracing after the fact violates the principal of non-reputability, where you would not be able to trust that the data in your system is precisely as it was intended be by the source application. </p>"},{"location":"faq/faq/#what-is-a-blast-radius","title":"What is a blast radius?","text":"<p>The blast radius of a change is the potential damage that it can create in your environment. For example, if you make a database schema change then the potential risk could include the data in the database plus all of the applications that depend on it.</p> <p>Generally speaking, working to reduce the blast radius of a change is a best practice, and breaking a change into smaller, safer, and reversible chunks is always recommended wherever feasible.</p>"},{"location":"faq/faq/#what-is-a-cloud-first-approach","title":"What is a \"cloud first\" approach?","text":"<p>Cloud-first strategies are where organization move all or most of their infrastructure to cloud-computing platforms. Instead of using physical resources like servers, they house resources in the cloud. </p> <p>To those used to co-located hardware, this might seem radical. However, the opposite is also true. Developers who adopt a cloud-first mentality find the idea of tying your servers to a physical location unthinkable. Cloud-first teams don\u2019t think of their servers as discrete pieces of hardware or even virtual servers. Instead, they think of them as software to fulfill a business function.</p> <p>Cloud-first is to the 2020's what mobile-first was to the 2010's, and virtualization was to the early 2000's. </p>"},{"location":"faq/faq/#what-is-technical-debt","title":"What is technical debt?","text":"<p>Taken from Wikipedia:</p> <p>In software development, technical debt (also known as design debt or code debt) is the implied cost of additional rework caused by choosing an easy (limited) solution now instead of using a better approach that would take longer.</p> <p>Basically, you accumulate debt over time as you add more to your workload without removing legacy code, applications, or human processes. Technical debt detracts from your absolute productivity. </p> <p>For example, if you have to spend 10% of your time performing maintenance on a legacy system that provides little or no direct value to your business, then that 10% is a cost that you pay. Reduction of technical debt equals increasing effective time to create new products that add value. </p>"},{"location":"faq/faq/#what-is-the-separation-of-concerns","title":"What is the separation of concerns","text":"<p>In the context of observability solutions, the separation of concerns means to divide functional areas of a workload or an application into discrete components that are independently managed. Each component addresses a separate concern (such as log structure and the emitting of logs). Controlling configuration of a component without modifying the underlying code means that developers can focus on their concerns (application functionality and feature development), while DevOps personas can focus on optimizing system performance and troubleshooting.</p> <p>Separation of concerns is a core concept in computer science.</p>"},{"location":"faq/faq/#what-is-operational-excellence","title":"What is operational excellence?","text":"<p>Operational excellence is the performance of best practices that align with operating workloads. AWS has an entire framework dedicated to being Well-Architected. See this page to get started with operational excellence.</p>"},{"location":"faq/x-ray/","title":"AWS X-Ray - FAQ","text":"<ol> <li>Does AWS Distro for Open Telemetry (ADOT) support trace propagation across AWS services such as Event Bridge or SQS?     Technically, that\u2019s not ADOT but AWS X-Ray. We are working on expanding the number and types of AWS services that propagate and/or generate spans. If you have a use case depending on this, please reach out to us.</li> <li>Will I be able to use the W3C trace header to ingest spans into AWS X-Ray using ADOT?     Yes, later in 2023. We\u2019re working on supporting W3C trace context propagation. </li> <li>Can I trace requests across Lambda functions when SQS is involved in the middle?     Yes. X-Ray now supports tracing across Lambda functions when SQS is involved in the middle. Traces from upstream message producers are automatically linked to traces from downstream Lambda consumer nodes, creating an end-to-end view of the application.</li> <li>Should I use X-Ray SDK or the OTel SDK to instrument my application?     OTel offers more features than the X-Ray SDK, but to choose which one is right for your use case see Choosing between ADOT and X-Ray SDK</li> <li>Are span events supported in AWS X-Ray?  Span events do not fit into the X-Ray model and are hence dropped.</li> <li>How can I extract data out of AWS X-Ray?  You can retrieve Service Graph, Traces and Root cause analytics data using X-Ray APIs.</li> <li>Can I achieve 100% sampling? That is, I want all traces to be recorded without sampling at all.  You can adjust the sampling rules to capture significantly increased amount of trace data. As long as the total segments sent do not breach the service quota limits mentioned here, X-Ray will make an effort to collect data as configured. There is no guarantee that this will result in 100% trace data capture as a result.</li> <li>Can I dynamically increase or decrease sampling rules through APIs? Yes, you can use the X-Ray sampling APIs to make adjustments dynamically as necessary. See this blog for a use-case based explanation.</li> <li>Product FAQ https://aws.amazon.com/xray/faqs/</li> </ol>"},{"location":"guides/","title":"Best practices overview","text":"<p>Observability is a broad topic with a mature landscape of tools. Not every tool is right for every solution though! To help you navigate through your observability requirements, configuration, and final deployment, we have summarized five key best practices that will inform your decision making process on your Observability strategy.</p>"},{"location":"guides/#monitor-what-matters","title":"Monitor what matters","text":"<p>The most important consideration with observability is not your servers, network, applications, or customers. It is what matters to you, your business, your project, or your users.</p> <p>Start first with what your success criteria are. For example, if you run an e-commerce application, your measures of success may be number of purchases made over the past hour. If you run a non-profit, then it may be donations vs. your target for the month. A payment processor may watch for transaction processing time, whereas universities would want to measure student attendance.</p> <p>Tip</p> <p>Success metrics are different for everyone! We may use an e-commerce application as an example here, but your projects can have a very different measurement. Regardless, the advice remains the same: know what good looks like and measure for it.</p> <p>Regardless of your application, you must start with identifying your key metrics. Then work backwards<sup>1</sup> from that to see what impacts it from an application or infrastructure perspective. For example, if high CPU on your web servers endangers customer satisfaction, and in-turn your sales, then monitoring CPU utilization is important!</p>"},{"location":"guides/#know-your-objectives-and-measure-them","title":"Know your objectives, and measure them!","text":"<p>Having identified your important top-level KPIs, your next job is to have an automated way to track and measure them. A critical success factor is doing so in the same system that watches your workload's operations. For our e-commerce workload example this may mean:</p> <ul> <li>Publishing sales data into a time series</li> <li>Tracking user registrations in this same system</li> <li>Measure how long customers stay on web pages, and (again) push this data to a time series</li> </ul> <p>Most customers have this data already, though not necessarily in the right places from an observability perspective. Sales data can typically be found in relational databases or business intelligence reporting systems, along with user registrations. And data from visit duration can be extracted from logs or from Real User Monitoring.</p> <p>Regardless of your metric data's original location or format, it must be maintained as a time series. Every key metric that matters most to you, whether it is business, personal, academic, or for any other purpose, must be in a time series format for you to correlate it with other observability data (sometimes known as signals or telemetry).</p> <p> Figure 1: example of a time series</p>"},{"location":"guides/#context-propagation-and-tool-selection","title":"Context propagation and tool selection","text":"<p>Tool selection is important and has a profound difference in how you operate and remediate problems. But worse than choosing a sub-optimal tool is tooling for all basic signal types. For example, collecting basic logs from a workload, but missing transaction traces, leaves you with a gap. The result is an incohesive view of your entire application experiece. All modern approaches to observability depend on \"connecting the dots\" with application traces.</p> <p>A complete picture of your health and operations requires tools that collect logs, metrics, and traces, and then performs correlation, analysis, anomaly detection, dashboarding, alarms and more.</p> <p>Info</p> <p>Some observability solutions may not contain all of the above but are intended to augment, extend, or give added value to existing systems. In all cases, tool interoperability and extensibility is an important consideration when beginning an observability project.</p>"},{"location":"guides/#every-workload-is-different-but-common-tools-make-for-a-faster-results","title":"Every workload is different, but common tools make for a faster results","text":"<p>Using a common set of tools across every workload has add benefits such as reducing operational friction and training, and generally you should strive for a reduced number of tools or vendors. Doing so lets you rapidly deploy existing observability solutions to new environments or workloads, and with faster time-to-resolution when things go wrong.</p> <p>Your tools should be broad enough to observe every tier of your workload: basic infrastructure, applications, web sites, and everything in between. In places where a single tool is not possible, the best practice is to use those that have an open standard, are open source, and therefore have the broadest cross-platform integration possibilities.</p>"},{"location":"guides/#integrate-with-existing-tools-and-processes","title":"Integrate with existing tools and processes","text":"<p>Don't reinvent the wheel! \"Round\" is a great shape already, and we should always be building collaborative and open systems, not data silos.</p> <ul> <li>Integrate with existing identity providers (e.g. Active Directory, SAML based IdPs).</li> <li>If you have existing IT trouble tracking system (e.g. JIRA, ServiceNow) then integrate with it to quickly manage problems as they arise.</li> <li>Use existing workload management and escalation tools (e.g. PagerDuty, OpsGenie) if you already have them!</li> <li>Infrastructure as code tools such as Ansible, SaltStack, CloudFormation, TerraForm, and CDK are all great tools. Use them to manage your observability as well as everything else, and build your observability solution with the same infrastructure as code tools you already use today (see include observability from day one).</li> </ul>"},{"location":"guides/#use-automation-and-machine-learning","title":"Use automation and machine learning","text":"<p>Computers are good at finding patterns, and at finding when data does not follow a pattern! If you have hundreds, thousands, or even millions of datapoints to monitor, then it would impossible to understand healthy thresholds for every single one of them. But many observability solutions have anomaly detection and machine learning capabilities that manage the undifferentiated heavy lifting of baselining your data.</p> <p>We refer to this as \"knowing what good looks like\". If you have load-tested your workload thoroughly then you may know these healthy performance metrics already, but for a complex distributed application it can be unwieldy to create baselines for every metric. This is where anomaly detection, automation, and machine learning are invaluable.</p> <p>Leverage tools that manage the baselining and alerting of applications health on your behalf, thereby letting you focus on your goals, and monitor what matters.</p>"},{"location":"guides/#collect-telemetry-from-all-tiers-of-your-workload","title":"Collect telemetry from all tiers of your workload","text":"<p>Your applications do not exist in isolation, and interactions with your network infrastructure, cloud providers, internet service providers, SasS partners, and other components both within and outside your control can all impact your outcomes. It is important that you have a holistic view of your entire workload.</p>"},{"location":"guides/#focus-on-integrations","title":"Focus on integrations","text":"<p>If you have to pick one area to instrument, it will undoubtedly be your integrations between components. This is where the power of observability is most evident. As a rule, every time one component or service calls another, that call must have at least these data points measured:</p> <ol> <li>The duration of the request and response</li> <li>The status of the response</li> </ol> <p>And to create the cohesive, holistic view that observability requires, a single unique identier for the entire request chain must be included in the signals collected.</p>"},{"location":"guides/#dont-forget-about-the-end-user-experience","title":"Don't forget about the end-user experience","text":"<p>Having a complete view of your workload means understanding it at all tiers, including how your end users experience it. Measuring, quantifying, and understanding when your objectives are at risk from a poor user experience is just as important as watching for free disk space or CPU utilization - if not more important!</p> <p>If your workloads are ones that interact directly with the end user (such as any application served as a web site or mobile app) then Real User Monitoring monitors not just the \"last mile\" of delivery to the user, but how they actually have experienced your application. Ultimately, none of the observability journey matters if your users are unable to actually use your services.</p>"},{"location":"guides/#data-is-power-but-dont-sweat-the-small-stuff","title":"Data is power, but don't sweat the small stuff","text":"<p>Depending on the size of your application, you may have a very large number of components to collect signals from. While doing so is important and empowering, there can be diminished returns from your efforts. This is why the best practice is to start by monitoring what matters, use this as a way to map your important integrations and critical components, and focus on the right details.</p>"},{"location":"guides/#include-observability-from-day-one","title":"Include observability from day one","text":"<p>Like security, observability should not be an afterthought to your development or operations. The best practice is to put observability early in your planning, just like security, which creates a model for people to work with and reduces opaque corners of your application. Adding transaction tracing after major development work is done takes time, even with auto-instrumentation. The effort returns far greater returns! But doing so late in your development cycle may create some rework.</p> <p>Rather than bolting observability in your workload later one, use it to help accelerate your work. Proper logging, metric, and trace collection enables faster application development, fosters good practices, and lays the foundation for rapid problem solving going forward.</p> <ol> <li> <p>Amazon uses the working backwards process extensively as a way to obsession over our customers and their outcomes, and we highly recommend that anyone working on observability solutions work backwards from their own objectives in the same way. You can read more about working backwards on Werner Vogels's blog.\u00a0\u21a9</p> </li> </ol>"},{"location":"guides/apm/","title":"Application Performance Monitoring","text":""},{"location":"guides/choosing-a-tracing-agent/","title":"Choosing a tracing agent","text":""},{"location":"guides/choosing-a-tracing-agent/#choose-the-right-agent","title":"Choose the right agent","text":"<p>AWS directly supports two toolsets for trace collection (plus our wealth of observability partners: </p> <ul> <li>The AWS Distro for OpenTelemetry, commonly called ADOT</li> <li>The X-Ray SDKs and daemon</li> </ul> <p>The selection of which tool or tools to use is a principal decision you must make as you evolve your observability solution. These tools are not mutually-exclusive, and you can mix them together as necessary. And there is a best practice for making this selection. However, first you should understand the current state of OpenTelemetry (OTEL).</p> <p>OTEL is the current industry standard specification for observabillity signalling, and contains definitions for each of the three core signal types: metrics, traces, and logs. However, OTEL has not always existed and has evolved out of earlier specifications such as OpenMetrics and OpenTracing. Observability vendors began openly supporting OpenTelemetry Line Protocol (OTLP) in recent years. </p> <p>AWS X-Ray and CloudWatch pre-date the OTEL specification, as do other leading observability solutions. However, the AWS X-Ray service readily accepts OTEL traces using ADOT. ADOT has the integrations already built into it to emit telemetry into X-Ray directly, as well as to other ISV solutions.</p> <p>Any transaction tracing solution requires an agent and an integration into the underlying application in order to collect signals. And this, in turn, creates technical debt in the form of libraries that must be tested, maintained, and upgraded, as well as possibly retooling if you choose to change your solution in the future.</p> <p>The SDKs included with X-Ray are part of a tightly integrated instrumentation solution offered by AWS. ADOT is part of a broader industry solution in which X-Ray is only one of many tracing solutions. You can implement end-to-end tracing in X-Ray using either approach, but it\u2019s important to understand the differences in order to determine the most useful approach for you.</p> <p>Success</p> <p>We recommend instrumenting your application with the AWS Distro for OpenTelemetry if you need the following:</p> <ul> <li> <p>The ability to send traces to multiple different tracing backends without having to re-instrument your code. For example, of you wish to shift from using the X-Ray console to Zipkin, then only configuration of the collector would change, leaving your applicaiton code untouched.</p> </li> <li> <p>Support for a large number of library instrumentations for each language, maintained by the OpenTelemetry community. </p> </li> </ul> <p>Success</p> <p>We recommend choosing an X-Ray SDK for instrumenting your application if you need the following:</p> <ul> <li> <p>A tightly integrated single-vendor solution.</p> </li> <li> <p>Integration with X-Ray centralized sampling rules, including the ability to configure sampling rules from the X-Ray console and automatically use them across multiple hosts, when using Node.js, Python, Ruby, or .NET</p> </li> </ul>"},{"location":"guides/containers/","title":"Containers","text":""},{"location":"guides/containers/#choosing-a-tracing-agent","title":"Choosing a tracing agent","text":""},{"location":"guides/dashboards/","title":"Dashboarding","text":""},{"location":"guides/dashboards/#best-practices","title":"Best practices","text":""},{"location":"guides/dashboards/#create-views-for-your-important-personas","title":"Create views for your important personas","text":""},{"location":"guides/dashboards/#share-your-business-metrics-along-side-operational-ones","title":"Share your business metrics along side operational ones","text":""},{"location":"guides/dashboards/#dashboards-should-build-bridges-not-walls","title":"Dashboards should build bridges, not walls","text":""},{"location":"guides/dashboards/#your-dashboards-should-tell-a-story","title":"Your dashboards should tell a story","text":""},{"location":"guides/dashboards/#dashboards-should-be-available-to-technical-and-non-technical-users","title":"Dashboards should be available to technical and non-technical users","text":""},{"location":"guides/dashboards/#recommendations","title":"Recommendations","text":""},{"location":"guides/dashboards/#leverage-existing-identity-providers","title":"Leverage existing identity providers","text":""},{"location":"guides/full-stack/","title":"Full-stack","text":""},{"location":"guides/hybrid-and-multicloud/","title":"Best practices for hybrid and multicloud","text":""},{"location":"guides/hybrid-and-multicloud/#intro","title":"Intro","text":"<p>We consider multicloud to be the concurrent use of more than one cloud services provider to operate your own workloads, and hybrid is the extending of your workloads across both on-premises and cloud environments. Observability across hybrid and multicloud environments may add significant complexity due to tool diversity, latency, and heterogenous workloads. Regardless, this remains a common goal for both development and business users. A rich ecosystem of products and services address this.</p> <p>However, the usefulness of observability tools for cloud-native workloads can vary dramatically. Consider the different requirements of monitoring a containerized batch processing workload, compared to a real-time banking application using a serverless framework: both have logs, metrics, and traces; however, the toolchain for observing them will vary, with a number of cloud-native, open source, and ISV products available. An open-source tool such as Prometheus may be an excellent fit for one, whereas a cloud-native tool provided as a managed service could better meet your requirements.</p> <p>Add to this the complexity of multicloud and hybrid, and gaining insights from your applications becomes considerably harder.</p> <p>In order to deal with these added dimensions and facilitate approaches to observability, customers tend to invest in a single toolchain with a unified interface. After all, reducing the signal-to-noise ratio is usually a good thing! However, a single approach does not work for all use cases, and the operating models of various platforms may add confusion. Our goal is to help you make informed decisions that compliment your needs and reduce your mean time to remediation when issues do occur. Below are the best practices that we have learned through working with customers of all sizes, and across every industry.</p> <p>Info</p> <p>These best practices are intended for a broad set of roles: enterprise architects, developers, DevOps, and more. We suggest evaluating them through the lens of your organization\u2019s business needs, and how observability in distributed environments can provide as much value as possible.</p>"},{"location":"guides/hybrid-and-multicloud/#dont-let-your-tooling-dictate-your-decisions","title":"Don\u2019t let your tooling dictate your decisions","text":"<p>Your applications, tools, and processes exist to help achieve business outcomes, such as increasing sales and customer satisfaction. A well-advised technology strategy is one that does everything possible to help you achieve those business goals. But the things that help you get there are simply tools, and they are meant to support your strategy \u2013 not be the strategy. To make an analogy, if you needed to build a house, you would not ask your tools how to design and build it. Rather, your tools are a means to an end.</p> <p>In a single, homogeneous environment, the decisions around tooling are easier. After all, if you run a single application in one environment, then you tooling can easily be the same across the board. But for hybrid and multicloud environments things are less clear, and keeping an eye on your business outcomes - and the value added by observing your workloads across these environments - is critical. Each Cloud Services Provider (CSP) has their own native observability solutions, and a rich set of partner and Independent Software Vendors (ISVs) whom you can use as well.</p> <p>Just because you operate in multiple environments does not mean that a single tool for every workload is advisable, nor even recommended. This can potentially mean using multiple services, frameworks, or providers, to observe your workloads. See \"a single pane of glass is less important than your workload\u2019s context\u201d below for details of how your operating model needs to reflect your needs. Regardless, when implementing your tools, remember to create \u201ctwo-way doors\u201d so you can evolve your observability solution in the future.</p> <p>Here are some examples of \u201ctool-first\u201d outcomes to avoid:</p> <ol> <li>Focusing on implementation of a single tool without a two-way door to upgrade it, or move to a new solution in the future, may create technical debt that is otherwise avoidable. This can happen when the tool is the solution, and one day may become the problem you need to solve.</li> <li>A company standard to use a single tool due to a volume discount may end-up without features they would benefit from. This may be \u201ccost over quality\u201d, and inadvertently creates a monolithic anti-pattern. This may discourage the collection of telemetry in order to remain under a volume threshold, thereby de-incentivizing the use of observability tooling.</li> <li>Not collecting an entire type of telemetry (usually traces) due to a lack of existing trace collection infrastructure, but a rich set of log and metric collectors, can lead to an incomplete observability solution.</li> <li>Support staff having been trained on only a single toolchain, in the desire to reduce labour and training costs, thereby reducing the potential value of other observability patterns.</li> </ol> <p>Success</p> <p>If your tooling is dictating your observability strategy, then you need to invert the approach. Tools are meant to enable and empower observability, not to limit your choices.</p> <p>Success</p> <p>Tool sprawl is a very real issue that companies struggle with, however a radical shift to a singular toolchain can likewise reduce your observability solution\u2019s usefulness. Hybrid and multicloud workloads have technologies that are unique to each platform, and higher-level services from each CSP are useful \u2013 though the trade-offs in using a single-source product require a value-based analysis. See \u201cInvest in OpenTelemetry\u201d for an approach that mitigates some of these risks.</p>"},{"location":"guides/hybrid-and-multicloud/#observability-data-has-gravity","title":"(Observability) data has gravity","text":"<p>All data has gravity \u2013 which is to say that it attracts workloads, solutions, tools, people, processes, and projects around it. For example, a database with your customer transactions in it will be the attractive force that brings compute and analytics workloads to it. This has direct implications for where you place your workloads, in which environment, and how you operate them going-forward. And the same is true for observability signals, though the gravity this data creates is tied to your workload and organizational context (see \"a single pane of glass is less important than your workload\u2019s context\u201d).</p> <p>One cannot completely separate the context of your observability telemetry from the underlying workload and data that it relates to. The same rule applies here: your telemetry is data, and it has gravity to it. This should influence where you place your telemetry agents, collectors, or systems that aggregate and analyze signals.</p> <p>Tip</p> <p>The value of observability data over time is considerably less than most other data types. You could call it the \u201chalf-life\u201d of observability data. Consider the additional latency in relaying telemetry to another environment as a potential forced devaluation of this data prior to its potential use, and then weigh that against the requirements you have for alerting when issues occur.</p> <p>Success</p> <p>The best practice is to emit data between environments only when there is business value to be gained from this aggregation. Having a single source for querying data does not solve many business needs on its own, and may create a more expensive solution than desired, with more points of failure.</p>"},{"location":"guides/hybrid-and-multicloud/#a-single-pane-of-glass-is-less-important-than-your-workloads-context","title":"A single pane of glass is less important than your workload\u2019s context","text":"<p>A common ask is for a \u201csingle pane of glass\u201d to observe all of your workloads. This arises from a natural desire to view as much data as possible, but in as simple a way as can be achieved, and reduce churn, frustration, and diagnosis time. Creating this one interface to see your entire observability solution at once is useful, but can come with the trade-off of separating your telemetry from the context it came from.</p> <p>For example, a dashboard with the CPU utilization of a hundred servers may show some anomalous spikes in consumption, but this does nothing to explain why this has happened, or what the contributing factors are for this behavior. And the importance of this metric may not be immediately clear.</p> <p>We have seen customers sometimes pursue the single pane of glass so aggressively that all business context is lost, and trying to see everything in one tool can actually dilute the value of that data. Your dashboards, and your tools, need to tell a story. And this story needs to include the business metrics and outcomes that are impacted by events in your workloads.</p> <p>Moreover, your tooling needs to align to your operating model. A single pane of glass can add value when your support teams are global with access to all of your environments, but if they are limited to only accessing a single workload, in a single CSP or hybrid environment, then there is no value added through this approach. In these instances, allowing teams to create dashboards within each environment natively may hasten time to value, and be more flexible changes in the future.</p> <p>Success</p> <p>The value of observability data is deeply integrated into the application from which it came. Your telemetry requires contextual awareness that comes from its environment. In hybrid and multicloud environments, the differences between technologies makes the need for context even greater (though systems such as Kubernetes can be similar between different cloud providers and on-premises).</p> <p>Success</p> <p>When building a single pane of glass for distributed system, display your business metrics and Service Level Objectives (SLOs) in the same view as other data (such as infrastructure metrics) that contributes to these SLOs. This gives context that may otherwise be lacking.</p> <p>Tip</p> <p>A single pane of glass can help to rapidly diagnose issues and reduce Time to Detection (MTTD) and thereby Mean Time to Resolution (MTTR), but only if the meaning of telemetry data can be preserved. Without this, a single pane of glass approach can increase the time to value, or become a net-negative for operations teams.</p> <p>Success</p> <p>If the value of a single pane of glass cannot be determined, or if workloads are bound entirely to a single CSP or on-premises environment, consider only rolling-up top-level business metrics to a single pane of glass, leaving the raw metrics and other contributing factors in their original environments.</p>"},{"location":"guides/hybrid-and-multicloud/#invest-in-opentelemetry","title":"Invest in OpenTelemetry","text":"<p>Across the observability vendor landscape, OpenTelemetry (OTel) has become the de-facto standard. OTEL can marshal each of your telemetry types into one or many collectors, which can include cloud-native services, or a wide variety of SaaS and ISV products. OTel agents and collectors communicate using the OpenTelemetry Protocol (OTLP), which encapsulates signals into a format allowing a wide variety of deployment patterns.</p> <p>To collect transaction traces with the most value, and with your business and infrastructure context, you will need to integrate trace collection into your application. Some auto-instrumentation agents can perform this with almost no effort. However, the most sophisticated use cases do require code changes on your behalf to support transaction traces. This creates some technical debt and ties-down your workload to a particular technology.</p> <p>OTel captures logs, metrics, and traces using a concept of a span. Spans contain these signals grouped together from a single transaction, packaging them into a contextualized, searchable object. This means you can view your signals from a single application event in one simple entity. For example, a user logging into a web site, and the requests this creates to all the downstream services this integrates with, can be presented as a single span.</p> <p>Tip</p> <p>OTel is not limited to application traces, and is widely used for logs and metrics. And many ISV products accept OTLP directly today.</p> <p>Success</p> <p>By instrumenting your applications using OTel, you remove the need to replace this instrumentation at the application layer in the future, should you choose to move from one observability platform to another. This turns part of your observability solution into a two-way door.</p> <p>Success</p> <p>OTel is future-proofing, scalable, and makes it easier to change your collection and analysis systems in the future without having to change application code, making it an efficient shift to the left.</p>"},{"location":"guides/observability-maturity-model/","title":"AWS Observability Maturity Model","text":""},{"location":"guides/observability-maturity-model/#introduction","title":"Introduction","text":"<p>At its core, observability is the ability to understand and gain insights into the internal state of a system by analyzing its external outputs. This concept has evolved from traditional monitoring approaches that focus on predefined metrics or events, to a more holistic approach that encompasses the collection, analysis, and visualization of data generated by various components in an environment. A system cannot be controlled or optimized unless it is observed. An effective observability strategy allows teams to quickly identify and resolve issues, optimize resource usage, and gain insights into the overall health of their systems. Observability gives the ability to efficiently detect, investigate and remediate issues that can and should improve overall operational availability and the health of the workloads.</p> <p></p> <p>The difference between Monitoring and Observability is that Monitoring tells whether a system is working or not, while Observability tells why the system isn\u2019t working. Monitoring is usually a reactive measure whereas the goal of Observability is to be able to improve your Key Performance Indicators (KPIs) in a proactive manner. Continuous Monitoring &amp; Observability increases agility, improves customer experience and reduces risk in the cloud environment.</p>"},{"location":"guides/observability-maturity-model/#observability-maturity-model","title":"Observability maturity model","text":"<p>The observability maturity model serves as an essential framework for organizations looking to optimize their workload observability and management processes. This model provides a comprehensive roadmap for businesses to assess their current capabilities, identify areas for improvement, and strategically invest in the right tools and processes to achieve optimal observability. In the era of cloud computing, microservices, ephemeral and distributed systems, observability has become a critical factor in ensuring the reliability and performance of digital services. By providing a structured approach to improving observability, this model allows organizations to gain a more profound understanding and control over their systems, paving way for a more resilient, efficient, and high-performing business.</p>"},{"location":"guides/observability-maturity-model/#stages-of-observability-maturity-model","title":"Stages of Observability Maturity Model","text":"<p>As organizations expand their workloads, the observability maturity model is expected to mature as well. However, the path to the observability maturity doesn\u2019t always grow along with the workload. The intention is to help customers achieve the required maturity level as they expand and grow their organizational capabilities.</p> <ol> <li> <p>The first stage in the observability maturity model typically involves establishing a baseline understanding of the organization's current state. This entails assessing existing monitoring tools and processes, as well as identifying gaps in visibility or functionality. At this stage, organizations can take stock of their current capabilities and set realistic goals for improvement starting even at the early stages of engineering cycle.</p> </li> <li> <p>In the next stage, organizations move towards a more sophisticated approach by adopting advanced observability strategies and services. This may include implementing proactive alerting, distributed tracing to gain insights into the interactions between disparate systems, by which organizations can begin to reap the benefits of increased visibility, reduce cognitive load and more efficient troubleshooting.</p> </li> <li> <p>As businesses progress through the third stage of the observability maturity model, they can leverage additional capabilities such as automated remediation, artificial intelligence and machine learning technologies to automate anomaly detection and root cause analysis. These advanced features enable organizations to not only detect issues but also take corrective actions before they impact end-users or disrupt business operations. By integrating observability tools with other critical systems such as incident management platforms, organizations can streamline their incident response processes and minimize the time it takes to resolve issues.</p> </li> <li> <p>The final stage of the observability maturity model involves leveraging the wealth of data generated by monitoring and observability tools to drive continuous improvement. This can involve using advanced analytics to identify patterns and trends in workload performance, as well as feeding this information back into engineering and operations processes to optimize resource allocation, architecture, and deployment strategies.</p> </li> </ol> <p></p>"},{"location":"guides/observability-maturity-model/#stage1-foundational-monitoring-collecting-telemetry-data","title":"Stage1: Foundational monitoring - Collecting Telemetry Data","text":"<p>Adopted as the bare minimum and worked in siloes, basic monitoring has an undefined strategy of what is required to monitor the totality of the systems or workloads in an organization. Most of the time, different teams like application owners, Network Operations Center (NOC) or CloudOps or DevOps teams use different tools for their monitoring needs, hence this approach is of little value in terms of debugging across or for optimization of the environment.</p> <p>Typically, customers at this stage have disparate solutions for monitoring their workloads. Different teams, most of the time they gather same data in different ways since there is no or limited partnership with others. The teams tend to optimize what they need by working with the data they obtain. Also, teams cannot use each other\u2019s data since the data obtained from another team could be in a dissimilar format. Creating a plan to identify critical workloads, aiming for a unified solution for observability, defining metrics and logs are key aspects in this level. Designing your workload to capture the essential telemetry it provides is necessary to understand its internal state and the workload health.</p> <p>To build a foundation towards improving the maturity level, instrumenting workloads through collection of metrics, logs, traces and gaining meaningful insights using the right monitoring and observability tools help customers control and optimize the environment. Instrumentation refers to measuring, tracking and capturing key data from environments that can be used to observe the behavior and performance of workloads. Examples include application metrics such as errors, successful or non-successful transactions, and infrastructure metrics such as the utilization of CPU and disk resources.</p>"},{"location":"guides/observability-maturity-model/#stage-2-intermediate-monitoring-telemetry-analysis-and-insights","title":"Stage 2: Intermediate Monitoring - Telemetry Analysis and Insights","text":"<p>In this stage, customers see their organizations becoming clearer in terms of collecting signals from various environments like on-premise and cloud. They have devised mechanisms to collect metrics, logs and traces from workloads as these form the foundational structure of observability, created visualizations, alerting strategies and also have the ability to prioritize issues based on well-defined criteria. Instead of being reactive and guessing, customers have a workflow that invokes required actions and relevant teams are able to analyze and troubleshoot based on captured information and historical knowledge. Customers in this level work towards accomplishing practices for observability of their environments that could be traditional or modern, highly scalable, distributed, agile and microservices architecture.</p> <p></p> <p>Although monitoring seems to be working well in most cases, organizations tend to spend more time debugging issues and as a result the overall Mean Time-To-Resolution (MTTR) is not consistent or meaningfully improved over a period of time. Also, there is higher than expected cognitive time and effort to debug issues hence longer incident response. There tends to be a data overload situation that overwhelms operations as well. We find most enterprises being caught in this stage without realizing where they could go next. Specific actions that can be taken to move the organization to the next level are: 1) Review your systems\u2019 architecture designs at regular intervals and deploy policies and practices to reduce the impact and downtime leading to fewer alerts. 2) Prevent alert fatigue by defining actionable KPIs, add valuable context to the alert findings, categorizing by severity/urgency, sending to different tools and teams to help engineers resolve the issues faster.</p> <p>Analyze these alerts on a regular basis and automate remediation for common repeated alerts. Share and communicate the alert findings with relevant teams to provide feedback on operational and process improvement.</p> <p>Develop a plan to gradually build a knowledge graph that helps you correlate different entities and understand the dependencies between different parts of a system. It enables customers visualize the impact of changes to a system, helping to predict and mitigate potential issues.</p>"},{"location":"guides/observability-maturity-model/#stage-3-advanced-observability-correlation-and-anomaly-detection","title":"Stage 3: Advanced Observability - Correlation and Anomaly Detection","text":"<p>In this stage organizations are able to clearly understand the root cause of issues without having to spend a lot of time troubleshooting. When an issue arises, alerts provide enough contextual information to relevant teams like Network Operations Center (NOC) or CloudOps or DevOps teams. The monitoring team are able to look at an alert and immediately determine the root cause of the issue through correlation of signals like metrics, logs as well as traces. Traces are data collected from your application about requests that can be used with tools to view, filter, and gain insights to identify issues and opportunities for optimization. Traced requests of your application provides detailed information not only about the request and response, but also about calls that your application makes to downstream AWS resources, microservices, databases, and web APIs. They can look at a trace, find the corresponding log events as traces are captured and also look at metrics from the infrastructure and applications obtaining a 360\u00b0 view of the situation, they are in.</p> <p>Appropriate teams can take remedial actions at once by providing a fix that solves the issue. In this scenario, the MTTR is very small, the Service Level Objectives (SLO) are green, and the burn rate through the error budget is tolerable. Typically, customers in this level have accomplished practices for observability of their modern, agile, highly scalable and microservices environments.</p> <p>There are many organizations that have achieved this level of sophistication and maturity in their observability environments. This stage already gives organizations the ability to support complex infrastructure, operate their systems with high availability, provide higher Service Level Availability (SLA) for their applications and achieve business innovation by providing reliable infrastructure. Customers also use anomaly detectors to monitor anomalies &amp; outliers which do not match usual patterns and have near real time alerting mechanisms.</p> <p>However, teams in such organizations always want to go beyond the art of the possible. Teams would like to understand repeated issues, create a knowledge base that they can make use of to model against scenarios in order to predict issues that might arise in the future. That is when customers move to the next stage of the maturity model, in which they get insights into the unknown. In order to get there, new tools are needed and also new skills and techniques in storing and making use of the data needs to be identified. One can make use of Artificial intelligence for IT operations (AIOps) to create systems that automatically correlate signals, identify root cause, create resolution plans based on models trained using data collected in the past.</p> <p></p>"},{"location":"guides/observability-maturity-model/#stage-4-proactive-observability-automatic-and-proactive-root-cause-identification","title":"Stage 4: Proactive Observability - Automatic and Proactive Root Cause Identification","text":"<p>Here Observability data is not only used \u201cafter\u201d an issue occurs, rather makes use of the data in real-time \u201cbefore\u201d an issue occurs. Using well-trained models, issue identifications are made proactively and the resolutions are accomplished easier and simpler. By analyzing collected signals, the monitoring system is able to provide insights into the issue automatically and also lay out resolution option(s) to resolve the issue.</p> <p>Observability software vendors are continuously expanding their capabilities into this space and this has only accelerated with Generative AI becoming popular, so that organizations aspiring to achieve such maturity level can accomplish with ease. Once this stage matures and takes shape, customers see a situation where the observability services are able to automatically create dynamic dashboards. The dashboards can only contain information that is relevant to the issue on hand. This will save time and cost in querying and visualizing data that don\u2019t really matter. With Generative AI (GenAI) and compute to perform Machine Learning being democratized by the day, we may see proactive monitoring capabilities becoming more common in future than it is now.</p> <p>An overview of the observability portfolio providing a holistic picture, with various AWS native and open-source solutions for data Collection, data processing, data insight &amp; analysis which the customers can make use of by choosing appropriate solutions for their end-to-end observability needs.</p> <p></p>"},{"location":"guides/observability-maturity-model/#aws-well-architected-and-cloud-adoption-framework-for-observability","title":"AWS Well-Architected and Cloud Adoption Framework for Observability","text":"<p>Organizations can leverage AWS Well-Architected and Cloud Adoption Framework to enhance their observability capabilities and effectively monitor and troubleshoot their cloud environment.</p> <p>AWS Well-Architected and Cloud Adoption Framework for observability provides a structured approach for designing, deploying, and operating workloads, ensuring best practices are followed. This leads to improved availability, system performance, scalability and reliability. These frameworks also provide organizations with a standardized set of practices and prescriptive guidance, making it easier to collaborate, share knowledge and implement consistent solutions across the organization.</p> <p>To effectively leverage, organizations need to understand the key components called the pillars (operational excellence, security, reliability, performance efficiency, cost optimization and sustainability) of AWS Well-Architected framework, which provide a holistic approach for designing and operating cloud environment. On the other hand, the Cloud Adoption Framework provides a structured approach to cloud adoption, focusing on areas such as business, people, governance, and platform. By aligning these components with observability requirements, organizations can build robust and scalable workloads.</p> <p>Implementing AWS Well-Architected and Cloud Adoption Frameworks for observability involves a few steps. Firstly, organizations need to assess their current state and identify areas for improvement. This can be done by conducting an Observability Maturity Model assessment, which evaluates the workloads against these frameworks. Based on the review findings, organizations can prioritize and plan their observability initiatives. This includes defining monitoring and logging requirements, selecting appropriate AWS services, and implementing the necessary infrastructure and tools. Lastly, organizations need to continuously monitor and optimize their observability solutions to ensure ongoing effectiveness.</p> <p>Also, customers can utilize AWS Well-Architected Tool which is a service in AWS to document and measure their workload using the best practices of AWS Well-Architected Framework. This tool provides a consistent process for measuring their workloads through the pillars of AWS Well-Architected Framework, assisting with documenting the decisions that they make, providing recommendations for improving their workloads, and guiding them in making their workloads more reliable, secure, efficient, and cost-effective.</p>"},{"location":"guides/observability-maturity-model/#assessment","title":"Assessment","text":"<p>Observability Maturity Model assessment can be used to gauge your current state of observability and identify areas for improvement. An assessment of each stage involves evaluating existing monitoring and management practices across different teams, identifying gaps and areas for improvement, and determining the overall readiness for the next stage is imperative. A maturity assessment begins with business process outline, workload inventory &amp; tools discovery, identifying current challenges and understanding organization priorities and objectives.</p> <p>The assessment helps identify the targeted metrics and KPIs that lays the foundation for further development and optimization of the existing layout. The assessment of your Observability Maturity Model plays a crucial role in ensuring that your business is prepared to handle the complex, dynamic nature of modern systems. It aids in identifying blind spots and areas of weakness that could potentially lead to system failures or performance issues.</p> <p>Moreover, regular assessments ensure that your business remains agile and adaptable. It allows you to keep pace with evolving technologies and methodologies, thereby ensuring that your systems are always at the peak of efficiency and reliability.</p>"},{"location":"guides/observability-maturity-model/#building-the-observability-strategy","title":"Building the observability strategy","text":"<p>Once the organization has identified their observability stage, they should start to build the strategy to optimize the current processes &amp; tools and also start to work towards the maturity. Organizations want to ensure that their customers have a great customer experience, so they start with those customer requirements and work backwards from there. Then work with your stakeholders because they understand those requirements really well. With the aim for an observability strategy, organizations must first define their observability goals as they should be aligned with the overall business objectives and should clearly articulate what the organization aims to achieve through the strategy, providing a roadmap for building and implementing the observability plan.</p> <p>Next, organizations need to identify key metrics (KPIs) that will provide insights into system performance. These could range from latency and error rates to resource utilization and transaction volumes. It is important to note that the choice of metrics will largely depend on the nature of the business and its specific needs.</p> <p>Once the key metrics have been identified, organizations can then decide on the tools and technologies required for data collection. The choice of tool should be based on its alignment with the organization's goals, its ease of integration with existing systems, optimize cost, achieve scalability, meet customer needs and improve the overall customer experience.</p> <p>Finally, organizations should also encourage a culture that values observability. This involves training team members on the importance of observability, encouraging them to proactively monitor system performance, and fostering a culture of continuous learning and improvement. This strategy creates virtuous cycle of continuous process of collection, action and improvement for the best possible customer experience.</p> <p></p> <p>In summary, to build an observability strategy the three main aspects need to be considered: 1) what needs be collected 2) what are all the systems and workloads that need to be observed and 3/ how to react when there are issues and what mechanisms should be in place to remediate them.</p>"},{"location":"guides/observability-maturity-model/#conclusion","title":"Conclusion","text":"<p>The observability maturity model serves as a roadmap for organizations to assess their current state and seeking ways to improve their ability to understand, analyze, and respond to the behavior of workloads and infrastructure. By following a structured approach to assess current capabilities, adopt advanced monitoring techniques, and leverage data-driven insights, businesses can achieve a higher level of observability and make more informed decisions about their workloads and infrastructure. This model outlines the key capabilities and practices that organizations need to develop in order to progress through different levels of maturity, ultimately reaching to a state where they can fully leverage the benefits of proactive observability.</p>"},{"location":"guides/observability-maturity-model/#helpful-resources","title":"Helpful Resources","text":"<ul> <li>AWS Observability Best Practices</li> <li>What is observability and Why does it matter?</li> <li>How to develop an Observability strategy?</li> <li>Guidance for Deep Application Observability on AWS</li> <li>How Discovery increased operational efficiency with AWS observability - AWS re:Invent 2022</li> <li>Developing an observability strategy - AWS re:Invent 2022</li> <li>Explore Cloud Native Observability with AWS - AWS Virtual Workshop</li> <li>Increase availability with AWS observability solutions - AWS re:Invent 2020</li> <li>Observability best practices at Amazon - AWS re:Invent 2022</li> <li>Observability: Best practices for modern applications - AWS re:Invent 2022</li> <li>Observability the open-source way - AWS re:Invent 2022</li> <li>Elevate your Observability Strategy with AIOps</li> <li>Let\u2019s Architect! Monitoring production systems at scale</li> <li>Full-stack observability and application monitoring with AWS - AWS Summit SF 2022</li> </ul>"},{"location":"guides/strategy/","title":"Creating an observability strategy","text":""},{"location":"guides/containers/aws-native/ecs/best-practices-metrics-collection-1/","title":"Collecting system metrics with Container Insights","text":"<p>System metrics pertain to low-level resources that include physical components on a server such as CPU, memory, disks and network interfaces.  Use CloudWatch Container Insights to collect, aggregate, and summarize system metrics from containerized applications deployed to Amazon ECS. Container Insights also provides diagnostic information, such as container restart failures, to help isolate issues and resolve them quickly. It is available for Amazon ECS clusters deployed on EC2 and Fargate. </p> <p>Container Insights collects data as performance log events using embedded metric format. These performance log events are entries that use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, service and task level as CloudWatch metrics. </p> <p>Note</p> <p>For Container Insights metrics to appear in CloudWatch, you must enable Container Insights on your Amazon ECS clusters. This can be done either at the account level or at the individual cluster level. To enable at the account level, use the following AWS CLI command:</p> <pre><code>aws ecs put-account-setting --name \"containerInsights\" --value \"enabled\n</code></pre> <p>To enable at the individual cluster level, use the following AWS CLI command:</p> <pre><code>aws ecs update-cluster-settings --cluster $CLUSTER_NAME --settings name=containerInsights,value=enabled\n</code></pre>"},{"location":"guides/containers/aws-native/ecs/best-practices-metrics-collection-1/#collecting-cluster-level-and-service-level-metrics","title":"Collecting cluster-level and service-level metrics","text":"<p>By default, CloudWatch Container Insights collects metrics at the task, service and cluster level. The Amazon ECS agent collects these metrics for each task on an EC2 container instance (for both ECS on EC2 and ECS on Fargate) and sends them to CloudWatch as performance log events. You don't need to deploy any agents to the cluster. These log events from which the metrics are extracted are collected under the CloudWatch log group named /aws/ecs/containerinsights/$CLUSTER_NAME/performance. The complete list of metrics extracted from these events are documented here. The metrics that Container Insights collects are readily viewable in pre-built dashboards available in the CloudWatch console by selcting Container Insights from the navigation page and then selecting performance monitoring from the dropdown list. They are also viewable in the Metrics section of the CloudWatch console.</p> <p></p> <p>Note</p> <p>If you're using Amazon ECS on an Amazon EC2 instance, and you want to collect network and storage metrics from Container Insights, launch that instance using an AMI that includes Amazon ECS agent version 1.29.</p> <p>Warning</p> <p>Metrics collected by Container Insights are charged as custom metrics. For more information about CloudWatch pricing, see Amazon CloudWatch Pricing</p>"},{"location":"guides/containers/aws-native/ecs/best-practices-metrics-collection-1/#collecting-instance-level-metrics","title":"Collecting instance-level metrics","text":"<p>Deploying the CloudWatch agent to an Amazon ECS cluster hosted on EC2, allows you to collect instance-level metrics from the cluster. The agent is deployed as a daemon service and sends instance-level metrics as performance log events from each EC2 container instance in the cluster. The complete list of instance-level extracted from these events are documented here</p> <p>Info</p> <p>Steps to deploy the CloudWatch agent to an Amazon ECS cluster to collect instance-level metrics are documented in the Amazon CloudWatch User Guide. Note that this option is not availavble for Amazon ECS clusters that are hosted on Fargate.</p>"},{"location":"guides/containers/aws-native/ecs/best-practices-metrics-collection-1/#analyzing-performance-log-events-with-logs-insights","title":"Analyzing performance log events with Logs Insights","text":"<p>Container Insights collects metrics by using performance log events with embedded metric format. Each log event may contain performance data observed on system resources such as CPU and memory or ECS resources such as tasks and services. Examples of performance log events that Container Insights collects from an Amazon ECS at the cluster, service, task and container level are listed here. CloudWatch generates metrics based only on some of the performance data in these log events. But you can use these log events to perform a deeper analysis of the performance data using CloudWatch Logs Insights queries.</p> <p>The user interface to run Logs Insights queries is available in the CloudWatch console by selecting Logs Insights from the navigation page. When you select a log group, CloudWatch Logs Insights automatically detects fields in the performance log events in the log group and displays them in Discovered fields in the right pane. The results of a query execution are displayed as a bar graph of log events in this log group over time. This bar graph shows the distribution of events in the log group that matches your query and time range.</p> <p></p> <p>Info</p> <p>Here's a sample Logs Insights query to display container-level metrics for CPU and memory usage.</p> <pre><code>stats avg(CpuUtilized) as CPU, avg(MemoryUtilized) as Mem by TaskId, ContainerName | sort Mem, CPU desc\n</code></pre>"},{"location":"guides/containers/aws-native/ecs/best-practices-metrics-collection-2/","title":"Collecting service metrics with Container Insights","text":"<p>Service metrics are applicaton-level metrics that are captured by adding instrumentation to your code. These metrics can be captured from an application using two different approaches. </p> <ol> <li>Push approach: Here, an application sends the metrics data directly to a destination. For example, using the CloudWatch PutMetricData API, an application can publish metric data points to CloudWatch. An application may also send the data via gRPC or HTTP using the OpenTelemetry Protocol (OTLP) to an agent such as the OpenTelemetry Collector. The latter will then send the data the metrics data to the final destination.</li> <li>Pull approach: Here, the application exposes the metrics data at an HTTP endpoint in a pre-defined format. The data are then scraped by an agent that has access to this endpoint and then sent to the destination.</li> </ol> <p></p>"},{"location":"guides/containers/aws-native/ecs/best-practices-metrics-collection-2/#cloudwatch-container-insights-monitoring-for-prometheus","title":"CloudWatch Container Insights monitoring for Prometheus","text":"<p>Prometheus is a popular open-source systems monitoring and alerting toolkit. It has emerged as the de facto standard for collecting metrics using the pull approach from containerized applications. To capture metrics using Prometheus, you will have to first instrument your application code using the Prometheus client library which is available in all the major programming languages. Metrics are usually exposed by the application over HTTP, to be read by the Prometheus server. When Prometheus server scrapes your applications's HTTP endpoint, the client library sends the current state of all tracked metrics to the server. The server can either store the metrics in a local storage that it manages or send the metrics data to a remote destination such as CloudWatch.</p> <p>CloudWatch Container Insights monitoring for Prometheus enables you to leverage the capabilities of Prometheus in an Amazon ECS cluster. It is available for Amazon ECS clusters deployed on EC2 and Fargate The CloudWatch agent can be used as a drop-in replacement for a Prometheus server, reducing the number of monitoring tools required to improve observability. It automates the discovery of Prometheus metrics from containerized applications deployed to Amazon ECS and sends the metrics data to CloudWatch as performance log events. </p> <p>Info</p> <p>Steps to deploy the CloudWatch agent with Prometheus metrics collection on an Amazon ECS cluster are documented in the Amazon CloudWatch User Guide</p> <p>Warning</p> <p>Metrics collected by Container Insights monitoring for Prometheus are charged as custom metrics. For more information about CloudWatch pricing, see Amazon CloudWatch Pricing</p>"},{"location":"guides/containers/aws-native/ecs/best-practices-metrics-collection-2/#autodiscovery-of-targets-on-amazon-ecs-clusters","title":"Autodiscovery of targets on Amazon ECS clusters","text":"<p>The CloudWatch agent supports the standard Prometheus scrape configurations under the scrape_config section in the Prometheus documentation. Prometheus supports both static and dynamic discovery of scraping targets using one of the dozens of supported service-discovery mechanisms. . As Amazon ECS does not have any built-in service discovery mechanism, the agent relies on Prometheus' support for file-based discovery of targets. To setup the agent for file-based discovery of targets, the agent needs two configuration parameters, which are both defined in the task definition used for launching the agent. You can customize these parameters to have granular control over the metrics collected by the agent.</p> <p>The first parameter contains Prometheus global configuration that looks like the following sample:</p> <pre><code>global:\n  scrape_interval: 30s\n  scrape_timeout: 10s\nscrape_configs:\n  - job_name: cwagent_ecs_auto_sd\n    sample_limit: 10000\n    file_sd_configs:\n      - files: [ \"/tmp/cwagent_ecs_auto_sd.yaml\" ] \n</code></pre> <p>The second parameter contains configuration that helps the agent discover scraping targets. The agent periodically makes API calls to Amazon ECS to retrieve the metadata of the running ECS tasks that match the task definition patterns defined in the ecs_service_discovery section of this configurtion. All discovered targets are written into the result file /tmp/cwagent_ecs_auto_sd.yaml that resides on the file system mounted to CloudWatch agent container. The sample configuration below will result in the agent scraping metrics from all tasks that are named with the prefix BackendTask. Refer to the detaild guide for autodiscovery of targets in an Amazon ECS Cluster.</p> <pre><code>{\n   \"logs\":{\n      \"metrics_collected\":{\n         \"prometheus\":{\n            \"log_group_name\":\"/aws/ecs/containerinsights/{ClusterName}/prometheus\"\n            \"prometheus_config_path\":\"env:PROMETHEUS_CONFIG_CONTENT\",\n            \"ecs_service_discovery\":{\n               \"sd_frequency\":\"1m\",\n               \"sd_result_file\":\"/tmp/cwagent_ecs_auto_sd.yaml\",\n               \"task_definition_list\":[\n                  {\n                     \"sd_job_name\":\"backends\",\n                     \"sd_metrics_ports\":\"3000\",\n                     \"sd_task_definition_arn_pattern\":\".*:task-definition/BackendTask:[0-9]+\",\n                     \"sd_metrics_path\":\"/metrics\"\n                  }\n               ]\n            },\n            \"emf_processor\":{\n               \"metric_declaration\":[\n                  {\n                     \"source_labels\":[\n                        \"job\"\n                     ],\n                     \"label_matcher\":\"^backends$\",\n                     \"dimensions\":[\n                        [\n                           \"ClusterName\",\n                           \"TaskGroup\"\n                        ]\n                     ],\n                     \"metric_selectors\":[\n                        \"^http_requests_total$\"\n                     ]\n                  }\n               ]\n            }\n         }\n      },\n      \"force_flush_interval\":5\n   }\n}\n</code></pre>"},{"location":"guides/containers/aws-native/ecs/best-practices-metrics-collection-2/#importing-prometheus-metrics-into-cloudwatch","title":"Importing Prometheus metrics into CloudWatch","text":"<p>The metrics collected by the agent are sent to CloudWatch as performance log events based on the filtering rules specified in metric_declaration section of the configuration. This section is also used to specify the array of logs with embedded metric format to be generated. The sample configuration above will generate log events, as shown below, only for a metric named http_requests_total with the label job:backends. Using this data, CloudWatch will create the metric http_requests_total under the CloudWatch namespace ECS/ContainerInsights/Prometheus with the dimensions ClusterName and TaskGroup. <pre><code>{\n   \"CloudWatchMetrics\":[\n      {\n         \"Metrics\":[\n            {\n               \"Name\":\"http_requests_total\"\n            }\n         ],\n         \"Dimensions\":[\n            [\n               \"ClusterName\",\n               \"TaskGroup\"\n            ]\n         ],\n         \"Namespace\":\"ECS/ContainerInsights/Prometheus\"\n      }\n   ],\n   \"ClusterName\":\"ecs-sarathy-cluster\",\n   \"LaunchType\":\"EC2\",\n   \"StartedBy\":\"ecs-svc/4964126209508453538\",\n   \"TaskDefinitionFamily\":\"BackendAlarmTask\",\n   \"TaskGroup\":\"service:BackendService\",\n   \"TaskRevision\":\"4\",\n   \"Timestamp\":\"1678226606712\",\n   \"Version\":\"0\",\n   \"container_name\":\"go-backend\",\n   \"exported_job\":\"storebackend\",\n   \"http_requests_total\":36,\n   \"instance\":\"10.10.100.191:3000\",\n   \"job\":\"backends\",\n   \"path\":\"/popular/category\",\n   \"prom_metric_type\":\"counter\"\n}\n</code></pre></p>"},{"location":"guides/containers/aws-native/eks/amazon-cloudwatch-container-insights/","title":"Amazon CloudWatch Container Insights","text":"<p>In this section of Observability best practices guide, we will deep dive on to following topics related to Amazon CloudWatch Container Insights :</p> <ul> <li>Introduction to Amazon CloudWatch Container Insights</li> <li>Using Amazon CloudWatch Container Insights with AWS Distro for Open Telemetry</li> <li>Fluent Bit Integration in CloudWatch Container Insights for Amazon EKS</li> <li>Cost savings with Container Insights on Amazon EKS</li> <li>Using EKS Blueprints to setup Container Insights</li> </ul>"},{"location":"guides/containers/aws-native/eks/amazon-cloudwatch-container-insights/#introduction","title":"Introduction","text":"<p>Amazon CloudWatch Container Insights helps customers collect, aggregate, and summarize metrics and logs from containerized applications and microservices. Metrics data is collected as performance log events using the embedded metric format. These performance log events use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards. Container Insights are available for Amazon EKS clusters with self managed node groups, managed node groups and AWS Fargate profiles.</p> <p>From a cost optimization standpoint and to help you manage your Container Insights cost, CloudWatch does not automatically create all possible metrics from the log data. However, you can view additional metrics and additional levels of granularity by using CloudWatch Logs Insights to analyze the raw performance log events. Metrics collected by Container Insights are charged as custom metrics. For more information about CloudWatch pricing, see Amazon CloudWatch Pricing.</p> <p>In Amazon EKS, Container Insights uses a containerized version of the CloudWatch agent which is provided by Amazon via Amazon Elastic Container Registry to discover all of the running containers in a cluster. It then collects performance data at every tier of the performance stack. Container Insights supports encryption with the AWS KMS key for the logs and metrics that it collects. To enable this encryption, you must manually enable AWS KMS encryption for the log group that receives Container Insights data. This results in CloudWatch Container Insights encrypting this data using the provided AWS KMS key. Only symmetric keys are supported and asymmetric AWS KMS keys are not supported to encrypt your log groups. Container Insights are supported only in Linux instances. Container Insights for Amazon EKS is supported in the these AWS Regions.</p>"},{"location":"guides/containers/aws-native/eks/amazon-cloudwatch-container-insights/#using-amazon-cloudwatch-container-insights-with-aws-distro-for-open-telemetry","title":"Using Amazon CloudWatch Container Insights  with AWS Distro for Open Telemetry","text":"<p>We will now deep dive in to AWS Distro for OpenTelemetry (ADOT) which is one of the options to enable collection of Container insight metrics from Amazon EKS workloads. AWS Distro for OpenTelemetry (ADOT) is a secure, AWS-supported distribution of the OpenTelemetry project. With ADOT, users can instrument their applications just once to send correlated metrics and traces to multiple monitoring solutions. With ADOT support for CloudWatch Container Insights, customers can collect system metrics such as CPU, memory, disk, and network usage from Amazon EKS clusters running on Amazon Elastic Cloud Compute (Amazon EC2), providing the same experience as Amazon CloudWatch agent. ADOT Collector is now available with support for CloudWatch Container Insights for Amazon EKS and AWS Fargate profile for Amazon EKS. Customers can now collect container and pod metrics such as CPU and memory utilization for their pods that are deployed to an Amazon EKS cluster and view them in CloudWatch dashboards without any changes to their existing CloudWatch Container Insights experience. This will enable customers to also determine whether to scale up or down to respond to traffic and save costs.</p> <p>The ADOT Collector has the concept of a pipeline which comprises three key types of components, namely, receiver, processor, and exporter. A receiver is how data gets into the collector. It accepts data in a specified format, translates it into the internal format and passes it to processors and exporters defined in the pipeline. It can be pull or push based. A processor is an optional component that is used to perform tasks such as batching, filtering, and transformations on data between being received and being exported. An exporter is used to determine which destination to send the metrics, logs or traces. The collector architecture allows multiple instances of such pipelines to be defined via YAML configuration. The following diagrams illustrates the pipeline components in an ADOT Collector instance deployed to Amazon EKS and Amazon EKS with Fargate profile.</p> <p></p> <p>Figure: Pipeline components in an ADOT Collector instance deployed to Amazon EKS</p> <p>In the above architecture, we are deploying we are using an instance of AWS Container Insights Receiver in the pipeline and collect the metrics directly from the Kubelet. AWS Container Insights Receiver (<code>awscontainerinsightreceiver</code>) is an AWS specific receiver that supports CloudWatch Container Insights. CloudWatch Container Insights collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. Data are collected as as performance log events using embedded metric format. From the EMF data, Amazon CloudWatch can create the aggregated CloudWatch metrics at the cluster, node, pod, task, and service level. Below is an example of a sample <code>awscontainerinsightreceiver</code> configuration :</p> <pre><code>receivers:\n  awscontainerinsightreceiver:\n    # all parameters are optional\n    collection_interval: 60s\n    container_orchestrator: eks\n    add_service_as_attribute: true \n    prefer_full_pod_name: false \n    add_full_pod_name_metric_label: false \n</code></pre> <p>This entails deploying the collector as a DaemonSet using the above configuration on Amazon EKS. You will also have access to a fuller set of metrics collected by this receiver directly from the Kubelet. Having more than one instances of ADOT Collector will suffice to collect resource metrics from all the nodes in a cluster. Having a single instance of ADOT collector can be overwhelming during higher loads so always recommend to deploy more than one collector.</p> <p></p> <p>Figure: Pipeline components in an ADOT Collector instance deployed to Amazon EKS with Fargate profile</p> <p>In the above architecture, the kubelet on a worker node in a Kubernetes cluster exposes resource metrics such as CPU, memory, disk, and network usage at the /metrics/cadvisor endpoint. However, in EKS Fargate networking architecture, a pod is not allowed to directly reach the kubelet on that worker node. Hence, the ADOT Collector calls the Kubernetes API Server to proxy the connection to the kubelet on a worker node, and collect kubelet\u2019s cAdvisor metrics for workloads on that node. These metrics are made available in Prometheus format. Therefore, the collector uses an instance of Prometheus Receiver as a drop-in replacement for a Prometheus server and scrapes these metrics from the Kubernetes API server endpoint. Using Kubernetes service discovery, the receiver can discover all the worker nodes in an EKS cluster. Hence, more than one instances of ADOT Collector will suffice to collect resource metrics from all the nodes in a cluster. Having a single instance of ADOT collector can be overwhelming during higher loads so always recommend to deploy more than one collector.</p> <p>The metrics then go through a series of processors that perform filtering, renaming, data aggregation and conversion, and so on. The following is the list of processors used in the pipeline of an ADOT Collector instance for Amazon EKS illustrated above.</p> <ul> <li>Filter Processor is part of the AWS OpenTelemetry distribution to include or exclude metrics based on their name. It can be used as part of the metrics collection pipeline to filter out unwanted metrics. For example, suppose that you want Container Insights to only collect pod-level metrics (with name prefix <code>pod_</code>) excluding those for networking, with name prefix <code>pod_network</code>.</li> </ul> <pre><code>      # filter out only renamed metrics which we care about\n      filter:\n        metrics:\n          include:\n            match_type: regexp\n            metric_names:\n              - new_container_.*\n              - pod_.*\n</code></pre> <ul> <li>Metrics Transform Processor can be used to rename metrics, and add, rename or delete label keys and values. It can also be used to perform scaling and aggregations on metrics across labels or label values. </li> </ul> <pre><code>     metricstransform/rename:\n        transforms:\n          - include: container_spec_cpu_quota\n            new_name: new_container_cpu_limit_raw\n            action: insert\n            match_type: regexp\n            experimental_match_labels: {\"container\": \"\\\\S\"}\n</code></pre> <ul> <li>Cumulative to Delta Processor converts monotonic, cumulative sum and histogram metrics to monotonic, delta metrics. Non-monotonic sums and exponential histograms are excluded.</li> </ul> <pre><code>` # convert cumulative sum datapoints to delta\n cumulativetodelta:\n    metrics:\n        - pod_cpu_usage_seconds_total \n        - pod_network_rx_errors`\n</code></pre> <ul> <li>Delta to Rate Processor to convert delta sum metrics to rate metrics. This rate is a gauge.</li> </ul> <pre><code>` # convert delta to rate\n    deltatorate:\n        metrics:\n            - pod_memory_hierarchical_pgfault \n            - pod_memory_hierarchical_pgmajfault \n            - pod_network_rx_bytes \n            - pod_network_rx_dropped \n            - pod_network_rx_errors \n            - pod_network_tx_errors \n            - pod_network_tx_packets \n            - new_container_memory_pgfault \n            - new_container_memory_pgmajfault \n            - new_container_memory_hierarchical_pgfault \n            - new_container_memory_hierarchical_pgmajfault`\n</code></pre> <ul> <li>Metrics Generation Processor can be used to create new metrics using existing metrics following a given rule. </li> </ul> <pre><code>      experimental_metricsgeneration/1:\n        rules:\n          - name: pod_memory_utilization_over_pod_limit\n            unit: Percent\n            type: calculate\n            metric1: pod_memory_working_set\n            metric2: pod_memory_limit\n            operation: percent\n</code></pre> <p>The final component in the pipeline is AWS CloudWatch EMF Exporter, which converts the metrics to embedded metric format (EMF) and then sends them directly to CloudWatch Logs using the PutLogEvents API. The following list of metrics is sent to CloudWatch by the ADOT Collector for each of the workloads running on Amazon EKS.</p> <ul> <li>pod_cpu_utilization_over_pod_limit</li> <li>pod_cpu_usage_total</li> <li>pod_cpu_limit</li> <li>pod_memory_utilization_over_pod_limit</li> <li>pod_memory_working_set</li> <li>pod_memory_limit</li> <li>pod_network_rx_bytes</li> <li>pod_network_tx_bytes</li> </ul> <p>Each metric will be associated with the following dimension sets and collected under the CloudWatch namespace named ContainerInsights.</p> <ul> <li>ClusterName, LaunchType</li> <li>ClusterName, Namespace, LaunchType</li> <li>ClusterName, Namespace, PodName, LaunchType</li> </ul> <p>Further, Please learn about Container Insights Prometheus support for ADOT and deploying ADOT collector on Amazon EKS to visualize Amazon EKS resource metrics using CloudWatch Container Insights to setup ADOT collector pipeline in your Amazon EKS cluster and how to visualize your Amazon EKS resource metrics in CloudWatch Container Insights. Additionally, please reference Easily Monitor Containerized Applications with Amazon CloudWatch Container Insights, which includes step-by-step instructions on configuring an Amazon EKS cluster, deploying a containerized application, and monitoring the application's performance using Container Insights.</p>"},{"location":"guides/containers/aws-native/eks/amazon-cloudwatch-container-insights/#fluent-bit-integration-in-cloudwatch-container-insights-for-amazon-eks","title":"Fluent Bit Integration in CloudWatch Container Insights for Amazon EKS","text":"<p>Fluent Bit is an open source and multi-platform log processor and forwarder that allows you to collect data and logs from different sources, and unify and send them to different destinations including CloudWatch Logs. It\u2019s also fully compatible with Docker and Kubernetes environments. Using the newly launched Fluent Bit daemonset, you can send container logs from your EKS clusters to CloudWatch logs for logs storage and analytics.</p> <p>Due to its lightweight nature, using Fluent Bit as the default log forwarder in Container Insights on EKS worker nodes will allow you to stream application logs into CloudWatch logs efficiently and reliably. With Fluent Bit, Container Insights is able to deliver thousands of business critical logs at scale in a resource efficient manner, especially in terms of CPU and memory utilization at the pod level. In other words, compared to FluentD, which was the log forwarder used prior, Fluent Bit has a smaller resource footprint and, as a result, is more resource efficient for memory and CPU. On the other hand, AWS for Fluent Bit image, which includes Fluent Bit and related plugins, gives Fluent Bit an additional flexibility of adopting new AWS features faster as the image aims to provide a unified experience within AWS ecosystem.</p> <p>The architecture below shows individual components used by CloudWatch Container Insights for EKS:</p> <p></p> <p>Figure: Individual components used by CloudWatch Container Insights for EKS.</p> <p>While working with containers, it is recommended to push all the logs, including application logs, through the standard output (stdout) and standard error output (stderr) methods whenever possible using the Docker JSON logging driver. For this reason, in EKS, the logging driver is configured by default and everything that a containerized application writes to <code>stdout</code> or <code>stderr</code> is streamed into a JSON file under <code>\u201c/var/log/containers\"</code> on the worker node. Container Insights classifies those logs into three different categories by default and creates dedicated input streams for each category within Fluent Bit and independent log groups within CloudWatch Logs. Those categories are:</p> <ul> <li>Application logs: All applications logs stored under <code>\u201c/var/log/containers/*.log\"</code> are streamed into the dedicated <code>/aws/containerinsights/Cluster_Name/application</code> log group. All non-application logs such as kube-proxy and aws-node logs are excluded by default. However, additional Kubernetes add-on logs, such as CoreDNS logs, are also processed and streamed into this log group.</li> <li>Host logs: system logs for each EKS worker node are streamed into the <code>/aws/containerinsights/Cluster_Name/host</code> log group. These system logs include the contents of <code>\u201c/var/log/messages,/var/log/dmesg,/var/log/secure\u201d</code> files. Considering the stateless and dynamic nature of containerized workloads, where EKS worker nodes are often being terminated during scaling activities, streaming those logs in real time with Fluent Bit and having those logs available in CloudWatch logs, even after the node is terminated, are critical in terms of observability and monitoring health of EKS worker nodes. It also enables you to debug or troubleshoot cluster issues without logging into worker nodes in many cases and analyze these logs in more systematic way.</li> <li>Data plane logs: EKS already provides control plane logs. With Fluent Bit integration in Container Insights, the logs generated by EKS data plane components, which run on every worker node and are responsible for maintaining running pods are captured as data plane logs. These logs are also streamed into a dedicated CloudWatch log group under <code>\u2018 /aws/containerinsights/Cluster_Name/dataplane</code>. kube-proxy, aws-node, and Docker runtime logs are saved into this log group. In addition to control plane logs, having data plane logs stored in CloudWatch Logs helps to provide a complete picture of your EKS clusters.</li> </ul> <p>Further, Please learn more on topics such as Fluent Bit Configurations, Fluent Bit Monitoring and Log analysis from Fluent Bit Integration with Amazon EKS.</p>"},{"location":"guides/containers/aws-native/eks/amazon-cloudwatch-container-insights/#cost-savings-with-container-insights-on-amazon-eks","title":"Cost savings with Container Insights on Amazon EKS","text":"<p>With the default configuration, the Container Insights receiver collects the complete set of metrics as defined by the receiver documentation. The number of metrics and dimensions collected is high, and for large clusters this will significantly increase the costs for metric ingestion and storage. We are going to demonstrate two different approaches that you can use to configure the ADOT Collector to send only metrics that bring value and saves cost.</p>"},{"location":"guides/containers/aws-native/eks/amazon-cloudwatch-container-insights/#using-processors","title":"Using processors","text":"<p>This approach involves the introduction of OpenTelemetry processors as discussed above to filter out metrics or attributes to reduce the size of EMF logs. We will demonstrate the basic usage of two processors namely Filter and Resource.</p> <p>Filter processors can be included in the <code>ConfigMap</code> named <code>otel-agent-conf</code>:</p> <pre><code>processors:\n  # filter processors example\n  filter/include:\n    # any names NOT matching filters are excluded from remainder of pipeline\n    metrics:\n      include:\n        match_type: regexp\n        metric_names:\n          # re2 regexp patterns\n          - ^pod_.*\n  filter/exclude:\n    # any names matching filters are excluded from remainder of pipeline\n    metrics:\n      exclude:\n        match_type: regexp\n        metric_names:\n          - ^pod_network.*\n</code></pre> <p>Resource processor is also built into the AWS OpenTelemetry Distro and can be used to remove unwanted metric attributes. For example, if you want to remove the <code>Kubernetes</code> and <code>Sources</code> fields from the EMF logs, you can add the resource processor to the pipeline:</p> <pre><code>  # resource processors example\n  resource:\n    attributes:\n    - key: Sources\n      action: delete\n    - key: kubernetes\n      action: delete\n</code></pre>"},{"location":"guides/containers/aws-native/eks/amazon-cloudwatch-container-insights/#customize-metrics-and-dimensions","title":"Customize Metrics and Dimensions","text":"<p>In this approach, you will configure the CloudWatch EMF exporter to generate only the set of metrics that you want to send to CloudWatch Logs. The metric_declaration section of CloudWatch EMF exporter configuration can be used to define the set of metrics and dimensions that you want to export. For example, you can keep only pod metrics from the default configuration. This <code>metric_declaration</code> section will look like the following and to reduce the number of metrics, you can keep the dimension set only <code>[PodName, Namespace, ClusterName]</code> if you do not care about others:</p> <pre><code>  awsemf:\n    namespace: ContainerInsights\n    log_group_name: '/aws/containerinsights/{ClusterName}/performance'\n    log_stream_name: '{NodeName}'\n    resource_to_telemetry_conversion:\n      enabled: true\n    dimension_rollup_option: NoDimensionRollup\n    parse_json_encoded_attr_values: [Sources, kubernetes]\n    # Customized metric declaration section\n    metric_declarations:\n      # pod metrics\n      - dimensions: [[PodName, Namespace, ClusterName]]\n        metric_name_selectors:\n          - pod_cpu_utilization\n          - pod_memory_utilization\n          - pod_cpu_utilization_over_pod_limit\n          - pod_memory_utilization_over_pod_limit\n</code></pre> <p>This configuration will produce and stream the following four metrics within single dimension <code>[PodName, Namespace, ClusterName]</code> rather than 55 different metrics for multiple dimensions in the default configuration:</p> <ul> <li>pod_cpu_utilization</li> <li>pod_memory_utilization</li> <li>pod_cpu_utilization_over_pod_limit</li> <li>pod_memory_utilization_over_pod_limit</li> </ul> <p>With this configuration, you will only send the metrics that you are interested in rather than all the metrics configured by default. As a result, you will be able to decrease metric ingestion cost for Container Insights considerably. Having this flexibility will provide Container Insights costumers with high level of control over metrics being exported. Customizing metrics by modifying the <code>awsemf</code> exporter configuration is also highly flexible, and you can customize both the metrics that you want to send and their dimensions. Note that this is only applicable to logs that are sent to CloudWatch.</p> <p>The two approaches demonstrated discussed above are not mutually exclusive with each other. In fact, they both can be combined for a high degree of flexibility in customizing metrics that we want ingested into our monitoring system. We use this approach to decrease costs associated with metric storage and processing, as show in following graph.</p> <p></p> <p>Figure: AWS Cost Explorer</p> <p>In the preceding AWS Cost Explorer graph, we can see daily cost associated with CloudWatch using different configurations on the ADOT Collector in a small EKS cluster (20 Worker nodes, 220 pods). Aug 15th shows CloudWatch bill using ADOT Collector with the default configuration. On Aug 16th, we have used the Customize EMF exporter approach and can see about 30% cost savings. On Aug 17th, we used the Processors approach, which achieves about 45% costs saving. You must consider the trade-offs of customizing metrics sent by Container Insights as you will be able to decrease monitoring costs by sacrificing visibility of the monitored cluster. But also, the built-in dashboard provided by Container Insights within the AWS Console can be impacted by customized metrics as you can select not sending metrics and dimensions used by the dashboard. For further learning please check on Cost savings by customizing metrics sent by Container Insights in Amazon EKS.</p>"},{"location":"guides/containers/aws-native/eks/amazon-cloudwatch-container-insights/#using-eks-blueprints-to-setup-container-insights","title":"Using EKS Blueprints to setup Container Insights","text":"<p>EKS Blueprints is a collection of Infrastructure as Code (IaC) modules that will help you configure and deploy consistent, batteries-included EKS clusters across accounts and regions. You can use EKS Blueprints to easily bootstrap an EKS cluster with Amazon EKS add-ons as well as a wide range of popular open-source add-ons, including Prometheus, Karpenter, Nginx, Traefik, AWS Load Balancer Controller, Container Insights, Fluent Bit, Keda, Argo CD, and more. EKS Blueprints is implemented in two popular IaC frameworks, HashiCorp Terraform and AWS Cloud Development Kit (AWS CDK), which help you automate infrastructure deployments. </p> <p>As part of your Amazon EKS Cluster creation process using EKS Blueprints, you can setup Container Container Insights as a Day 2 operational tooling to collect, aggregate, and summarize metrics and logs from containerized applications and micro-services to Amazon CloudWatch console.</p>"},{"location":"guides/containers/aws-native/eks/amazon-cloudwatch-container-insights/#conclusion","title":"Conclusion","text":"<p>In this section of Observability best practices guide, we covered lot of deeper details around CloudWatch Container insights which included a introduction to Amazon CloudWatch Container Insights and how it can help you to observe your containerized workloads on Amazon EKS. We covered deeper grounds on using Amazon CloudWatch Container Insights  with AWS Distro for Open Telemetry to enable collection fo Container insight metrics to visualize the metrics our your containerzied workloads on Amazon CloudWatch console. Next, we covered lot of depth around Fluent Bit Integration in CloudWatch Container Insights for Amazon EKS to create dedicated input streams within Fluent Bit and independent log groups within CloudWatch Logs for Application, Host and Data Plane logs. Next, we talked about two different approaches such as processors, metrics dimensions to achieve cost savings with CloudWatch Container insights. Finally we talked in brief about how use EKS Blueprints as a vehicle to setup Container Insights during the Amazon EKS cluster creation process. You can get hands-on experience with the CloudWatch Container Insights module with in theOne Observability Workshop.</p>"},{"location":"guides/containers/aws-native/eks/container-tracing-with-aws-xray/","title":"Container Tracing with AWS X-Ray","text":"<p>In this section of Observability best practices guide, we will deep dive on to following topics related to Container Tracing with AWS X-Ray :</p> <ul> <li>Introduction to AWS X-Ray</li> <li>Traces collection using Amazon EKS add-ons for AWS Distro for OpenTelemetry</li> <li>Conclusion</li> </ul>"},{"location":"guides/containers/aws-native/eks/container-tracing-with-aws-xray/#introduction","title":"Introduction","text":"<p>AWS X-Ray is a service that collects data about requests that your application serves, and provides tools that you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization. For any traced request to your application, you can see detailed information not only about the request and response, but also about calls that your application makes to downstream AWS resources, microservices, databases, and web APIs.</p> <p>Instrumenting your application involves sending trace data for incoming and outbound requests and other events within your application, along with metadata about each request. Many instrumentation scenarios require only configuration changes. For example, you can instrument all incoming HTTP requests and downstream calls to AWS services that your Java application makes. There are several SDKs, agents, and tools that can be used to instrument your application for X-Ray tracing. See Instrumenting your application for more information.</p> <p>We will learn about about containerized application tracing by collect traces from your Amazon EKS cluster using Amazon EKS add-ons for AWS Distro for OpenTelemetry. </p>"},{"location":"guides/containers/aws-native/eks/container-tracing-with-aws-xray/#traces-collection-using-amazon-eks-add-ons-for-aws-distro-for-opentelemetry","title":"Traces collection using Amazon EKS add-ons for AWS Distro for OpenTelemetry","text":"<p>AWS X-Ray provides application-tracing functionality, giving deep insights into all microservices deployed. With X-Ray, every request can be traced as it flows through the involved microservices. This provides your DevOps teams the insights they need to understand how your services interact with their peers and enables them to analyze and debug issues much faster.</p> <p>AWS Distro for OpenTelemetry (ADOT) is a secure, AWS-supported distribution of the OpenTelemetry project. Users can instrument their applications just once and, using ADOT, send correlated metrics and traces to multiple monitoring solutions. Amazon EKS now allows users to enable ADOT as an add-on at any time after the cluster is up and running. The ADOT add-on includes the latest security patches and bug fixes and is validated by AWS to work with Amazon EKS.</p> <p>The ADOT add-on is an implementation of a Kubernetes Operator, which is a software extension to Kubernetes that makes use of custom resources to manage applications and their components. The add-on watches for a custom resource named OpenTelemetryCollector and manages the lifecycle of an ADOT Collector based on the configuration settings specified in the custom resource.</p> <p>The ADOT Collector has the concept of a pipeline that comprises three key types of components, namely, receiver, processor, and exporter. A receiver is how data gets into the collector. It accepts data in a specific format, translates it into the internal format, and passes it to processors and exporters defined in the pipeline. It can be pull- or push-based. A processor is an optional component that is used to perform tasks such as batching, filtering, and transformations on data between being received and being exported. An exporter is used to determine which destination to send the metrics, logs, or traces to. The collector architecture allows multiple instances of such pipelines to be set up via a Kubernetes YAML manifest.</p> <p>The following diagram illustrates an ADOT Collector configured with a traces pipeline, which sends telemetry data to AWS X-Ray. The traces pipeline comprises an instance of AWS X-Ray Receiver and AWS X-Ray Exporter and sends traces to AWS X-Ray. </p> <p></p> <p>Figure: Traces collection using Amazon EKS add-ons for AWS Distro for OpenTelemetry.</p> <p>Let\u2019s delve into the details of installing the ADOT add-on in an EKS cluster and then collect telemetry data from workloads. The following is a list of prerequisites needed before we can install the ADOT add-on.</p> <ul> <li>An EKS cluster supporting Kubernetes version 1.19 or higher. You may create the EKS cluster using one of the approaches outlined here.</li> <li>Certificate Manager, if not already installed in the cluster. It can be installed with the default configuration as per this documentation.</li> <li>Kubernetes RBAC permissions specifically for EKS add-ons to install the ADOT add-on in your cluster. This can be done by applying the settings in this YAML file to the cluster using a CLI tool such as kubectl.</li> </ul> <p>You can check the list of add-ons enabled for different versions of EKS using the following command:</p> <p><code>aws eks describe-addon-versions</code></p> <p>The JSON output should list the ADOT add-on among others, as shown below. Note that when an EKS cluster is created, EKS add-ons does not install any add-ons on it.</p> <pre><code>{\n   \"addonName\":\"adot\",\n   \"type\":\"observability\",\n   \"addonVersions\":[\n      {\n         \"addonVersion\":\"v0.45.0-eksbuild.1\",\n         \"architecture\":[\n            \"amd64\"\n         ],\n         \"compatibilities\":[\n            {\n               \"clusterVersion\":\"1.22\",\n               \"platformVersions\":[\n                  \"*\"\n               ],\n               \"defaultVersion\":true\n            },\n            {\n               \"clusterVersion\":\"1.21\",\n               \"platformVersions\":[\n                  \"*\"\n               ],\n               \"defaultVersion\":true\n            },\n            {\n               \"clusterVersion\":\"1.20\",\n               \"platformVersions\":[\n                  \"*\"\n               ],\n               \"defaultVersion\":true\n            },\n            {\n               \"clusterVersion\":\"1.19\",\n               \"platformVersions\":[\n                  \"*\"\n               ],\n               \"defaultVersion\":true\n            }\n         ]\n      }\n   ]\n}\n</code></pre> <p>Next, you can install the ADOT add-on with the following command :</p> <p><code>aws eks create-addon --addon-name adot --addon-version v0.45.0-eksbuild.1 --cluster-name $CLUSTER_NAME</code></p> <p>The version string must match the value of addonVersion field in the previously shown output. The output from a successful execution of this command looks as follows:</p> <pre><code>{\n    \"addon\": {\n        \"addonName\": \"adot\",\n        \"clusterName\": \"k8s-production-cluster\",\n        \"status\": \"ACTIVE\",\n        \"addonVersion\": \"v0.45.0-eksbuild.1\",\n        \"health\": {\n            \"issues\": []\n        },\n        \"addonArn\": \"arn:aws:eks:us-east-1:123456789000:addon/k8s-production-cluster/adot/f0bff97c-0647-ef6f-eecf-0b2a13f7491b\",\n        \"createdAt\": \"2022-04-04T10:36:56.966000+05:30\",\n        \"modifiedAt\": \"2022-04-04T10:38:09.142000+05:30\",\n        \"tags\": {}\n    }\n}\n</code></pre> <p>Wait until the add-on is in ACTIVE status before proceeding to the next step. The status of the add-on can be checked using the following command ;</p> <p><code>aws eks describe-addon --addon-name adot --cluster-name $CLUSTER_NAME</code></p>"},{"location":"guides/containers/aws-native/eks/container-tracing-with-aws-xray/#deploying-the-adot-collector","title":"Deploying the ADOT Collector","text":"<p>The ADOT add-on is an implementation of a Kubernetes Operator, which is a software extension to Kubernetes that makes use of custom resources to manage applications and their components. The add-on watches for a custom resource named OpenTelemetryCollector and manages the lifecycle of an ADOT Collector based on the configuration settings specified in the custom resource. The following figure shows an illustration of how this works.</p> <p></p> <p>Figure: Deploying the ADOT Collector.</p> <p>Next, let\u2019s take a look at how to deploy an ADOT Collector. The YAML configuration file here defines an OpenTelemetryCollector custom resource. When deployed to an EKS cluster, this will trigger the ADOT add-on to provision an ADOT Collector that includes a traces and metrics pipelines with components, as shown in the first illustration above. The collector is launched into the <code>aws-otel-eks</code> namespace as a Kubernetes Deployment with the name <code>${custom-resource-name}-collector</code>. A ClusterIP service with the same name is launched as well. Let\u2019s look into the individual components that make up the pipelines of this collector.</p> <p>The AWS X-Ray Receiver in the traces pipeline accepts segments or spans in X-Ray Segment format, which enables it to process segments sent by microservices instrumented with X-Ray SDK. It is configured to listen for traffic on UDP port 2000 and is exposed as a Cluster IP service. Per this configuration, workloads that want to send trace data to this receiver should be configured with the environment variable <code>AWS_XRAY_DAEMON_ADDRESS</code> set to <code>observability-collector.aws-otel-eks:2000</code>. The exporter sends these segments directly to X-Ray using the PutTraceSegments API.</p> <p>ADOT Collector is configured to be launched under the identity of a Kubernetes service account named <code>aws-otel-collector</code>, which is granted these permissions using a ClusterRoleBinding and ClusterRole, also shown in the configuration. The exporters need IAM permissions to send data to X-Ray. This is done by associating the service account with an IAM role using the IAM roles for service accounts feature supported by EKS. The IAM role should be associated with the AWS-managed policies such as AWSXRayDaemonWriteAccess. The helper script here may be used, after setting the CLUSTER_NAME and REGION variables, to create an IAM role named <code>EKS-ADOT-ServiceAccount-Role</code> that is granted these permissions and is associated with the <code>aws-otel-collector</code> service account.</p>"},{"location":"guides/containers/aws-native/eks/container-tracing-with-aws-xray/#end-to-end-test-of-traces-collection","title":"End-to-end test of traces collection","text":"<p>Let\u2019s now put all this together and test traces collection from workloads deployed to an EKS cluster. The following illustration shows the setup employed for this test. It comprises a front-end service that exposes a set of REST APIs and interacts with S3 as well as a datastore service that, in turn, interacts with an instance of Aurora PostgreSQL database. The services are instrumented with X-Ray SDK. ADOT Collector is launched in Deployment mode by deploying an OpenTelemetryCollector custom resource using the YAML manifest that was discussed in the last section. Postman client is used as an external traffic generator, targeting the front-end service.</p> <p></p> <p>Figure: End-to-end test of traces collection.</p> <p>The following image shows the service graph generated by X-Ray using the segment data captured from the services, with the average response latency for each segment.</p> <p></p> <p>Figure: CloudWatch Service Map console.*</p> <p>Please check on Traces pipeline with OTLP Receiver and AWS X-Ray Exporter sending traces to AWS X-Ray for OpenTelemetryCollector custom resource definitions that pertain to traces pipeline configurations. Customers who want to use ADOT Collector in conjunction with AWS X-Ray may start with these configuration templates, replace the placeholder variables with values based on their target environments and quickly deploy the collector to their Amazon EKS clusters using EKS add-on for ADOT.</p>"},{"location":"guides/containers/aws-native/eks/container-tracing-with-aws-xray/#using-eks-blueprints-to-setup-container-tracing-with-aws-x-ray","title":"Using EKS Blueprints to setup container tracing with AWS X-Ray","text":"<p>EKS Blueprints is a collection of Infrastructure as Code (IaC) modules that will help you configure and deploy consistent, batteries-included EKS clusters across accounts and regions. You can use EKS Blueprints to easily bootstrap an EKS cluster with Amazon EKS add-ons as well as a wide range of popular open-source add-ons, including Prometheus, Karpenter, Nginx, Traefik, AWS Load Balancer Controller, Container Insights, Fluent Bit, Keda, Argo CD, and more. EKS Blueprints is implemented in two popular IaC frameworks, HashiCorp Terraform and AWS Cloud Development Kit (AWS CDK), which help you automate infrastructure deployments. </p> <p>As part of your Amazon EKS Cluster creation process using EKS Blueprints, you can setup AWS X-Ray as a Day 2 operational tooling to collect, aggregate, and summarize metrics and logs from containerized applications and micro-services to Amazon CloudWatch console.</p>"},{"location":"guides/containers/aws-native/eks/container-tracing-with-aws-xray/#conclusion","title":"Conclusion","text":"<p>In this section of Observability best practices guide, we learned about using AWS X-Ray for container tracing your applications on Amazon EKS by traces collection using Amazon EKS add-ons for AWS Distro for OpenTelemetry. For further learning, please check on Metrics and traces collection using Amazon EKS add-ons for AWS Distro for OpenTelemetry to Amazon Managed Service for Prometheus and Amazon CloudWatch. Finally we talked in brief about how use EKS Blueprints as a vehicle to setup Container tracing using AWS X-Ray during the Amazon EKS cluster creation process. For further deep dive, we would highly recommend you to practice X-Ray Traces module under AWS native Observability category of AWS One Observability Workshop.</p>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/","title":"Amazon EKS API Server Monitoring","text":"<p>In this section of Observability best practices guide, we will deep dive on to following topics related to API Server Monitoring:</p> <ul> <li>Introduction to Amazon EKS API Server Monitoring</li> <li>Setting up an API Server Troubleshooter Dashboard</li> <li>Using API Troubleshooter Dashboard to Understand API Server Problems</li> <li>Understanding Unbounded list calls to API Server</li> <li>Stopping bad behavior to API Server</li> <li>API Priority and Fairness</li> <li>Identifying slowest API calls and API Server Latency Issues</li> </ul>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#introduction","title":"Introduction","text":"<p>Monitoring your Amazon EKS managed control plane is a very important Day 2 operational activity to proactively identity issues with health of your EKS cluster. Amazon EKS Control plane monitoring helps you to take proactive measures based on the collected metrics. These metrics would helps us to troubleshoot the API servers and pin point the problem under the hood. </p> <p>We will be using Amazon Managed Service for Prometheus (AMP) for our demonstration in this section for Amazon EKS API server monitoring and Amazon Managed Grafana (AMG) for visualization of metrics. Prometheus is a popular open source monitoring tool that provides powerful querying features and has wide support for a variety of workloads. Amazon Managed Service for Prometheus is a fully managed Prometheus-compatible service that makes it easier to monitor environments, such as Amazon EKS, Amazon Elastic Container Service (Amazon ECS), and Amazon Elastic Compute Cloud (Amazon EC2), securely and reliably. Amazon Managed Grafana is a fully managed and secure data visualization service for open source Grafana that enables customers to instantly query, correlate, and visualize operational metrics, logs, and traces for their applications from multiple data sources</p> <p>We will first setup a starter dashboard using Amazon Managed Service for Prometheus and Amazon Managed Grafana to help you with troubleshooting Amazon Elastic Kubernetes Service (Amazon EKS) API Servers with Prometheus. We will diving deep in up coming sections around understanding problems while troubleshooting the EKS API Servers, API priority and fairness, stopping bad behaviours. Finally we will deep dive in indentifying API calls that are slowest and API server latency issues which helps us to take actions to keep state of our Amazon EKS cluster healthy.</p>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#setting-up-an-api-server-troubleshooter-dashboard","title":"Setting up an API Server Troubleshooter Dashboard","text":"<p>We will setup a starter dashboard to help you with troubleshooting Amazon Elastic Kubernetes Service (Amazon EKS) API Servers with AMP. We will use this to help you understand the metrics while troubleshooting your production EKS clusters. We will further focus deep on the collected metrics to understand its importance while troubleshooting your Amazon EKS clusters.</p> <p>First, setup an ADOT collector to collect metrics from your Amazon EKS cluster to Amazon Manager Service for Prometheus. In this setup you will be using EKS ADOT Addon which  allows users to enable ADOT as an add-on at any time after the EKS cluster is up and running. The ADOT add-on includes the latest security patches and bug fixes and is validated by AWS to work with Amazon EKS. This setup will show you how to install the ADOT add-on in an EKS cluster and then use it to collect metrics from your cluster.</p> <p>Next, setup your Amazon Managed Grafana workspace to visualize metrics using AMP as a data source which you have setup in the first step. Finally download the API troubleshooter dashboard, navigate to Amazon Managed Grafana to upload the API troubleshooter dashboard json to visualize the metrics for further troubleshooting.</p>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#using-api-troubleshooter-dashboard-to-understand-problems","title":"Using API Troubleshooter Dashboard to Understand Problems","text":"<p>Let\u2019s say you found an interesting open-source project that you wanted to install in your cluster. That operator deploys a DaemonSet to your cluster that might be using malformed requests, a needlessly high volume of LIST calls, or maybe each of its DaemonSets across all your 1,000 nodes are requesting status of all 50,000 pods on your cluster every minute! Does this really happen often? Yes, it does! Let\u2019s take a quick detour on how that happens.</p>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#understanding-list-vs-watch","title":"Understanding LIST vs. WATCH","text":"<p>Some applications need to understand the state of the objects in your cluster. For example, your machine learning (ML) application wants to know the job status by understanding how many pods are not in the Completed status. In Kubernetes, there are well-behaved ways to do this with something called a WATCH, and some not-so-well-behaved ways that list every object on the cluster to find the latest status on those pods.</p>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#a-well-behaved-watch","title":"A well-behaved WATCH","text":"<p>Using a WATCH or a single, long-lived connection to receive updates via a push model is the most scalable way to do updates in Kubernetes. To oversimplify, we ask for the full state of the system, then only update the object in a cache when changes are received for that object, periodically running a re-sync to ensure that no updates were missed.</p> <p>In the below image we use the <code>apiserver_longrunning_gauge</code> to get an idea of the number of these long-lived connections across both API servers.</p> <p></p> <p>Figure: <code>apiserver_longrunning_gauge</code> metric</p> <p>Even with this efficient system, we can still have too much of a good thing. For example, if we use many very small nodes, each using two or more DaemonSets that need to talk to the API server, it is quite easy to dramatically increase the number of WATCH calls on the system unnecessarily. For example, let\u2019s look at the difference between eight xlarge nodes vs. a single 8xlarge. Here we see an 8x increase of WATCH calls on the system.</p> <p></p> <p>Figure: WATCH calls between 8 xlarge nodes.</p> <p>Now these are efficient calls, but what if instead they were the ill-behaved calls we alluded to earlier? Imagine if one of the above DaemonSets on each of the 1,000 nodes is requesting updates on each of the total 50,000 pods in the cluster. We will explore this idea of an unbounded list call in next section.</p> <p>A quick word of caution before continuing, the type of consolidation in the above example must be done with great care, and has many other factors to consider. Everything from the delay of the number of threads competing for a limited number of CPUs on the system, Pod churn rate, to the maximum number of volume attachments a node can handle safely. However, our focus will be on the metrics that lead us to actionable steps that can prevent issues from happening\u2014and maybe give us new insight into our designs.</p> <p>The WATCH metric is a simple one, but it can be used to track and reduce the number of watches, if that is a problem for you. Here are a few options you could consider to reduce this number:</p> <ul> <li>Limit the number of ConfigMaps Helm creates to track History</li> <li>Use Immutable ConfigMaps and Secrets which do not use a WATCH</li> <li>Sensible node sizing and consolidation</li> </ul>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#understanding-unbounded-list-calls-to-api-server","title":"Understanding Unbounded list calls to API Server","text":"<p>Now for the LIST call we have been talking about. A list call is pulling the full history on our Kubernetes objects each time we need to understand an object\u2019s state, nothing is being saved in a cache this time.</p> <p>How impactful is all this? That will vary depending on how many agents are requesting data, how often they are doing so, and how much data they are requesting. Are they asking for everything on the cluster, or just a single namespace? Does that happen every minute, on very node? Let\u2019s use an example of a logging agent that is appending Kubernetes metadata on every log sent from a node. This could be an overwhelming amount of data in larger clusters. There are many ways for the agent to get that data via a list call, so let\u2019s look at a few.</p> <p>The below request is asking for pods from a specific namespace.</p> <p><code>/api/v1/namespaces/my-namespace/pods</code></p> <p>Next, we request all 50,000 pods on the cluster, but in chunks of 500 pods at a time.</p> <p><code>/api/v1/pods?limit=500</code></p> <p>The next call is the most disruptive. Fetching all 50,000 pods on the entire cluster at the same time.</p> <p><code>/api/v1/pods</code></p> <p>This happens quite commonly in the field and can be seen in the logs.</p>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#stopping-bad-behavior-to-api-server","title":"Stopping bad behavior to API Server","text":"<p>How can we protect our cluster from such bad behavior? Before Kubernetes 1.20, the API server would protect itself by limiting the number of inflight requests processed per second. Since etcd can only handle so many requests at one time in a performant way, we need to ensure the number of requests is limited to a value per second that keeps etcd reads and writes in a reasonable latency band. Unfortunately, at the time of this writing, there is no dynamic way to do this.</p> <p>In the below chart we see a breakdown of read requests, which has a default maximum of 400 inflight request per API server and a default max of 200 concurrent write requests. In a default EKS cluster you will see two API servers for a total of 800 reads and 400 writes. However, caution is advised as these servers can have asymmetric loads on them at different times like right after an upgrade, etc.</p> <p></p> <p>Figure: Grafana chart with breakdown of read requests.</p> <p>It turns out that the above was not a perfect scheme. For example, how could we keep this badly behaving new operator we just installed from taking up all the inflight write requests on the API server and potentially delaying important requests such as node keepalive messages?</p>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#api-priority-and-fairness","title":"API Priority and Fairness","text":"<p>Instead of worrying about how many read/write requests were open per second, what if we treated the capacity as one total number, and each application on the cluster got a fair percentage or share of that total maximum number?</p> <p>To do that that effectively, we would need to identify who sent the request to the API server, then give that request a name tag of sorts. With this new name tag, we could then see all these requests are coming from a new agent we will call \u201cChatty.\u201d Now we can group all of Chatty\u2019s requests into something called a flow, that identifies those requests are coming from the same DaemonSet. This concept now gives us the ability to restrict this bad agent and ensure it does not consume the whole cluster.</p> <p>However, not all requests are created equal. The control plane traffic that is needed to keep the cluster operational should be a higher priority than our new operator. This is where the idea of priority levels comes into play. What if, by default, we had a several \u201cbuckets\u201d or queues for critical, high, and low priority traffic? We do not want the chatty agent flow getting a fair share of traffic in the critical traffic queue. We can however put that traffic in a low priority queue so that flow is competing with perhaps other chatty agents. We then would want to ensure that each priority level had the right number of shares or percentage of the overall maximum the API server can handle to ensure the requests were not too delayed.</p>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#priority-and-fairness-in-action","title":"Priority and fairness in action","text":"<p>Since this is a relatively new feature, many existing dashboards will use the older model of maximum inflight reads and maximum inflight writes. Why this can be problematic?</p> <p>What if we were giving high priority name tags to everything in the kube-system namespace, but we then installed that bad agent into that important namespace, or even simply deployed too many applications in that namespace? We could end up having the same problem we were trying to avoid! So best to keep a close eye on such situations.</p> <p>I have broken out for you some of the metrics I find most interesting to track these kinds of issues.</p> <ul> <li>What percentage of a priority group\u2019s shares are used?</li> <li>What is the longest time a request waited in a queue?</li> <li>Which flow is using the most shares?</li> <li>Are there unexpected delays on the system?</li> </ul>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#percent-in-use","title":"Percent in use","text":"<p>Here we see the different default priority groups on the cluster and what percentage of the max is used.</p> <p></p> <p>Figure: Priority groups on the cluster.</p>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#time-request-was-in-queue","title":"Time request was in queue","text":"<p>How long in seconds the request sat in the priority queue before being processed.</p> <p></p> <p>Figure: Time the request was in priority queue.</p>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#top-executed-requests-by-flow","title":"Top executed requests by flow","text":"<p>Which flow is taking up the most shares?</p> <p></p> <p>Figure: Top executing requests by flow.</p>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#request-execution-time","title":"Request Execution Time","text":"<p>Are there any unexpected delays in processing?</p> <p></p> <p>Figure: Flow control request execution time.</p>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#identifying-slowest-api-calls-and-api-server-latency-issues","title":"Identifying slowest API calls and API Server Latency Issues","text":"<p>Now that we understand the nature of the things that cause API latency, we can take a step back and look at the big picture. It\u2019s important to remember that our dashboard designs are simply trying to get a quick snapshot if there is a problem we should be investigating. For detailed analysis, we would use ad-hoc queries with PromQL\u2014or better yet, logging queries.</p> <p>What are some ideas for the high-level metrics we would want to look at?</p> <ul> <li>What API call is taking the most time to complete?<ul> <li>What is the call doing? (Listing objects, deleting them, etc.)</li> <li>What objects is it trying to do that operation on? (Pods, Secrets, ConfigMaps, etc.)</li> </ul> </li> <li>Is there a latency problem on the API server itself?<ul> <li>Is there a delay in one of my priority queues causing a backup in requests?</li> </ul> </li> <li>Does it just look like API server is slow because the etcd server is experiencing latency?</li> </ul>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#slowest-api-call","title":"Slowest API call","text":"<p>In the below chart we are looking for the API calls that took the most time to complete for that period. In this case we see a custom resource definition (CRD) is calling a LIST function that is the most latent call during the 05:40 time frame. Armed with this data we can use CloudWatch Insights to pull LIST requests from the audit log in that timeframe to see which application this might be.</p> <p></p> <p>Figure: Top 5 slowest API calls.</p>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#api-request-duration","title":"API Request Duration","text":"<p>This API latency chart helps us to understand if any requests are approaching the timeout value of one minute. I like the histogram over time format below as I can see outliers in the data that a line graph would hide.</p> <p></p> <p>Figure: API Request duration heatmap.</p> <p>Simply hovering over a bucket shows us the exact number of calls that took around 25 milliseconds. [Image: Image.jpg]Figure: Calls over 25 milliseconds.</p> <p>This concept is important when we are working with other systems that cache requests. Cache requests will be fast; we do not want to merge those request latencies with slower requests. Here we can see two distinct bands of latency, requests that have been cached, and those that have not.</p> <p></p> <p>Figure: Latency, requests cached.</p>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#etcd-request-duration","title":"ETCD Request Duration","text":"<p>ETCD latency is one of the most important factors in Kubernetes performance. Amazon EKS allows you see this performance from the API server\u2019s perspective by looking at the <code>request_duration_seconds_bucket</code> metric.</p> <p></p> <p>Figure : <code>request_duration_seconds_bucket</code> metric.</p> <p>We can now start to put the things we learned together by seeing if certain events are correlated. In the below chart we see API server latency, but we also see much of this latency is coming from the etcd server. Being able to quickly move to the right problem area with just a glance is what makes a dashboard powerful.</p> <p></p> <p>Figure: Etcd Requests</p>"},{"location":"guides/containers/aws-native/eks/eks-api-server-monitoring/#conclusion","title":"Conclusion","text":"<p>In this section of Observability best practices guide, We used a starter dashboard using Amazon Managed Service for Prometheus and Amazon Managed Grafana to help you with troubleshooting Amazon Elastic Kubernetes Service (Amazon EKS) API Servers. Further, we deep dived around understanding problems while troubleshooting the EKS API Servers, API priority and fairness, stopping bad behaviours. Finally deep dived  in indentifying API calls that are slowest and API server latency issues which helps us to take actions to keep state of our Amazon EKS cluster healthy. For further deep dive, we would highly recommend you to practice Application Monitoring module under AWS native Observability category of AWS One Observability Workshop.</p>"},{"location":"guides/containers/aws-native/eks/log-aggregation/","title":"Log Aggregation","text":"<p>In this section of Observability best practices guide, we will deep dive on to following topics related to Amazon EKS Logging with AWS Native services:</p> <ul> <li>Introduction to AWS EKS logging</li> <li>Amazon EKS control plane logging</li> <li>Amazon EKS data plane logging</li> <li>Amazon EKS application logging</li> <li>Unified log aggregation from Amazon EKS and other compute platforms using AWS Native services</li> <li>Conclusion</li> </ul>"},{"location":"guides/containers/aws-native/eks/log-aggregation/#introduction","title":"Introduction","text":"<p>Amazon EKS logging can be divided into three types such as control plane logging, node logging, and application logging. The Kubernetes control plane is a set of components that manage Kubernetes clusters and produce logs used for auditing and diagnostic purposes. With Amazon EKS, you can turn on logs for different control plane components and send them to CloudWatch.</p> <p>Kubernetes also runs system components such as <code>kubelet</code> and <code>kube-proxy</code> on each Kubernetes node that runs your pods. These components write logs within each node and you can configure CloudWatch and Container Insights to capture these logs for each Amazon EKS node.</p> <p>Containers are grouped as pods within a Kubernetes cluster and are scheduled to run on your Kubernetes nodes. Most containerized applications write to standard output and standard error, and the container engine redirects the output to a logging driver. In Kubernetes, the container logs are found in the <code>/var/log/pods</code> directory on a node. You can configure CloudWatch and Container Insights to capture these logs for each of your Amazon EKS pods.</p> <p>There are three common approaches for capturing logs Shipping container logs to a centralized log aggregation system in Kubernetes:</p> <ul> <li>Node level agent, like a Fluentd daemonset. This is the recommended pattern.</li> <li>Sidecar container, like a Fluentd sidecar container.</li> <li>Directly writing to log collection system. In this approach, the application is responsible for shipping the logs. This is the least recommended option because you will have to include the log aggregation system\u2019s SDK in your application code instead of reusing community build solutions like Fluentd. This pattern also disobeys the principle of separation of concerns, according to which, logging implementation should be independent of the application. Doing so allows you to change the logging infrastructure without impacting or changing your application.</li> </ul> <p>We will now dive in to each of these logging categories for Amazon EKS logging along with talking about unified log aggregation from Amazon EKS and other compute platforms.</p>"},{"location":"guides/containers/aws-native/eks/log-aggregation/#amazon-eks-control-plane-logging","title":"Amazon EKS control plane logging","text":"<p>An Amazon EKS cluster consists of a high availability, single-tenant control plane for your Kubernetes cluster and the Amazon EKS nodes that run your containers. The control plane nodes run in an account managed by AWS. The Amazon EKS cluster control plane nodes are integrated with CloudWatch and you can turn on logging for specific control plane components. Logs are provided for each Kubernetes control plane component instance. AWS manages the health of your control plane nodes and provides a service-level agreement (SLA) for the Kubernetes endpoint.</p> <p>Amazon EKS control plane logging consists of following cluster control plane log types. Each log type corresponds to a component of the Kubernetes control plane. To learn more about these components, see Kubernetes Components in the Kubernetes documentation.</p> <ul> <li>API server (<code>api</code>) \u2013 Your cluster's API server is the control plane component that exposes the Kubernetes API. If you enable API server logs when you launch the cluster, or shortly thereafter, the logs include API server flags that were used to start the API server. For more information, see <code>kube-apiserver</code> and the audit policy in the Kubernetes documentation.</li> <li>Audit (<code>audit</code>) \u2013 Kubernetes audit logs provide a record of the individual users, administrators, or system components that have affected your cluster. For more information, see Auditing in the Kubernetes documentation.</li> <li>Authenticator (<code>authenticator</code>) \u2013 Authenticator logs are unique to Amazon EKS. These logs represent the control plane component that Amazon EKS uses for Kubernetes Role Based Access Control (RBAC) authentication using IAM credentials. For more information, see Cluster management.</li> <li>Controller manager (<code>controllerManager</code>) \u2013 The controller manager manages the core control loops that are shipped with Kubernetes. For more information, see kube-controller-manager in the Kubernetes documentation.</li> <li>Scheduler (<code>scheduler</code>) \u2013 The scheduler component manages when and where to run pods in your cluster. For more information, see kube-scheduler in the Kubernetes documentation.</li> </ul> <p>Please follow enabling and disabling control plane logs section and enable control plane logs via AWS console or via AWS CLI.</p>"},{"location":"guides/containers/aws-native/eks/log-aggregation/#querying-control-plane-logs-from-cloudwatch-console","title":"Querying control plane logs from CloudWatch console","text":"<p>After you enable control plane logging on your Amazon EKS cluster, you can find EKS control plane logs in the <code>/aws/eks/cluster-name/cluster</code> log group. For more information, see Viewing cluster control plane logs. Please make sure to replace <code>cluster-name</code> with your cluster's name.</p> <p>You can use CloudWatch Logs Insights to search through the EKS control plane log data. For more information, see Analyzing log data with CloudWatch Insights. It is important to node that, you can view log events in CloudWatch Logs only after you turn on control plane logging in a cluster. Before you select a time range to run queries in CloudWatch Logs Insights, verify that you turned on control plane logging. Please check the screenshot below showing an example of a EKS control plane log query with query output.</p> <p></p> <p>Figure: CloudWatch Logs Insights.</p>"},{"location":"guides/containers/aws-native/eks/log-aggregation/#sample-queries-for-common-eks-use-cases-on-cloudwatch-logs-insights","title":"Sample queries for common EKS use cases on CloudWatch Logs Insights","text":"<p>To find the cluster creator, search for the IAM entity that's mapped to the kubernetes-admin user.</p> <pre><code>fields @logStream, @timestamp, @message| sort @timestamp desc\n| filter @logStream like /authenticator/\n| filter @message like \"username=kubernetes-admin\"\n| limit 50\n</code></pre> <p>Example output:</p> <pre><code>@logStream, @timestamp @messageauthenticator-71976 ca11bea5d3083393f7d32dab75b,2021-08-11-10:09:49.020,\"time=\"\"2021-08-11T10:09:43Z\"\" level=info msg=\"\"access granted\"\" arn=\"\"arn:aws:iam::12345678910:user/awscli\"\" client=\"\"127.0.0.1:51326\"\" groups=\"\"[system:masters]\"\" method=POST path=/authenticate sts=sts.eu-west-1.amazonaws.com uid=\"\"heptio-authenticator-aws:12345678910:ABCDEFGHIJKLMNOP\"\" username=kubernetes-admin\"\n</code></pre> <p>In this output, IAM user arn:aws:iam::12345678910:user/awscli is mapped to user kubernetes-admin.</p> <p>To find requests that a specific user performed, search for operations that the kubernetes-admin user performed.</p> <pre><code>fields @logStream, @timestamp, @message| filter @logStream like /^kube-apiserver-audit/\n| filter strcontains(user.username,\"kubernetes-admin\")\n| sort @timestamp desc\n| limit 50\n</code></pre> <p>Example output:</p> <pre><code>@logStream,@timestamp,@messagekube-apiserver-audit-71976ca11bea5d3083393f7d32dab75b,2021-08-11 09:29:13.095,\"{...\"\"requestURI\"\":\"\"/api/v1/namespaces/kube-system/endpoints?limit=500\";\",\"string\"\"verb\"\":\"\"list\"\",\"\"user\"\":{\"\"username\"\":\"\"kubernetes-admin\"\",\"\"uid\"\":\"\"heptio-authenticator-aws:12345678910:ABCDEFGHIJKLMNOP\"\",\"\"groups\"\":[\"\"system:masters\"\",\"\"system:authenticated\"\"],\"\"extra\"\":{\"\"accessKeyId\"\":[\"\"ABCDEFGHIJKLMNOP\"\"],\"\"arn\"\":[\"\"arn:aws:iam::12345678910:user/awscli\"\"],\"\"canonicalArn\"\":[\"\"arn:aws:iam::12345678910:user/awscli\"\"],\"\"sessionName\"\":[\"\"\"\"]}},\"\"sourceIPs\"\":[\"\"12.34.56.78\"\"],\"\"userAgent\"\":\"\"kubectl/v1.22.0 (darwin/amd64) kubernetes/c2b5237\"\",\"\"objectRef\"\":{\"\"resource\"\":\"\"endpoints\"\",\"\"namespace\"\":\"\"kube-system\"\",\"\"apiVersion\"\":\"\"v1\"\"}...}\"\n</code></pre> <p>To find API calls that a specific userAgent made, you can use this example query:</p> <pre><code>fields @logStream, @timestamp, userAgent, verb, requestURI, @message| filter @logStream like /kube-apiserver-audit/\n| filter userAgent like /kubectl\\/v1.22.0/\n| sort @timestamp desc\n| filter verb like /(get)/\n</code></pre> <p>Shortened example output:</p> <pre><code>@logStream,@timestamp,userAgent,verb,requestURI,@messagekube-apiserver-audit-71976ca11bea5d3083393f7d32dab75b,2021-08-11 14:06:47.068,kubectl/v1.22.0 (darwin/amd64) kubernetes/c2b5237,get,/apis/metrics.k8s.io/v1beta1?timeout=32s,\"{\"\"kind\"\":\"\"Event\"\",\"\"apiVersion\"\":\"\"audit.k8s.io/v1\"\",\"\"level\"\":\"\"Metadata\"\",\"\"auditID\"\":\"\"863d9353-61a2-4255-a243-afaeb9183524\"\",\"\"stage\"\":\"\"ResponseComplete\"\",\"\"requestURI\"\":\"\"/apis/metrics.k8s.io/v1beta1?timeout=32s\"\",\"\"verb\"\":\"\"get\"\",\"\"user\"\":{\"\"username\"\":\"\"kubernetes-admin\"\",\"\"uid\"\":\"\"heptio-authenticator-aws:12345678910:AIDAUQGC5HFOHXON7M22F\"\",\"\"groups\"\":[\"\"system:masters\"\",\"\"system:authenticated\"\"],\"\"extra\"\":{\"\"accessKeyId\"\":[\"\"ABCDEFGHIJKLMNOP\"\"],\"\"arn\"\":[\"\"arn:aws:iam::12345678910:user/awscli\"\"],\"\"canonicalArn\"\":[\"\"arn:aws:iam::12345678910:user/awscli\"\"],\"\"sourceIPs\"\":[\"\"12.34.56.78\"\"],\"\"userAgent\"\":\"\"kubectl/v1.22.0 (darwin/amd64) kubernetes/c2b5237\"\"...}\"\n</code></pre> <p>To find mutating changes made to the aws-auth ConfigMap, you can use this example query:</p> <pre><code>fields @logStream, @timestamp, @message| filter @logStream like /^kube-apiserver-audit/\n| filter requestURI like /\\/api\\/v1\\/namespaces\\/kube-system\\/configmaps/\n| filter objectRef.name = \"aws-auth\"\n| filter verb like /(create|delete|patch)/\n| sort @timestamp desc\n| limit 50\n</code></pre> <p>Shortened example output:</p> <pre><code>@logStream,@timestamp,@messagekube-apiserver-audit-f01c77ed8078a670a2eb63af6f127163,2021-10-27 05:43:01.850,{\"\"kind\"\":\"\"Event\"\",\"\"apiVersion\"\":\"\"audit.k8s.io/v1\"\",\"\"level\"\":\"\"RequestResponse\"\",\"\"auditID\"\":\"\"8f9a5a16-f115-4bb8-912f-ee2b1d737ff1\"\",\"\"stage\"\":\"\"ResponseComplete\"\",\"\"requestURI\"\":\"\"/api/v1/namespaces/kube-system/configmaps/aws-auth?timeout=19s\"\",\"\"verb\"\":\"\"patch\"\",\"\"responseStatus\"\": {\"\"metadata\"\": {},\"\"code\"\": 200 },\"\"requestObject\"\": {\"\"data\"\": { contents of aws-auth ConfigMap } },\"\"requestReceivedTimestamp\"\":\"\"2021-10-27T05:43:01.033516Z\"\",\"\"stageTimestamp\"\":\"\"2021-10-27T05:43:01.042364Z\"\" }\n</code></pre> <p>To find requests that were denied, you can use this example query:</p> <pre><code>fields @logStream, @timestamp, @message| filter @logStream like /^authenticator/\n| filter @message like \"denied\"\n| sort @timestamp desc\n| limit 50\n</code></pre> <p>Example output:</p> <pre><code>@logStream,@timestamp,@messageauthenticator-8c0c570ea5676c62c44d98da6189a02b,2021-08-08 20:04:46.282,\"time=\"\"2021-08-08T20:04:44Z\"\" level=warning msg=\"\"access denied\"\" client=\"\"127.0.0.1:52856\"\" error=\"\"sts getCallerIdentity failed: error from AWS (expected 200, got 403)\"\" method=POST path=/authenticate\"\n</code></pre> <p>To find the node that a pod was scheduled on, query the kube-scheduler logs.</p> <pre><code>fields @logStream, @timestamp, @message| sort @timestamp desc\n| filter @logStream like /kube-scheduler/\n| filter @message like \"aws-6799fc88d8-jqc2r\"\n| limit 50\n</code></pre> <p>Example output:</p> <pre><code>@logStream,@timestamp,@messagekube-scheduler-bb3ea89d63fd2b9735ba06b144377db6,2021-08-15 12:19:43.000,\"I0915 12:19:43.933124       1 scheduler.go:604] \"\"Successfully bound pod to node\"\" pod=\"\"kube-system/aws-6799fc88d8-jqc2r\"\" node=\"\"ip-192-168-66-187.eu-west-1.compute.internal\"\" evaluatedNodes=3 feasibleNodes=2\"\n</code></pre> <p>In this example output, pod aws-6799fc88d8-jqc2r was scheduled on node ip-192-168-66-187.eu-west-1.compute.internal.</p> <p>To find HTTP 5xx server errors for Kubernetes API server requests, you can use this example query:</p> <pre><code>fields @logStream, @timestamp, responseStatus.code, @message| filter @logStream like /^kube-apiserver-audit/\n| filter responseStatus.code &gt;= 500\n| limit 50\n</code></pre> <p>Shortened example output:</p> <pre><code>@logStream,@timestamp,responseStatus.code,@messagekube-apiserver-audit-4d5145b53c40d10c276ad08fa36d1f11,2021-08-04 07:22:06.518,503,\"...\"\"requestURI\"\":\"\"/apis/metrics.k8s.io/v1beta1?timeout=32s\"\",\"\"verb\"\":\"\"get\"\",\"\"user\"\":{\"\"username\"\":\"\"system:serviceaccount:kube-system:resourcequota-controller\"\",\"\"uid\"\":\"\"36d9c3dd-f1fd-4cae-9266-900d64d6a754\"\",\"\"groups\"\":[\"\"system:serviceaccounts\"\",\"\"system:serviceaccounts:kube-system\"\",\"\"system:authenticated\"\"]},\"\"sourceIPs\"\":[\"\"12.34.56.78\"\"],\"\"userAgent\"\":\"\"kube-controller-manager/v1.21.2 (linux/amd64) kubernetes/d2965f0/system:serviceaccount:kube-system:resourcequota-controller\"\",\"\"responseStatus\"\":{\"\"metadata\"\":{},\"\"code\"\":503},...\"}}\"\n</code></pre> <p>To troubleshoot a CronJob activation, search for API calls that the cronjob-controller made.</p> <pre><code>fields @logStream, @timestamp, @message| filter @logStream like /kube-apiserver-audit/\n| filter user.username like \"system:serviceaccount:kube-system:cronjob-controller\"\n| display @logStream, @timestamp, @message, objectRef.namespace, objectRef.name\n| sort @timestamp desc\n| limit 50\n</code></pre> <p>Shortened example output:</p> <pre><code>{ \"kind\": \"Event\", \"apiVersion\": \"audit.k8s.io/v1\", \"objectRef\": { \"resource\": \"cronjobs\", \"namespace\": \"default\", \"name\": \"hello\", \"apiGroup\": \"batch\", \"apiVersion\": \"v1\" }, \"responseObject\": { \"kind\": \"CronJob\", \"apiVersion\": \"batch/v1\", \"spec\": { \"schedule\": \"*/1 * * * *\" }, \"status\": { \"lastScheduleTime\": \"2021-08-09T07:19:00Z\" } } }\n</code></pre> <p>In this example output, the hello job in the default namespace runs every minute and was last scheduled at 2021-08-09T07:19:00Z.</p> <p>To find API calls that the replicaset-controller made, you can use this example query:</p> <pre><code>fields @logStream, @timestamp, @message| filter @logStream like /kube-apiserver-audit/\n| filter user.username like \"system:serviceaccount:kube-system:replicaset-controller\"\n| display @logStream, @timestamp, requestURI, verb, user.username\n| sort @timestamp desc\n| limit 50\n</code></pre> <p>Example output:</p> <pre><code>@logStream,@timestamp,requestURI,verb,user.usernamekube-apiserver-audit-8c0c570ea5676c62c44d98da6189a02b,2021-08-10 17:13:53.281,/api/v1/namespaces/kube-system/pods,create,system:serviceaccount:kube-system:replicaset-controller\nkube-apiserver-audit-4d5145b53c40d10c276ad08fa36d1f11,2021-08-04 0718:44.561,/apis/apps/v1/namespaces/kube-system/replicasets/coredns-6496b6c8b9/status,update,system:serviceaccount:kube-system:replicaset-controller\n</code></pre> <p>To find operations that are made against a Kubernetes resource, you can use this example query:</p> <pre><code>fields @logStream, @timestamp, @message| filter @logStream like /^kube-apiserver-audit/\n| filter verb == \"delete\" and requestURI like \"/api/v1/namespaces/default/pods/my-app\"\n| sort @timestamp desc\n| limit 10\n</code></pre> <p>The preceding example query filters for delete API calls on the default namespace for pod my-app. Shortened example output:</p> <pre><code>@logStream,@timestamp,@messagekube-apiserver-audit-e7b3cb08c0296daf439493a6fc9aff8c,2021-08-11 14:09:47.813,\"...\"\"requestURI\"\":\"\"/api/v1/namespaces/default/pods/my-app\"\",\"\"verb\"\":\"\"delete\"\",\"\"user\"\":{\"\"username\"\"\"\"kubernetes-admin\"\",\"\"uid\"\":\"\"heptio-authenticator-aws:12345678910:ABCDEFGHIJKLMNOP\"\",\"\"groups\"\":[\"\"system:masters\"\",\"\"system:authenticated\"\"],\"\"extra\"\":{\"\"accessKeyId\"\":[\"\"ABCDEFGHIJKLMNOP\"\"],\"\"arn\"\":[\"\"arn:aws:iam::12345678910:user/awscli\"\"],\"\"canonicalArn\"\":[\"\"arn:aws:iam::12345678910:user/awscli\"\"],\"\"sessionName\"\":[\"\"\"\"]}},\"\"sourceIPs\"\":[\"\"12.34.56.78\"\"],\"\"userAgent\"\":\"\"kubectl/v1.22.0 (darwin/amd64) kubernetes/c2b5237\"\",\"\"objectRef\"\":{\"\"resource\"\":\"\"pods\"\",\"\"namespace\"\":\"\"default\"\",\"\"name\"\":\"\"my-app\"\",\"\"apiVersion\"\":\"\"v1\"\"},\"\"responseStatus\"\":{\"\"metadata\"\":{},\"\"code\"\":200},\"\"requestObject\"\":{\"\"kind\"\":\"\"DeleteOptions\"\",\"\"apiVersion\"\":\"\"v1\"\",\"\"propagationPolicy\"\":\"\"Background\"\"},\n...\"\n</code></pre> <p>To retrieve a count of HTTP response codes for calls made to the Kubernetes API server, you can use this example query:</p> <pre><code>fields @logStream, @timestamp, @message| filter @logStream like /^kube-apiserver-audit/\n| stats count(*) as count by responseStatus.code\n| sort count desc\n</code></pre> <p>Example output:</p> <pre><code>responseStatus.code,count200,35066\n201,525\n403,125\n404,116\n101,2\n</code></pre> <p>To find changes that are made to DaemonSets/Addons in the kube-system namespace, you can use this example query:</p> <pre><code>filter @logStream like /^kube-apiserver-audit/| fields @logStream, @timestamp, @message\n| filter verb like /(create|update|delete)/ and strcontains(requestURI,\"/apis/apps/v1/namespaces/kube-system/daemonsets\")\n| sort @timestamp desc\n| limit 50\n</code></pre> <p>Example output:</p> <pre><code>{ \"kind\": \"Event\", \"apiVersion\": \"audit.k8s.io/v1\", \"level\": \"RequestResponse\", \"auditID\": \"93e24148-0aa6-4166-8086-a689b0031612\", \"stage\": \"ResponseComplete\", \"requestURI\": \"/apis/apps/v1/namespaces/kube-system/daemonsets/aws-node?fieldManager=kubectl-set\", \"verb\": \"patch\", \"user\": { \"username\": \"kubernetes-admin\", \"groups\": [ \"system:masters\", \"system:authenticated\" ] }, \"userAgent\": \"kubectl/v1.22.2 (darwin/amd64) kubernetes/8b5a191\", \"objectRef\": { \"resource\": \"daemonsets\", \"namespace\": \"kube-system\", \"name\": \"aws-node\", \"apiGroup\": \"apps\", \"apiVersion\": \"v1\" }, \"requestObject\": { \"REDACTED\": \"REDACTED\" }, \"requestReceivedTimestamp\": \"2021-08-09T08:07:21.868376Z\", \"stageTimestamp\": \"2021-08-09T08:07:21.883489Z\", \"annotations\": { \"authorization.k8s.io/decision\": \"allow\", \"authorization.k8s.io/reason\": \"\" } }\n</code></pre> <p>In this example output, the kubernetes-admin user used kubectl v1.22.2 to patch the aws-node DaemonSet.</p> <p>To find the user that deleted a node, you can use this example query:</p> <pre><code>fields @logStream, @timestamp, @message| filter @logStream like /^kube-apiserver-audit/\n| filter verb == \"delete\" and requestURI like \"/api/v1/nodes\"\n| sort @timestamp desc\n| limit 10\n</code></pre> <p>Shortened example output:</p> <pre><code>@logStream,@timestamp,@messagekube-apiserver-audit-e503271cd443efdbd2050ae8ca0794eb,2022-03-25 07:26:55.661,\"{\"kind\":\"Event\",\"\n</code></pre> <p>Finally, if you have started using control plane logging feature, we would highly recommend you to learn more about Understanding and Cost Optimizing Amazon EKS Control Plane Logs.</p>"},{"location":"guides/containers/aws-native/eks/log-aggregation/#amazon-eks-data-plane-logging","title":"Amazon EKS data plane logging","text":"<p>We recommend that you use CloudWatch Container Insights to capture logs and metrics for Amazon EKS. Container Insights implements cluster, node, and pod-level metrics with the CloudWatch agent, and Fluent Bit or Fluentd for log capture to CloudWatch. Container Insights also provides automatic dashboards with layered views of your captured CloudWatch metrics. Container Insights is deployed as CloudWatch DaemonSet and Fluent Bit DaemonSet that runs on every Amazon EKS node. Fargate nodes are not supported by Container Insights because the nodes are managed by AWS and don\u2019t support DaemonSets. Fargate logging for Amazon EKS is covered separately in this guide.</p> <p>The following table shows the CloudWatch log groups and logs captured by the default Fluentd or Fluent Bit log capture configuration for Amazon EKS.</p> <code>/aws/containerinsights/Cluster_Name/host</code> Logs from <code>/var/log/dmesg</code>, <code>/var/log/secure</code>, and <code>/var/log/messages</code>. <code>/aws/containerinsights/Cluster_Name/dataplane</code> The logs in <code>/var/log/journal</code> for <code>kubelet.service</code>, <code>kubeproxy.service</code>, and <code>docker.service</code>. <p>If you don\u2019t want to use Container Insights with Fluent Bit or Fluentd for logging, you can capture node and container logs with the CloudWatch agent installed on Amazon EKS nodes. Amazon EKS nodes are EC2 instances, which means you should include them in your standard system-level logging approach for Amazon EC2. If you install the CloudWatch agent using Distributor and State Manager, then Amazon EKS nodes are also included in the CloudWatch agent installation, configuration, and update. The following table shows logs that are specific to Kubernetes and that you must capture if you aren\u2019t using Container Insights with Fluent Bit or Fluentd for logging.</p> <code>var/log/aws-routed-eni/ipamd.log``/var/log/aws-routed-eni/plugin.log</code> The logs for the L-IPAM daemon can be found here <p>Please reference Amazon EKS node logging prescriptive guidance to learn more about data plane logging.</p>"},{"location":"guides/containers/aws-native/eks/log-aggregation/#amazon-eks-application-logging","title":"Amazon EKS application logging","text":"<p>Amazon EKS application logging becomes inevitable while running applications at scale in Kubernetes environment. To collect application logs you must install a log aggregator, such as Fluent Bit, Fluentd, or CloudWatch Container Insights, in your Amazon EKS cluster.</p> <p>Fluent Bit is an open-source log processor and forwarder that is written in C++, which means that you can collect data from different sources, enrich them with filters, and send them to multiple destinations. By using this guide's solution you can enable <code>aws-for-fluent-bit</code> or <code>fargate-fluentbit</code> for logging. Fluentd is an open-source data collector for unified logging layer and written in Ruby. Fluentd acts as a unified logging layer that can aggregate data from multiple sources, unify data with different formats into JSON-formatted objects, and route them to different output destinations. Choosing a log collector is important for CPU and memory utilization when you monitor thousands of servers. If you have multiple Amazon EKS clusters, you can use Fluent Bit as a lightweight shipper to collect data from different nodes in the cluster and forward it to Fluentd for aggregation, processing and routing to a supported output destination.</p> <p>We recommend to use Fluent Bit as the log collector and forwarder to send application and cluster logs to CloudWatch. You can then stream the logs to Amazon OpenSearch Service by using a subscription filter in CloudWatch. This option is shown in this section's architecture diagram.</p> <p></p> <p>Figure: Amazon EKS application logging architecture.</p> <p>The diagram shows the following workflow when application logs from Amazon EKS clusters are streamed to Amazon OpenSearch Service. The Fluent Bit service in the Amazon EKS cluster pushes the logs to CloudWatch. The AWS Lambda function streams the logs to Amazon OpenSearch Service using a subscription filter. You can then use Kibana to visualize the logs in the configured indexes. You can also stream logs by using Amazon Kinesis Data Firehose and store them in an S3 bucket for analysis and querying with Amazon Athena.</p> <p>In most clusters, using Fluentd or Fluent Bit for log aggregation needs little optimization. This changes when you\u2019re dealing with larger clusters with thousands of pods and nodes. We have published our findings from studying the impact of Fluentd and Fluent Bit in clusters with thousands of pods. For further learning, we would recommend you to check on enhancement to Fluent Bit that is designed to reduce the volume API calls it makes to the Kubernetes API servers using Use_Kubelet option. This Fluent Bit\u2019s <code>Use_Kubelet</code> feature allows it to retrieve pod metadata from the kubelet on the host. Amazon EKS customers can use Fluent Bit to capture logs in clusters that run tens of thousands of pods with this feature enabled without overloading the Kubernetes API server. We recommend enabling the feature even if you aren\u2019t running a large Kubernetes cluster.</p>"},{"location":"guides/containers/aws-native/eks/log-aggregation/#logging-for-amazon-eks-on-fargate","title":"Logging for Amazon EKS on Fargate","text":"<p>With Amazon EKS on Fargate, you can deploy pods without allocating or managing your Kubernetes nodes. This removes the need to capture system-level logs for your Kubernetes nodes. To capture the logs from your Fargate pods, you can use Fluent Bit to forward the logs directly to CloudWatch. This enables you to automatically route logs to CloudWatch without further configuration or a sidecar container for your Amazon EKS pods on Fargate. For more information about this, see Fargate logging in the Amazon EKS documentation and Fluent Bit for Amazon EKS on the AWS Blog. This solution captures the <code>STDOUT</code> and <code>STDERR</code> input/output (I/O) streams from your container and sends them to CloudWatch through Fluent Bit, based on the Fluent Bit configuration established for the Amazon EKS cluster on Fargate.</p> <p>With Fluent Bit support for Amazon EKS, you no longer need to run a sidecar to route container logs from Amazon EKS pods running on Fargate. With the new built-in logging support, you can select a destination of your choice to send the records to. Amazon EKS on Fargate uses a version of Fluent Bit for AWS, an upstream conformant distribution of Fluent Bit managed by AWS.</p> <p></p> <p>Figure: Logging for Amazon EKS on Fargate.</p> <p>Please learn more about Fluent Bit support for Amazon EKS see Fargate logging in the Amazon EKS documentation.</p> <p>For some reasons, for pods running on AWS Fargate where you need to use the sidecar pattern. You can run a Fluentd (or Fluent Bit) sidecar container to capture logs produced by your applications. This option requires that the application writes logs to filesystem instead of <code>stdout</code> or <code>stderr</code>. A consequence of this approach is that you will not be able use <code>kubectl</code> logs to view container logs. To make logs appear in <code>kubectl logs</code>, you can write application logs to both <code>stdout</code> and filesystem simultaneously.</p> <p>Pods on Fargate get 20GB of ephemeral storage, which is available to all the containers that belong to a pod. You can configure your application to write logs to the local filesystem and instruct Fluentd to watch the log directory (or file). Fluentd will read events from the tail of log files and send the events to a destination like CloudWatch for storage. Ensure that you rotate logs regularly to prevent logs from usurping the entire volume.</p> <p>Please learn more about How to capture application logs when using Amazon EKS on AWS Fargate to operate and observe your kubernets applications at scale on AWS Fargate. We will also <code>tee</code> write to file and <code>stdout</code> so we see logs in <code>kubectl logs</code> in this approach.</p>"},{"location":"guides/containers/aws-native/eks/log-aggregation/#unified-log-aggregation-from-amazon-eks-and-other-compute-platforms-using-aws-native-services","title":"Unified log aggregation from Amazon EKS and other compute platforms using AWS Native services","text":"<p>Customers these days want to  unify and centralize logs across different computing platforms such as Amazon Elastic Kubernetes Service (Amazon EKS), Amazon Elastic Compute Cloud (Amazon EC2), Amazon Elastic Container Service (Amazon ECS), Amazon Kinesis Data Firehose, and AWS Lambda using agents, log routers, and extensions. We can then use Amazon OpenSearch Service with OpenSearch Dashboards to visualize and analyze the logs, collected across different computing platforms to get application insights. </p> <p>A unified aggregated log system provides the following benefits:</p> <ul> <li>A single point of access to all the logs across different computing platforms</li> <li>Help defining and standardizing the transformations of logs before they get delivered to downstream systems like Amazon Simple Storage Service (Amazon S3), Amazon OpenSearch Service, Amazon Redshift, and other services</li> <li>The ability to use Amazon OpenSearch Service to quickly index, and OpenSearch Dashboards to search and visualize logs from its routers, applications, and other devices</li> </ul> <p>The following diagram shows the architecture which performs log aggregation across different compute platforms such as  Amazon Elastic Kubernetes Service (Amazon EKS), Amazon Elastic Compute Cloud (Amazon EC2), Amazon Elastic Container Service (Amazon ECS),and AWS Lambda.</p> <p></p> <p>Figure: Log aggregation across different compute platforms.</p> <p>The architecture uses various log aggregation tools such as log agents, log routers, and Lambda extensions to collect logs from multiple compute platforms and deliver them to Kinesis Data Firehose. Kinesis Data Firehose streams the logs to Amazon OpenSearch Service. Log records that fail to get persisted in Amazon OpenSearch service will get written to AWS S3. To scale this architecture, each of these compute platforms streams the logs to a different Firehose delivery stream, added as a separate index, and rotated every 24 hours.</p> <p>For further learning check on how to unify and centralize logs across different compute platforms such as Amazon Elastic Kubernetes Service (Amazon EKS), Amazon Elastic Compute Cloud (Amazon EC2), Amazon Elastic Container Service (Amazon ECS),and AWS Lambda using Kinesis Data Firehose and Amazon OpenSearch Service. This approach allows you to analyze logs quickly and the root cause of failures, using a single platform rather than different platforms for different services. </p>"},{"location":"guides/containers/aws-native/eks/log-aggregation/#conclusion","title":"Conclusion","text":"<p>In this section of Observability best practices guide, we started with deep diving in three types of Kubernetes logging such as control plane logging, node logging, and application logging. Further we learned about unified log aggregation from Amazon EKS and other compute platforms using AWS Native services such as Kinesis Data Firehose and Amazon OpenSearch Service. For further deep dive, we would highly recommend you to practice Logs and Insights modules under AWS native Observability category of AWS One Observability Workshop.</p>"},{"location":"guides/containers/oss/ecs/best-practices-metrics-collection-1/","title":"Collecting system metrics in an ECS cluster using AWS Distro for OpenTelemetry","text":"<p>AWS Distro for OpenTelemetry (ADOT) is a secure, AWS-supported distribution of the OpenTelemetry project. Using ADOT, you can collect telemetry data from multiple sources and send correlated metrics, traces and logs to multiple monitoring solutions. ADOT may be deployed on Amazon ECS cluster in two difference patterns. </p>"},{"location":"guides/containers/oss/ecs/best-practices-metrics-collection-1/#deployment-patterns-for-adot-collector","title":"Deployment patterns for ADOT Collector","text":"<ol> <li> <p>In the sidecar pattern, the ADOT collector runs inside each task in the cluster and it processes telemetry data collected from application containers only within that task. This deployment pattern is required only when you need the collector to read task metadata from Amazon ECS Task Metadata Endpoint, and generate resource usage metrics (such as CPU, memory, network, and disk) from them.  </p> </li> <li> <p>In the central collector pattern, a single instance of ADOT collector is deployed on the cluster and it processes telemetry data from all the tasks running on the cluster. This is the most commonly used deployment pattern. The collector is deployed using either REPLICA or DAEMON service scheduler strategy. </p> </li> </ol> <p>The ADOT collector architecture has the concept of a pipeline. A single collector can contain more than one pipeline. Each pipeline is dedicated to processing one of the three types of telemetry data, namely, metrics, traces and logs. You can configure multiple pipelines for each type of telemetry data. This versatile architecture thus allows a single collector to perform the role of multiple observability agents that would otherwise have to be deployed on the cluster. It significantly reduces the deployment footprint of obsevrability agents on the cluster. The primary components of a collector that make up a pipeline are grouped into three categories, namely, Receiver, Processor, and Exporter. There are secondary components called Extensions which provide capabilities that can be added to the collector, but which are not part of pipelines. </p> <p>Info</p> <p>Refer to the OpenTelemetry documentaton for a detailed explanation of Receivers, Processors, Exporters and Extensions.</p>"},{"location":"guides/containers/oss/ecs/best-practices-metrics-collection-1/#deploying-adot-collector-for-ecs-task-metrics-collection","title":"Deploying ADOT Collector for ECS task metrics collection","text":"<p>To collect resource utilization metrics at the ECS task level, the ADOT collector should be deployed using the sidecar pattern, using a task definition as shown below. The container image used for the collector is bundled with several pipeline configurations. You can choose one of them based on your requirments and specify the configuration file path in the command section of the container defintion. Setting this value to <code>--config=/etc/ecs/container-insights/otel-task-metrics-config.yaml</code> will result in the use of a pipeline configuration that collects resource utilization metrics and traces from other containers running within the same task as the collector and send them to Amazon CloudWatch and AWS X-Ray. Specifically, the collector uses an AWS ECS Container Metrics Receiver that reads task metadata and docker stats from Amazon ECS Task Metadata Endpoint, and generates resource usage metrics (such as CPU, memory, network, and disk) from them. </p> <pre><code>{\n    \"family\":\"AdotTask\",\n    \"taskRoleArn\":\"arn:aws:iam::123456789012:role/ECS-ADOT-Task-Role\",\n    \"executionRoleArn\":\"arn:aws:iam::123456789012:role/ECS-Task-Execution-Role\",\n    \"networkMode\":\"awsvpc\",\n    \"containerDefinitions\":[\n       {\n          \"name\":\"application-container\",\n          \"image\":\"...\"\n       },\n       {\n          \"name\":\"aws-otel-collector\",\n          \"image\":\"public.ecr.aws/aws-observability/aws-otel-collector:latest\",\n          \"cpu\":512,\n          \"memory\":1024,\n          \"command\": [\n            \"--config=/etc/ecs/container-insights/otel-task-metrics-config.yaml\"\n          ],          \n          \"portMappings\":[\n             {\n                \"containerPort\":2000,\n                \"protocol\":\"udp\"\n             }\n          ],             \n          \"essential\":true\n       }\n    ],\n    \"requiresCompatibilities\":[\n       \"EC2\"\n    ],\n    \"cpu\":\"1024\",\n    \"memory\":\"2048\"\n }\n</code></pre> <p>Info</p> <p>Refer to the documentation for details about setting up the IAM task role and task execution role that the ADOT collector will use when deployed on an Amazon ECS cluster.</p> <p>Info</p> <p>The AWS ECS Container Metrics Receiver works only for ECS Task Metadata Endpoint V4. Amazon ECS tasks on Fargate that use platform version 1.4.0 or later and Amazon ECS tasks on Amazon EC2 that are running at least version 1.39.0 of the Amazon ECS container agent can utilize this receiver. For more information, see Amazon ECS Container Agent Versions</p> <p>As seen in the default pipeline configuration, the collector's pipeline first uses the Filter Processor which filters out a subset of metrics pertaining to CPU, memory, network, and disk usage. Then it uses the Metrics Transform Processor that performs a set of transformations to change the names of these metrics as well as update their attributes. Finally, the metrics are sent to CloudWatch as performance log events using the Amazon CloudWatch EMF Exporter. Using this default configuration will result in collection of the following resource usage metrics under the CloudWatch namespace ECS/ContainerInsights.</p> <ul> <li>MemoryUtilized</li> <li>MemoryReserved</li> <li>CpuUtilized</li> <li>CpuReserved</li> <li>NetworkRxBytes</li> <li>NetworkTxBytes</li> <li>StorageReadBytes</li> <li>StorageWriteBytes</li> </ul> <p>Info</p> <p>Note that these are the same metrics collected by Container Insights for Amazon ECS and are made readily available in CloudWatch when you enable Container Insights at the cluster or account level. Hence, enabling Container Insights is the recommended approach for collecting ECS resource usage metrics in CloudWatch.</p> <p>The AWS ECS Container Metrics Receiver emits 52 unique metrics which it reads from the Amazon ECS Task Metadata Endpoint. The complete list of metrics collected by the receiver is documented here. You may not want to send all of them to your preferred destination. If you want more explicit control over the ECS resource usage metrics, then you can create a custom pipeline configuration, filtering and transforming the available metrics with your choice of processors/transfomers and send them to a destination based on your choice of exporters. Refer to the documentation for additional examples of pipeline configurations to capture ECS task level metrics.</p> <p>If you want to use a custom pipeline configuration, then you can use the task definition shown below and deploy the collector using the sidecar pattern. Here, the configuration of the collector pipeline is loaded from a parameter named otel-collector-config in AWS SSM Parameter Store. </p> <p>Important</p> <p>The SSM Parameter Store parameter name must be exposed to the collector using an environment variable named AOT_CONFIG_CONTENT.</p> <pre><code>{\n    \"family\":\"AdotTask\",\n    \"taskRoleArn\":\"arn:aws:iam::123456789012:role/ECS-ADOT-Task-Role\",\n    \"executionRoleArn\":\"arn:aws:iam::123456789012:role/ECS-Task-Execution-Role\",\n    \"networkMode\":\"awsvpc\",\n    \"containerDefinitions\":[\n       {\n          \"name\":\"application-container\",\n          \"image\":\"...\"\n       },        \n       {\n          \"name\":\"aws-otel-collector\",\n          \"image\":\"public.ecr.aws/aws-observability/aws-otel-collector:latest\",\n          \"cpu\":512,\n          \"memory\":1024,\n          \"secrets\":[\n             {\n                \"name\":\"AOT_CONFIG_CONTENT\",\n                \"valueFrom\":\"arn:aws:ssm:us-east-1:123456789012:parameter/otel-collector-config\"\n             }\n          ],          \n          \"portMappings\":[\n             {\n                \"containerPort\":2000,\n                \"protocol\":\"udp\"\n             }\n          ],             \n          \"essential\":true\n       }\n    ],\n    \"requiresCompatibilities\":[\n       \"EC2\"\n    ],\n    \"cpu\":\"1024\",\n    \"memory\":\"2048\"\n }\n</code></pre>"},{"location":"guides/containers/oss/ecs/best-practices-metrics-collection-1/#deploying-adot-collector-for-ecs-container-instance-metrics-collection","title":"Deploying ADOT Collector for ECS container instance metrics collection","text":"<p>To collect EC2 instance-level metrics from your ECS cluster, the ADOT collector can be deployed using a task definition as shown below. It should be deployed with the daemon service scheduler strategy. You can choose a pipeline configuration bundled into the container image. The configuration file path in the command section of the container defintion should be set to <code>--config=/etc/ecs/otel-instance-metrics-config.yaml</code>. The collector uses the AWS Container Insights Receiver to collect EC2 instance-level infrastructure metrics for many resources such as such as CPU, memory, disk, and network. the metrics are sent to CloudWatch as performance log events using the Amazon CloudWatch EMF Exporter. The functionality of the collector with this configuration is equivalent to that of deploying the CloudWatch agent to an Amazon ECS cluster hosted on EC2,</p> <p>Info</p> <p>The ADOT Collector deployment for collecting EC2 instance-level metrics is not supported on ECS clusters running on AWS Fargate</p> <pre><code>{\n    \"family\":\"AdotTask\",\n    \"taskRoleArn\":\"arn:aws:iam::123456789012:role/ECS-ADOT-Task-Role\",\n    \"executionRoleArn\":\"arn:aws:iam::123456789012:role/ECS-Task-Execution-Role\",\n    \"networkMode\":\"awsvpc\",\n    \"containerDefinitions\":[\n       {\n          \"name\":\"application-container\",\n          \"image\":\"...\"\n       },\n       {\n          \"name\":\"aws-otel-collector\",\n          \"image\":\"public.ecr.aws/aws-observability/aws-otel-collector:latest\",\n          \"cpu\":512,\n          \"memory\":1024,\n          \"command\": [\n            \"--config=/etc/ecs/otel-instance-metrics-config.yaml\"\n          ],          \n          \"portMappings\":[\n             {\n                \"containerPort\":2000,\n                \"protocol\":\"udp\"\n             }\n          ],             \n          \"essential\":true\n       }\n    ],\n    \"requiresCompatibilities\":[\n       \"EC2\"\n    ],\n    \"cpu\":\"1024\",\n    \"memory\":\"2048\"\n }\n</code></pre>"},{"location":"guides/containers/oss/ecs/best-practices-metrics-collection-2/","title":"Collecting service metrics in an ECS cluster using AWS Distro for OpenTelemetry","text":""},{"location":"guides/containers/oss/ecs/best-practices-metrics-collection-2/#deploying-adot-collector-with-default-configuration","title":"Deploying ADOT Collector with default configuration","text":"<p>The ADOT collector can be deployed using a task definition as shown below, using the sidecar pattern. The container image used for the collector is bundled with two collector pipeline configurations which can be specified in the command section of the container defintion. Seting this value <code>--config=/etc/ecs/ecs-default-config.yaml</code> will result in the use of a pipeline configuration that will collect application metrics and traces from other containers running within the same task as the collector and send them to Amazon CloudWatch and AWS X-Ray. Specifically, the collector uses an OpenTelemetry Protocol (OTLP) Receiver to receive metrics sent by applications that have been instrumented with OpenTelemetry SDKs as well as a StatsD Receiver to collect StatsD metrics. Additionally, it uses an AWS X-ray Receiver to collect traces from applications that have been instrumented with AWS X-Ray SDK.</p> <p>Info</p> <p>Refer to the documentation for details about setting up the IAM task role and task execution role that the ADOT collector will use when deployed on an Amazon ECS cluster.</p> <pre><code>{\n    \"family\":\"AdotTask\",\n    \"taskRoleArn\":\"arn:aws:iam::123456789012:role/ECS-ADOT-Task-Role\",\n    \"executionRoleArn\":\"arn:aws:iam::123456789012:role/ECS-Task-Execution-Role\",\n    \"networkMode\":\"awsvpc\",\n    \"containerDefinitions\":[\n       {\n          \"name\":\"application-container\",\n          \"image\":\"...\"\n       },      \n       {\n          \"name\":\"aws-otel-collector\",\n          \"image\":\"public.ecr.aws/aws-observability/aws-otel-collector:latest\",\n          \"cpu\":512,\n          \"memory\":1024,\n          \"command\": [\n            \"--config=/etc/ecs/ecs-default-config.yaml\"\n          ],          \n          \"portMappings\":[\n             {\n                \"containerPort\":2000,\n                \"protocol\":\"udp\"\n             }\n          ],             \n          \"essential\":true\n       }\n    ],\n    \"requiresCompatibilities\":[\n       \"EC2\"\n    ],\n    \"cpu\":\"1024\",\n    \"memory\":\"2048\"\n }\n</code></pre>"},{"location":"guides/containers/oss/ecs/best-practices-metrics-collection-2/#deploying-adot-collector-for-prometheus-metrics-collection","title":"Deploying ADOT Collector for Prometheus metrics collection","text":"<p>To deploy ADOT with the central collector pattern, with a pipeline that is different from the default configuration, the task definition shown below can be used. Here, the configuration of the collector pipeline is loaded from a parameter named otel-collector-config in AWS SSM Parameter Store. The collector is launched using REPLICA service scheduler strategy. </p> <pre><code>{\n    \"family\":\"AdotTask\",\n    \"taskRoleArn\":\"arn:aws:iam::123456789012:role/ECS-ADOT-Task-Role\",\n    \"executionRoleArn\":\"arn:aws:iam::123456789012:role/ECS-Task-Execution-Role\",\n    \"networkMode\":\"awsvpc\",\n    \"containerDefinitions\":[\n       {\n          \"name\":\"aws-otel-collector\",\n          \"image\":\"public.ecr.aws/aws-observability/aws-otel-collector:latest\",\n          \"cpu\":512,\n          \"memory\":1024,\n          \"secrets\":[\n             {\n                \"name\":\"AOT_CONFIG_CONTENT\",\n                \"valueFrom\":\"arn:aws:ssm:us-east-1:123456789012:parameter/otel-collector-config\"\n             }\n          ],          \n          \"portMappings\":[\n             {\n                \"containerPort\":2000,\n                \"protocol\":\"udp\"\n             }\n          ],             \n          \"essential\":true\n       }\n    ],\n    \"requiresCompatibilities\":[\n       \"EC2\"\n    ],\n    \"cpu\":\"1024\",\n    \"memory\":\"2048\"\n }\n</code></pre> <p>Important</p> <p>The SSM Parameter Store parameter name must be exposed to the collector using an environment variable named AOT_CONFIG_CONTENT.</p> <p>Important</p> <p>When using the ADOT collector for Prometheus metrics collection from applications and deploying it with REPLICA service scheduler startegy, make sure that you set the replica count to 1. Deploying more than 1 replica of the collector will result in an incorrect representation of metrics data in the target destination.</p> <p>The configuration shown below enables the ADOT collector to collect Prometheus metrics from services in the cluster using a Prometheus Receiver. The receiver is meant to minimally be a drop-in replacement for Prometheus server. To collect metrics with this receiver, you need a mechanism for discovering the set of target services to be scraped. The receiver supports both static and dynamic discovery of scraping targets using one of the dozens of supported service-discovery mechanisms. </p> <p>As Amazon ECS does not have any built-in service discovery mechanism, the collector relies on Prometheus' support for file-based discovery of targets. To setup the Prometheus receiver for file-based discovery of targets, the collector makes use of the Amazon ECS Observer extension. The extension uses ECS/EC2 API to discover Prometheus scrape targets from all running tasks and filter them based on service names, task definitions and container labels listed under the ecs_observer/task_definitions section in the configuration. All discovered targets are written into the file specified by the result_file field, which resides on the file system mounted to ADOT collector container. Subequently, the Prometheus receiver scrapes metrics from the targets listed in this file. </p>"},{"location":"guides/containers/oss/ecs/best-practices-metrics-collection-2/#sending-metrics-data-to-amazon-managed-prometheus-workspace","title":"Sending metrics data to Amazon Managed Prometheus workspace","text":"<p>The metrics collected by the Prometheus Receiver can be sent to an Amazon Managed Prometheus workspace using a Prometheus Remote Write Exporter in the collector pipeline, as shown in the exporters section of the configuration below. The exporter is configured with the remote write URL of the workspace and it sends the metrics data using HTTP POST requests. It makes use of the built-in AWS Signature Version 4 authenticator to sign the requests sent to the workspace. </p> <pre><code>extensions:\n  health_check:\n  sigv4auth:\n    region: us-east-1\n  ecs_observer:\n    refresh_interval: 60s \n    cluster_name: 'ecs-ec2-cluster'\n    cluster_region: us-east-1\n    result_file: '/etc/ecs_sd_targets.yaml' \n    services:\n      - name_pattern: '^WebAppService$'\n    task_definitions:\n      - job_name: \"webapp-tasks\"\n        arn_pattern: '.*:task-definition/WebAppTask:[0-9]+'\n        metrics_path: '/metrics'\n        metrics_ports:\n          - 3000\n\nreceivers:\n  awsxray:\n  prometheus:\n    config:\n      scrape_configs:\n        - job_name: ecs_services\n          file_sd_configs:\n            - files:\n                - '/etc/ecs_sd_targets.yaml'\n              refresh_interval: 30s\n          relabel_configs: \n            - source_labels: [ __meta_ecs_cluster_name ] \n              action: replace\n              target_label: cluster\n            - source_labels: [ __meta_ecs_service_name ] \n              action: replace\n              target_label: service\n            - source_labels: [ __meta_ecs_task_definition_family ] \n              action: replace\n              target_label: taskdefinition       \n            - source_labels: [ __meta_ecs_task_container_name ] \n              action: replace\n              target_label: container                        \n\nprocessors:\n    filter/include:\n      metrics:\n        include:\n          match_type: regexp\n          metric_names:\n            - ^http_requests_total$  \n\nexporters:\n  awsxray:\n  prometheusremotewrite:\n    endpoint: https://aps-workspaces.us-east-1.amazonaws.com/workspaces/WORKSPACE_ID/api/v1/remote_write\n    auth:\n      authenticator: sigv4auth\n    resource_to_telemetry_conversion:\n      enabled: true\n\nservice:\n  extensions:\n    - ecs_observer\n    - health_check\n    - sigv4auth\n  pipelines:\n    metrics:\n      receivers: [prometheus]\n      exporters: [prometheusremotewrite]       \n    traces:\n      receivers: [awsxray]\n      exporters: [awsxray]       \n</code></pre>"},{"location":"guides/containers/oss/ecs/best-practices-metrics-collection-2/#sending-metrics-data-to-amazon-cloudwatch","title":"Sending metrics data to Amazon CloudWatch","text":"<p>Alternatively, the metrics data can be sent to Amazon CloudWatch by using the Amazon CloudWatch EMF Exporter in the collector pipeline, as shown in the exporters section of the configuration below. This exporter sends metrics data to CloudWatch as performance log events. The metric_declaration field in the exporter is used to specify the array of logs with embedded metric format to be generated. The configurtion below will generate log events only for a metric named http_requests_total. Using this data, CloudWatch will create the metric http_requests_total under the CloudWatch namespace ECS/ContainerInsights/Prometheus with the dimensions ClusterName, ServiceName and TaskDefinitionFamily.</p> <pre><code>extensions:\n  health_check:\n  sigv4auth:\n    region: us-east-1\n  ecs_observer:\n    refresh_interval: 60s \n    cluster_name: 'ecs-ec2-cluster'\n    cluster_region: us-east-1\n    result_file: '/etc/ecs_sd_targets.yaml' \n    services:\n      - name_pattern: '^WebAppService$'\n    task_definitions:\n      - job_name: \"webapp-tasks\"\n        arn_pattern: '.*:task-definition/WebAppTask:[0-9]+'\n        metrics_path: '/metrics'\n        metrics_ports:\n          - 3000\n\nreceivers:\n  awsxray:\n  prometheus:\n    config:\n      global:\n        scrape_interval: 15s\n        scrape_timeout: 10s\n      scrape_configs:\n        - job_name: ecs_services\n          file_sd_configs::\n            - files:\n                - '/etc/ecs_sd_targets.yaml'\n          relabel_configs: \n            - source_labels: [ __meta_ecs_cluster_name ] \n              action: replace\n              target_label: ClusterName\n            - source_labels: [ __meta_ecs_service_name ] \n              action: replace\n              target_label: ServiceName\n            - source_labels: [ __meta_ecs_task_definition_family ] \n              action: replace\n              target_label: TaskDefinitionFamily       \n            - source_labels: [ __meta_ecs_task_container_name ] \n              action: replace\n              target_label: container          \n\nprocessors:\n    filter/include:\n      metrics:\n        include:\n          match_type: regexp\n          metric_names:\n            - ^http_requests_total$  \n\nexporters:\n  awsxray:\n  awsemf:\n    namespace: ECS/ContainerInsights/Prometheus\n    log_group_name: '/aws/ecs/containerinsights/{ClusterName}/prometheus'\n    dimension_rollup_option: NoDimensionRollup\n    metric_declarations:\n      - dimensions: [[ClusterName, ServiceName, TaskDefinitionFamily]]\n        metric_name_selectors:\n          - http_requests_total\n\nservice:\n  extensions:\n    - ecs_observer\n    - health_check\n    - sigv4auth\n  pipelines:\n    metrics:\n      receivers: [prometheus]\n      processors: [filter/include]\n      exporters: [awsemf]       \n    traces:\n      receivers: [awsxray]\n      exporters: [awsxray]       \n</code></pre>"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/","title":"EKS Observability : Essential Metrics","text":""},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#current-landscape","title":"Current Landscape","text":"<p>Monitoring is defined as a solution that allows infrastructure and application owners a way to see and understand both historical and current state of their systems, focused on gathering defined metrics or logs.  </p> <p>Monitoring has evolved through the years.  We started working with debug and dump logs to debug and troubleshoot issues to having basic monitoring using command-line tools like syslogs, top etc, which progressed to being able to visualize them in a dashboard.  In the advent of cloud and increase in scale,  we are tracking more today that we have ever been.  The industry has shifted more into Observability, which is defined as a solution to allow infrastructure and application owners to actively troubleshoot and debug their systems.  With Observability focusing more on looking at patterns derived from the metrics.</p>"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#metrics-why-does-it-matter","title":"Metrics, why does it matter?","text":"<p>Metrics are a series of numerical values that are kept in order with the time that they are created. They are used to track everything from the number of servers in your environment, their disk usage, number of requests they handle per second, or the latency in completing these requests.  Metrics are data that tell you how your systems are performing.  Whether you are running a small or large cluster, getting insights on your systems health and performance allows you to identify areas of improvement, ability to troubleshoot and trace an issue, as well as improve your workloads performance and efficiency as whole.  These changes can impact how much time and resources you spend on your cluster, which translates directly into cost.</p>"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#metrics-collection","title":"Metrics Collection","text":"<p>Collecting metrics from an EKS cluster consists of three components :</p> <ol> <li>Sources: where metrics come from like the ones listed in this guide.</li> <li>Agents: Applications running in the EKS environment, often called an agent, which collects the metrics monitoring data and pushes this data to the second component. Some examples of this component are AWS Distro for OpenTelemetry (ADOT) and CloudWatch Agent</li> <li>Destinations: A monitoring data storage and analysis solution, this component is typically a data service that is optimized for time series formatted data. Some examples of this component are Amazon Managed Service for Prometheus and AWS Cloudwatch.</li> </ol> <p>Note: In this section, configuration examples are links to relevant sections of the AWS Observability Accelerator. This is to ensure you get up to date guidance and examples on EKS metrics collection implementations.</p>"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#managed-open-source-solution","title":"Managed Open Source Solution","text":"<p>AWS Distro for OpenTelemetry (ADOT) is a supported version of the OpenTelemetry project that enables users to send correlated metrics and traces to various monitoring data collection solutions like Amazon Managed Service for Prometheus and AWS Cloudwatch. ADOT can be installed through EKS Managed Add-ons on to an EKS cluster and configured to collect metrics (like the ones listed on this page) and workload traces. AWS has validated that the ADOT add-on is compatible with Amazon EKS, and it is regularly updated with the latest bug fixes and security patches. ADOT best practices and more information.</p>"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#adot-amp","title":"ADOT + AMP","text":"<p>The quickest way to get up and running with AWS Distro for OpenTelemetry (ADOT), Amazon Managed Service for Prometheus (AMP), and Amazon Managed Service for Grafana (AMG) is to utilize the infrastructure monitoring example from AWS Observability Accelerator. The accelerator examples deploy the tools and services in your environment with out of the box metrics collection, alerting rules and Grafana dashboards.</p> <p>Please refer to the AWS documentation for additional information on installation, configuration and operation of EKS Managed Add-on for ADOT.</p>"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#sources","title":"Sources","text":"<p>EKS metrics are created from multiple locations at different layers of an overall solution. This is a table summarizing the metrics sources that are called out in essential metrics section.</p> Layer Source Tool Installation and More info Helm Chart Control Plane api server endpoint/metrics N/A - api server exposes metrics in prometheus format directly https://docs.aws.amazon.com/eks/latest/userguide/prometheus.html N/A Cluster State kube-state-metrics-http-endpoint:8080/metrics kube-state-metrics https://github.com/kubernetes/kube-state-metrics#overview https://github.com/kubernetes/kube-state-metrics#helm-chart Kube Proxy kube-proxy-http:10249/metrics N/A - kube proxy exposes metrics in prometheus format directly https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/ N/A VPC CNI vpc-cni-metrics-helper/metrics cni-metrics-helper https://github.com/aws/amazon-vpc-cni-k8s/blob/master/cmd/cni-metrics-helper/README.md https://github.com/aws/amazon-vpc-cni-k8s/tree/master/charts/cni-metrics-helper Core DNS core-dns:9153/metrics N/A - core DNS exposes metrics in prometheus format directly https://github.com/coredns/coredns/tree/master/plugin/metrics N/A Node prom-node-exporter-http:9100/metrics prom-node-exporter https://github.com/prometheus/node_exporter https://prometheus.io/docs/guides/node-exporter/#node-exporter-metrics https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter Kubelet/Pod kubelet/metrics/cadvisor kubelet or proxied through api server https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/ N/A"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#agent-aws-distro-for-opentelemetry","title":"Agent : AWS Distro for OpenTelemetry","text":"<p>AWS recommends installation, configuration and operations of ADOT on your EKS cluster through the AWS EKS ADOT managed addon. This addon utilized the ADOT operator/collector custom resource model allowing you to deploy, configure and manage multiple ADOT collectors on your cluster. For detailed information on installation, advanced configuration and operations of this addon check out this documentation.</p> <p>Note: The AWS EKS ADOT managed addon web console can be used for advanced configuration of the ADOT addon.</p> <p>There are two components to the ADOT collector configuration.</p> <ol> <li>The collector configuration which includes collector deployment mode (deployment, daemonset, etc).</li> <li>The OpenTelemetry Pipeline configuration which includes what receivers, processors, and exporters are needed for metrics collection. Example configuration snippet:</li> </ol> <pre><code>config: |\n    extensions:\n      sigv4auth:\n        region: &lt;YOUR_AWS_REGION&gt;\n        service: \"aps\"\n\n    receivers:\n      #\n      # Scrape configuration for the Prometheus Receiver\n      # This is the same configuration used when Prometheus is installed using the community Helm chart\n      #  \n      prometheus:\n        config:\n          global:\n            scrape_interval: 60s\n            scrape_timeout: 10s\n\n          scrape_configs:\n          - job_name: kubernetes-apiservers\n            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n            kubernetes_sd_configs:\n            - role: endpoints\n            relabel_configs:\n            - action: keep\n              regex: default;kubernetes;https\n              source_labels:\n              - __meta_kubernetes_namespace\n              - __meta_kubernetes_service_name\n              - __meta_kubernetes_endpoint_port_name\n            scheme: https\n            tls_config:\n              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n              insecure_skip_verify: true\n\n              ...\n              ...\n\n    exporters:\n      prometheusremotewrite:\n        endpoint: &lt;YOUR AMP WRITE ENDPOINT URL&gt;\n        auth:\n          authenticator: sigv4auth\n      logging:\n        loglevel: warn\n    extensions:\n      sigv4auth:\n        region: &lt;YOUR_AWS_REGION&gt;\n        service: aps\n      health_check:\n      pprof:\n        endpoint: :1888\n      zpages:\n        endpoint: :55679\n    processors:\n      batch/metrics:\n        timeout: 30s\n        send_batch_size: 500\n    service:\n      extensions: [pprof, zpages, health_check, sigv4auth]\n      pipelines:\n        metrics:\n          receivers: [prometheus]\n          processors: [batch/metrics]\n          exporters: [logging, prometheusremotewrite]\n</code></pre> <p>A complete best practices collector configuration, ADOT pipeline configuration and Prometheus scrape configuration can be found here as a Helm Chart in the Observability Accelerator.</p>"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#destination-amazon-managed-service-for-prometheus","title":"Destination: Amazon Managed Service for Prometheus","text":"<p>The ADOT collector pipeline utilizes Prometheus Remote Write capabilities to export metrics to an AMP instance. Example configuration snippet, note the AMP WRITE ENDPOINT URL</p> <pre><code>    exporters:\n      prometheusremotewrite:\n        endpoint: &lt;YOUR AMP WRITE ENDPOINT URL&gt;\n        auth:\n          authenticator: sigv4auth\n      logging:\n        loglevel: warn\n</code></pre> <p>A complete best practices collector configuration, ADOT pipeline configuration and Prometheus scrape configuration can be found here as a Helm Chart in the Observability Accelerator.</p> <p>Best practices on AMP configuration and usage is here.</p>"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#what-are-the-relevant-metrics","title":"What are the relevant metrics?","text":"<p>Gone are the days where you have little metrics available, nowadays it is the opposite, there are hundreds of metrics available.  Being able to determine what are the relevant metrics is important towards building a system with an observability first mindset.</p> <p>This guide outlines the different grouping of metrics available to you and explains which ones you should focus on as you build observability into your infrastructure and applications.  The list of metrics below are the list of metrics we recommend monitoring based on best practices.</p> <p>The metrics listed in the following sections are in addition to the metrics highlighted in the AWS Observability Accelerator Grafana Dashboards and Kube Prometheus Stack Dashboards.</p>"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#control-plane-metrics","title":"Control Plane Metrics","text":"<p>The Amazon EKS control plane is managed by AWS for you and runs in an account managed by AWS.  It consists of control plane nodes that run the Kubernetes components, such as etcd and the Kubernetes API server.   Kubernetes publishes various events to keep users informed of activities in the cluster, such as spinning up and tearing down pods, deployments, namespaces, and more.  The Amazon EKS control plane is a critical component that you need to track to make sure the core components are able function properly and perform the fundamental activities required by your cluster.</p> <p>The Control Plane API Server exposes thousands of metrics, the table below lists the essential control plane metrics that we recommend monitoring.</p> Name Metric Description Reason API Server total requests apiserver_request_total Counter of apiserver requests broken out for each verb, dry run value, group, version, resource, scope, component, and HTTP response code. API Server latency apiserver_request_duration_seconds Response latency distribution in seconds for each verb, dry  run value, group, version, resource, subresource, scope, and component. Request latency rest_client_request_duration_seconds Request latency in seconds. Broken down by verb and URL. Total requests rest_client_requests_total Number of HTTP requests, partitioned by status code, method,  and host. API Server request duration apiserver_request_duration_seconds_bucket Measures the latency for each request to the Kubernetes API server in seconds API server request latency sum apiserver_request_latencies_sum Cumulative Counter which tracks total time taken by the K8 API server to process requests API server registered watchers apiserver_registered_watchers The number of currently registered watchers for a given resource API server number of objects apiserver_storage_object Number of stored objects at the time of last check split by kind. Admission controller latency apiserver_admission_controller_admission_duration_seconds Admission controller latency histogram in seconds, identified  by name and broken out for each operation and API resource and type (validate  or admit). Etcd latency etcd_request_duration_seconds Etcd request latency in seconds for each operation and object  type. Etcd DB size apiserver_storage_db_total_size_in_bytes Etcd database size. This helps you proactively monitor etcd database usage, and avoid overrunning the limit."},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#cluster-state-metrics","title":"Cluster State metrics","text":"<p>The Cluster State Metrics are generated by <code>kube-state-metrics</code> (KSM). KSM is a utility that runs as a pod in the cluster, listening to the Kubernetes API Server, providing you insights into your cluster state and Kubernetes objects in your cluster as Prometheus metrics.  KSM will need to be installed before these metrics are available.  These metrics are used by Kubernetes to effectively do pod scheduling, and is focused on the health of various objects inside, such as deployments, replica sets, nodes and pods.  Cluster state metrics expose pod information on status, capacity and availability.  Its essential to keep track on how your cluster is performing on scheduling tasks for your cluster so you can keep track performance, get ahead of issues and monitor the health of your cluster. There are about X number of exposed Cluster State Metrics, the table below lists the essential metrics that should be tracked.</p> Name Metric Description Node status kube_node_status_condition Current health status of the node. Returns a set of node conditions and <code>true</code>, <code>false</code>, or <code>unknown</code> for each Desired pods kube_deployment_spec_replicas or kube_daemonset_status_desired_number_scheduled Number of pods specified for a Deployment or DaemonSet Current pods kube_deployment_status_replicas or kube_daemonset_status_current_number_scheduled Number of pods currently running in a Deployment or DaemonSet Pod capacity kube_node_status_capacity_pods Maximum pods allowed on the node Available pods kube_deployment_status_replicas_available or kube_daemonset_status_number_available Number of pods currently available for a Deployment or DaemonSet Unavailable pods kube_deployment_status_replicas_unavailable or kube_daemonset_status_number_unavailable Number of pods currently not available for a Deployment or DaemonSet Pod readiness kube_pod_status_ready If a pod is ready to serve client requests Pod status kube_pod_status_phase Current status of the pod; value would be pending/running/succeeded/failed/unknown Pod waiting reason kube_pod_container_status_waiting_reason Reason a container is in a waiting state Pod termination status kube_pod_container_status_terminated Whether the container is currently in a terminated state or not Pods pending scheduling pending_pods Number of pods awaiting node assignment Pod scheduling attempts pod_scheduling_attempts Number of attempts made to schedule pods"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#cluster-add-on-metrics","title":"Cluster Add-on Metrics","text":"<p>Cluster add-on is software that provides supporting operational capabilities to Kubernetes applications.  This includes software like observability agents or Kubernetes drives that allow the cluster to interact with underlying AWS resources for networking, compute and storage.   Add-on software is typically built and maintained by the Kubernetes community, cloud providers like AWS, or third-party vendors. Amazon EKS automatically installs self-managed add-ons such as the Amazon VPC CNI plugin for Kubernetes, <code>kube-proxy</code>, and CoreDNS for every cluster.</p> <p>These Cluster add-ons provide operational support in different areas like networking, domain name resolution, etc.  They provide you with insights on how the critical supporting infrastructure and components are operating. Tracking add-on metrics are important to understand your clusters operational health.</p> <p>Below are the essential add-ons that you should consider monitoring along with their essential metrics.</p>"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#amazon-vpc-cni-plugin","title":"Amazon VPC CNI Plugin","text":"<p>Amazon EKS implements cluster networking through the Amazon VPC Container Network Interface (VPC CNI) plugin.  The CNI plugin allows Kubernetes Pods to have the same IP address ad they do on the VPC network.  More specifically, all containers inside the Pod share a network namespace, and they can communicate with each-other using local ports. The VPC CNI add-on enables you to continuously ensure the security and stability of your Amazon EKS clusters and decrease the amount of effort required to install, configure and update add-ons.</p> <p>VPC CNI add-on metrics are exposed by the CNI Metrics Helper. Monitoring the IP address allocation is fundamental to ensuring a healthy cluster and avoiding IP exhaustion issues. Here is the latest networking best practices and VPC CNI metrics to collect and monitor.</p>"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#coredns-metrics","title":"CoreDNS Metrics","text":"<p>CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS.  The CoreDNS pods provide name resolution for all pods in the cluster.  Running DNS intensive workloads can sometimes experience intermittent CoreDNS failures due to DNS throttling, and this can impact applications.  </p> <p>Checkout the latest best practices for tracking key CoreDNS performance metrics here and Monitoring CoreDNS traffic for DNS throttling issues</p>"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#podcontainer-metrics","title":"Pod/Container Metrics","text":"<p>Tracking usage in across all layers of you application is important, these includes taking a closer look at your nodes and pods running inside your cluster. Out of all the metrics available at the pod dimension, this list of metrics are of practical use for you to understand the state of the workloads running on your cluster. Tracking CPU, memory and network usage allows for diagnosing and troubleshooting application related issues. Tracking your workload metrics provide you insights into your resource utilization to right size your workloads running on EKS.</p> Metric Example PromQL Query Dimension Number of running pods per namspace count by(namespace) (kube_pod_info) Per Cluster by Namespace CPU usage per container per pod sum(rate(container_cpu_usage_seconds_total{container!=\"\"}[5m])) by (namespace, pod) Per Cluster by Namespace by Pod Memory utilization per pod sum(container_memory_usage_bytes{container!=\"\"}) by (namespace, pod) Per Cluster by Namespace by Pod Network Received Bytes per pod sum by(pod) (rate(container_network_receive_bytes_total[5m])) Per Cluster by Namespace by Pod Network Transmitted Bytes per pod sum by(pod) (rate(container_network_transmit_bytes_total[5m])) Per Cluster by Namespace by Pod The number of container restarts per container increase(kube_pod_container_status_restarts_total[15m]) &gt; 3 Per Cluster by Namespace by Pod"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#node-metrics","title":"Node Metrics","text":"<p>Kube State Metrics and Prometheus node exporter gathers metric statistics on the nodes in your cluster.  Tracking your nodes status, cpu usage, memory, filesystem and traffic is important to understand your node utilization.  Understanding how your nodes resources are being utilized is important for properly selecting instance types and storage to effectively the types of workloads you expect to run on your cluster.  The metrics below are some of the essential metrics that you should be tracking.</p> Metric Example PromQL Query Dimension Node CPU Utilization sum(rate(container_cpu_usage_seconds_total{container!=\"\"}[5m])) by (node) Per Cluster by Node Node Memory Utilization sum(container_memory_usage_bytes{container!=\"\"}) by (node) Per Cluster by Node Node Network Total Bytes sum by (instance) (rate(node_network_receive_bytes_total[3m]))+sum by (instance) (rate(node_network_transmit_bytes_total[3m])) Per Cluster by Node Node CPU Reserved Capacity sum(kube_node_status_capacity{cluster!=\"\"}) by (node) Per Cluster by Node Number of Running Pods per Node sum(kubelet_running_pods) by (instance) Per Cluster by Node Node Filesystem Usage rate(container_fs_reads_bytes_total{job=\"kubelet\", device=~\"mmcblk.p.+ .*nvme.+ Cluster CPU Utilization sum(rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\"}[5m])) Per Cluster Cluster Memory Utilization 1 - sum(:node_memory_MemAvailable_bytes:sum{cluster=\"\"}) / sum(node_memory_MemTotal_bytes{job=\"node-exporter\",cluster=\"\"}) Per Cluster Cluster Network Total Bytes sum(rate(node_network_receive_bytes_total[3m]))+sum(rate(node_network_transmit_bytes_total[3m])) Per Cluster Number of Running Pods sum(kubelet_running_pod_count{cluster=\"\"}) Per Cluster Number of Running Containers sum(kubelet_running_container_count{cluster=\"\"}) Per Cluster Cluster CPU Limit sum(kube_node_status_allocatable{resource=\"cpu\"}) Per Cluster Cluster Memory Limit sum(kube_node_status_allocatable{resource=\"memory\"}) Per Cluster Cluster Node Count count(kube_node_info) OR sum(kubelet_node_name{cluster=\"\"}) Per Cluster"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#additional-resources","title":"Additional Resources","text":""},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#aws-services","title":"AWS Services","text":"<p>https://aws-otel.github.io/</p> <p>https://aws.amazon.com/prometheus</p> <p>https://aws.amazon.com/cloudwatch/features/</p>"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#blogs","title":"Blogs","text":"<p>https://aws.amazon.com/blogs/containers/</p> <p>https://aws.amazon.com/blogs/containers/metrics-and-traces-collection-using-amazon-eks-add-ons-for-aws-distro-for-opentelemetry/</p> <p>https://aws.amazon.com/blogs/containers/</p> <p>https://aws.amazon.com/blogs/containers/introducing-amazon-cloudwatch-container-insights-for-amazon-eks-fargate-using-aws-distro-for-opentelemetry/</p>"},{"location":"guides/containers/oss/eks/best-practices-metrics-collection/#infrastructure-as-code-resources","title":"Infrastructure as Code Resources","text":"<p>https://github.com/aws-observability/terraform-aws-observability-accelerator</p> <p>https://github.com/aws-ia/terraform-aws-eks-blueprints</p>"},{"location":"guides/containers/oss/eks/resource-optimization/","title":"Resource Optimization best practices for Kubernetes workloads","text":"<p>Kubernetes adoption continues to accelerate, as many move to microservice based architectures. A lot of the initial focus was on designing and building new cloud native architectures to support the applications. As environments grow, we are starting to see the focus to optimize resource allocation from customers. Resource optimization is the second most important  question operations team ask for after security. Let's talk about guidance on how to optimize resource allocation and right-size applications on Kubernetes environments. This includes applications running on Amazon EKS deployed with managed node groups, self-managed node groups, and AWS Fargate.</p>"},{"location":"guides/containers/oss/eks/resource-optimization/#reasons-for-right-sizing-applications-on-kubernetes","title":"Reasons for Right-sizing applications on Kubernetes","text":"<p>In Kubernetes, resource right-sizing is done through setting resource specifications on applications. These settings directly impact:</p> <ul> <li>Performance \u2014 Kubernetes applications will arbitrarily compete for resources without proper resource specifications, This can adversely impact application performance.</li> <li>Cost Optimization \u2014 Applications deployed with oversized resource specifications will result in increased costs and under utilized infrastructure.</li> <li>Autoscaling \u2014 The Kubernetes Cluster Autoscaler and Horizontal Pod Autoscaling require resource specifications to function.</li> </ul> <p>The most common resource specifications in Kubernetes are for CPU and memory requests and limits.</p>"},{"location":"guides/containers/oss/eks/resource-optimization/#requests-and-limits","title":"Requests and Limits","text":"<p>Containerized applications are deployed on Kubernetes as Pods. CPU and memory requests and limits are an optional part of the Pod definition. CPU is specified in units of Kubernetes CPUs while memory is specified in bytes, usually as mebibytes (Mi).</p> <p>Request and limits each serve different functions in Kubernetes and impact scheduling and resource enforcement differently.</p>"},{"location":"guides/containers/oss/eks/resource-optimization/#recommendations","title":"Recommendations","text":"<p>An application owner needs to choose the \"right\" values  for their CPU and memory resource requests. An ideal way  is to load test the application in a development environment and measure resource usage using observability tooling. While that might make sense for your organization\u2019s most critical applications, it\u2019s likely not feasible for every containerized application deployed in your cluster. Let's talk about the tools that can help us optimize and right-size the workloads:</p>"},{"location":"guides/containers/oss/eks/resource-optimization/#vertical-pod-autoscaler-vpa","title":"Vertical Pod Autoscaler (VPA)","text":"<p>VPA is Kubernetes sub-project owned by the Autoscaling special interest group (SIG). It\u2019s designed to automatically set Pod requests based on observed application performance. VPA collects resource usage using the Kubernetes Metric Server by default but can be optionally configured to use Prometheus as a data source. VPA has a recommendation engine  that measures application performance and makes sizing recommendations. The VPA recommendation engine can be deployed stand-alone so VPA will not perform any autoscaling actions. It\u2019s configured by creating a VerticalPodAutoscaler custom resource for each application and VPA updates the object\u2019s status field with resource sizing recommendations. Creating VerticalPodAutoscaler objects for every application in your cluster and trying to read and interpret the JSON results is challenging at scale. Goldilocks is an open source project that makes this easy.</p>"},{"location":"guides/containers/oss/eks/resource-optimization/#goldilocks","title":"Goldilocks","text":"<p>Goldilocks is an open source project from Fairwinds that is designed to help organizations get their Kubernetes application resource requests \u201cjust right\". The default configuration of Goldilocks is an opt-in model. You choose which workloads are monitored by adding the goldilocks.fairwinds.com/enabled: true label to a namespace.</p> <p></p> <p>The Metrics Server collects resource metrics from the Kubelet running on worker nodes and exposes them through Metrics API for use by the Vertical Pod Autoscaler. The Goldilocks controller watches for namespaces with the goldilocks.fairwinds.com/enabled: true label and creates VerticalPodAutoscaler objects for each workload in those namespaces.</p> <p>To enable namespaces for resource recommendation, run the below command:</p> <pre><code>kubectl create ns javajmx-sample\nkubectl label ns javajmx-sample goldilocks.fairwinds.com/enabled=true\n</code></pre> <p>To deploy goldilocks in the Amazon EKS Cluster, run the below command:</p> <pre><code>helm repo add fairwinds-stable https://charts.fairwinds.com/stable\nhelm upgrade --install goldilocks fairwinds-stable/goldilocks --namespace goldilocks --create-namespace --set vpa.enabled=true\n</code></pre> <p>Goldilocks-dashboard will expose the dashboard in the port 8080 and we can access it to get the resource recommendation.  Let\u2019s run the below command to access the dashboard:</p> <p><pre><code>kubectl -n goldilocks port-forward svc/goldilocks-dashboard 8080:80\n</code></pre> Then open your browser to http://localhost:8080</p> <p></p> <p>Let\u2019s analyze the sample namespace to see the recommendations provided by Goldilocks. We should be able to see the recommendations for the deployment. </p> <p>We could see the request &amp; limit recommendations for the javajmx-sample workload. The Current column under each Quality of Service (Qos) indicates the currently configured CPU and Memory request and limits. The Guranteed and Burstable column indicates the recommended CPU and Memory request limits for the respective QoS.</p> <p>We can clearly notice that we have over provisioned the resources and goldilocks has made the recommendations to optimize the CPU and Memory request. The CPU request and limits has been recommended to be 15m and 15m compared to 100m and 300m for Guranteed QoS and Memory request and limits to be 105M and 105M, compared to 180Mi and 300 Mi. You can simply copy the respective manifest file for the QoS class, they are interested in and deploy the workloads which is right-sized and optimized.</p>"},{"location":"guides/containers/oss/eks/resource-optimization/#understand-throttling-using-cadvisor-metric-and-configuring-the-resource-appropriately","title":"Understand throttling using cAdvisor metric and configuring the resource appropriately","text":"<p>When we configure limits, we are telling the Linux node how long a specific containerized application can run during a specific period of time. We do this to protect the rest of the workloads on a node from a wayward set of processes from taking an unreasonable amount of CPU cycles. We are not defining several physical \u201ccores\u201d sitting on a motherboard; however, we are configuring how much time a grouping of processes or threads in a single container can run before we want to temporarily pause the container to avoid overwhelming other applications.</p> <p>There is a handy cAdvisor metrics called <code>container_cpu_cfs_throttled_seconds_total</code> which adds up all the throttled 5 ms slices and gives us an idea how far over the quota the process is. This metric is in seconds, so we divide the value by 10 to get 100 ms, which is the real period of time associated with the container.</p> <p>PromQl query to understand the top three pods CPU usage over a 100 ms time. <pre><code>topk(3, max by (pod, container)(rate(container_cpu_usage_seconds_total{image!=\"\", instance=\"$instance\"}[$__rate_interval]))) / 10\n</code></pre>  A value of 400 ms of vCPU usage is observed.</p> <p></p> <p>PromQL gives us a per second throttling, with 10 periods in a second. To get the per period throttling, we divide by 10. If we want to know how much to increase the limits setting, then we can multiple by 10 (e.g., 400 ms * 10 = 4000 m).</p> <p>While the above tools provide ways to identify opportunities for resource optimization, applications team should spend time in identifying whether a given application is CPU / Memory intensive and allocate resources to prevent throttling / over-provisioning. </p>"},{"location":"guides/cost/kubecost/","title":"Cost visibility and resource right-sizing using Kubecost","text":"<p>Kubecost provides customers with visibility into spend and resource efficiency in Kubernetes environments. At a high level, Amazon EKS cost monitoring is deployed with Kubecost, which includes Prometheus, an open-source monitoring system and time series database. Kubecost reads metrics from Prometheus then performs cost allocation calculations and writes the metrics back to Prometheus. Finally, the Kubecost front end reads metrics from Prometheus and shows them on the Kubecost user interface (UI). The architecture is illustrated by the following diagram:</p> <p></p>"},{"location":"guides/cost/kubecost/#reasons-to-use-kubecost","title":"Reasons to use Kubecost","text":"<p>As customers modernize their applications and deploy workloads using Amazon EKS, they gain efficiencies by consolidating the compute resources required to run their applications. However, this utilization efficiency comes at a tradeoff of increased difficulty measuring application costs. Today, you can use one of these methods to distribute costs by tenant:</p> <ul> <li>Hard multi-tenancy \u2014 Run separate EKS clusters in dedicated AWS accounts.</li> <li>Soft multi-tenancy \u2014 Run multiple node groups in a shared EKS cluster.</li> <li>Consumption based billing \u2014 Use resource consumption to calculate the cost incurred in a shared EKS cluster.</li> </ul> <p>With Hard multi-tenancy, workloads get deployed in separate EKS clusters and you can identify the cost incurred for the cluster and its dependencies without having to run reports to determine each tenant\u2019s spend. With Soft multi-tenancy, you can use Kubernetes features like Node Selectors and Node Affinity to instruct Kubernetes Scheduler to run a tenant\u2019s workload on dedicated node groups. You can tag the EC2 instances in a node group with an identifier (like product name or team name) and use tags to distribute costs. A downside of the above two approach is that you may end up with unused capacity and may not fully utilize the cost savings that come when you run a densely packed cluster. You still need ways to allocate cost of shared resources like Elastic Load Balancing, network transfer charges.</p> <p>The most efficient way to track costs in multi-tenant Kubernetes clusters is to distribute incurred costs based on the amount of resources consumed by workloads. This pattern allows you to maximize the utilization of your EC2 instances because different workloads can share nodes, which allows you to increase the pod-density on your nodes. However, calculating costs by workload or namespaces is a challenging task. Understanding the cost-responsibility of a workload requires aggregating all the resources consumed or reserved during a time-frame, and evaluating the charges based on the cost of the resource and the duration of the usage. This is the exact challenge that Kubecost is dedicated to tackling.</p> <p>Tip</p> <p>Take a look at our One Observability Workshop to get a hands-on experience on Kubecost.</p>"},{"location":"guides/cost/kubecost/#recommendations","title":"Recommendations","text":""},{"location":"guides/cost/kubecost/#cost-allocation","title":"Cost Allocation","text":"<p>The Kubecost Cost Allocation dashboard allows you to quickly see allocated spend and optimization opportunity across all native Kubernetes concepts, e.g. namespace, k8s label, and service. It also allows for allocating cost to organizational concepts like team, product/project, department, or environment. You can modify Date range, filters to derive insights about specific workload and save the report. To optimize the Kubernetes cost, you should be paying attention to the efficiency and cluster idle costs.</p> <p></p>"},{"location":"guides/cost/kubecost/#efficiency","title":"Efficiency","text":"<p>Pod resource efficiency is defined as the resource utilization versus the resource request over a given time window. It is cost-weighted and can be expressed as follows: <pre><code>(((CPU Usage / CPU Requested) * CPU Cost) + ((RAM Usage / RAM Requested) * RAM Cost)) / (RAM Cost + CPU Cost)\n</code></pre> where CPU Usage = rate(container_cpu_usage_seconds_total) over the time window RAM Usage = avg(container_memory_working_set_bytes) over the time window</p> <p>As explicit RAM, CPU or GPU prices are not provided by AWS, the Kubecost model falls back to the ratio of base CPU, GPU and RAM price inputs supplied. The default values for these parameters are based on the marginal resource rates of the cloud provider, but they can be customized within Kubecost. These base resource (RAM/CPU/GPU) prices are normalized to ensure the sum of each component is equal to the total price of the node provisioned, based on billing rates from your provider</p> <p>It is the responsibility of each service team to move towards maximum efficiency and fine tune the workloads to achieve the goal.</p>"},{"location":"guides/cost/kubecost/#idle-cost","title":"Idle Cost","text":"<p>Cluster idle cost is defined as the difference between the cost of allocated resources and the cost of the hardware they run on. Allocation is defined as the max of usage and requests. It can also be expressed as follows: <pre><code>idle_cost = sum(node_cost) - (cpu_allocation_cost + ram_allocation_cost + gpu_allocation_cost)\n</code></pre> where allocation = max(request, usage)</p> <p>So, idle costs can also be thought of as the cost of the space that the Kubernetes scheduler could schedule pods, without disrupting any existing workloads, but it is not currently. It can be distributed to the workloads or cluster or by nodes depending on how you want to configure.</p>"},{"location":"guides/cost/kubecost/#network-cost","title":"Network Cost","text":"<p>Kubecost uses best-effort to allocate network transfer costs to the workloads generating those costs. The accurate way of determining the network cost is by using the combination of  AWS Cloud Integration and Network costs daemonset. </p> <p>You would want to take into account your efficiency score and Idle cost to fine tune the workloads to ensure you utilize the cluster to its complete potential. This takes us to the next topic namely Cluster right-sizing.</p>"},{"location":"guides/cost/kubecost/#right-sizing-workloads","title":"Right-Sizing Workloads","text":"<p>Kubecost provides right-sizing recommendations for your workloads based on Kubernetes-native metrics. The savings panel in the kubecost UI is a great place to start.</p> <p></p> <p></p> <p>Kubecost can give you recommendations on:</p> <ul> <li>Right sizing container request by taking a look at both over-provisioned and under-provisioned container request</li> <li>Adjust the number and size of the cluster nodes to stop over-spending on unused capacity</li> <li>Scale down, delete / resize pods that don\u2019t send or receive meaningful rate of traffic</li> <li>Identifying workloads ready for spot nodes</li> <li>Identifying volumes that are unused by any pods</li> </ul> <p>Kubecost also has a pre-release feature that can automatically implement its recommendations for container resource requests if you have the Cluster Controller component enabled. Using automatic request right-sizing allows you to instantly optimize resource allocation across your entire cluster, without testing excessive YAML or complicated kubectl commands. You can easily eliminate resource over-allocation in your cluster, which paves the way for vast savings via cluster right-sizing and other optimizations.</p>"},{"location":"guides/cost/kubecost/#integrating-kubecost-with-amazon-managed-service-for-prometheus","title":"Integrating Kubecost with Amazon Managed Service for Prometheus","text":"<p>Kubecost leverages the open-source Prometheus project as a time series database and post-processes the data in Prometheus to perform cost allocation calculations. Depending on the cluster size and scale of the workload, it could be overwhelming for a Prometheus server to scrape and store the metrics. In such case, you can use the Amazon Managed Service for Prometheus, a managed Prometheus-compatible monitoring service to store the metrics reliably and enable you to easily monitor Kubernetes cost at scale.</p> <p>You must setup IAM roles for Kubecost service accounts. Using the OIDC provider for the cluster, you grant IAM permissions to your cluster\u2019s service accounts. You must grant appropriate permissions to the kubecost-cost-analyzer and kubecost-prometheus-server service accounts. These will be used to send and retrieve metrics from the workspace. Run the following commands on the command line:</p> <p><pre><code>eksctl create iamserviceaccount \\ \n--name kubecost-cost-analyzer \\ \n--namespace kubecost \\ \n--cluster &lt;CLUSTER_NAME&gt; \\\n--region &lt;REGION&gt; \\ \n--attach-policy-arn arn:aws:iam::aws:policy/AmazonPrometheusQueryAccess \\ \n--attach-policy-arn arn:aws:iam::aws:policy/AmazonPrometheusRemoteWriteAccess \\ \n--override-existing-serviceaccounts \\ \n--approve \n\neksctl create iamserviceaccount \\ \n--name kubecost-prometheus-server \\ \n--namespace kubecost \\ \n--cluster &lt;CLUSTER_NAME&gt; --region &lt;REGION&gt; \\ \n--attach-policy-arn arn:aws:iam::aws:policy/AmazonPrometheusQueryAccess \\ \n--attach-policy-arn arn:aws:iam::aws:policy/AmazonPrometheusRemoteWriteAccess \\ \n--override-existing-serviceaccounts \\ \n--approve\n</code></pre> <code>CLUSTER_NAME</code> is the name of the Amazon EKS cluster where you want to install Kubecost and  is the region of the Amazon EKS cluster. <p>Once complete, you will have to upgrade the Kubecost helm chart as below : <pre><code>helm upgrade -i kubecost \\\noci://public.ecr.aws/kubecost/cost-analyzer --version &lt;$VERSION&gt; \\\n--namespace kubecost --create-namespace \\\n-f https://tinyurl.com/kubecost-amazon-eks \\\n-f https://tinyurl.com/kubecost-amp \\\n--set global.amp.prometheusServerEndpoint=${QUERYURL} \\\n--set global.amp.remoteWriteService=${REMOTEWRITEURL}\n</code></pre></p>"},{"location":"guides/cost/kubecost/#accessing-kubecost-ui","title":"Accessing Kubecost UI","text":"<p>Kubecost provides a web dashboard that you can access either through kubectl port-forward, an ingress, or a load balancer. The enterprise version of Kubecost also supports restricting access to the dashboard using SSO/SAML and providing varying level of access. For example, restricting team\u2019s view to only the products they are responsible for.</p> <p>In AWS environment, consider using the AWS Load Balancer Controller to expose Kubecost and use Amazon Cognito for authentication, authorization, and user management. You can learn more on this How to use Application Load Balancer and Amazon Cognito to authenticate users for your Kubernetes web apps</p>"},{"location":"guides/cost/kubecost/#multi-cluster-view","title":"Multi-cluster view","text":"<p>Your FinOps team would want to review the EKS cluster to share recommendations with business owners. When operating at large scale, it becomes challenging for the teams to log into each cluster to view the recommendations. Multi cluster allows you to have  a single-pane-of-glass view into all aggregated cluster costs globally.  There are three options that Kubecost supports for environments with multiple clusters: Kubecost Free, Kubecost Business, and Kubecost enterprise. In the free and business mode, the cloud-billing reconciliation will be performed at each cluster level. In the enterprise mode, the cloud billing reconciliation will be performed in a primary cluster that serves the kubecost UI and uses the shared bucket where the metrics are stored. It is important to note that metrics retention is unlimited only when you use enterprise mode.</p>"},{"location":"guides/cost/kubecost/#references","title":"References","text":"<ul> <li>Hands-On Kubecost experience on One Observability Workshop</li> <li>Blog - Integrating Kubecost with Amazon Managed Service for Prometheus</li> </ul>"},{"location":"guides/cost/cost-visualization/AmazonManagedServiceforPrometheus/","title":"Real-time cost monitoring","text":"<p>Amazon Managed Service for Prometheus is a server-less, Prometheus-compatible monitoring service for container metrics that makes it easier to securely monitor container environments at scale. Amazon Managed Service for Prometheus pricing model is based on Metric samples ingested, Query samples processed, and Metrics stored. You can find the latest pricing details here. </p> <p>As a managed service, Amazon Managed Service for Prometheus automatically scales the ingestion, storage, and querying of operational metrics as workloads scale up and down. Some of our customers asked us guidance on how to track <code>metric samples ingestion rate</code> and it's cost real-time. Let's explore how you can achieve that.</p>"},{"location":"guides/cost/cost-visualization/AmazonManagedServiceforPrometheus/#solution","title":"Solution","text":"<p>Amazon Managed Service for Prometheus vends usage metrics to Amazon CloudWatch. These metrics can be used to help you gain better visibility into your Amazon Managed Service for Prometheus workspace. The vended metrics can be found in the <code>AWS/Usage</code> and <code>AWS/Prometheus</code> namespaces in CloudWatch and these metrics are available in CloudWatch for no additional charge. You can always create a CloudWatch dashboard to further explore and visualize these metrics.</p> <p>Today, you will be using Amazon CloudWatch as a data-source for Amazon Managed Grafana and build dashboards in Grafana to visualize those metrics. The architecture diagram illustrates the following.  </p> <ul> <li> <p>Amazon Managed Service for Prometheus publishing vended metrics to Amazon CloudWatch  </p> </li> <li> <p>Amazon CloudWatch as a data-source for Amazon Managed Grafana  </p> </li> <li> <p>Users accessing the dashboards created in Amazon Managed Grafana</p> </li> </ul> <p></p>"},{"location":"guides/cost/cost-visualization/AmazonManagedServiceforPrometheus/#amazon-managed-grafana-dashboards","title":"Amazon Managed Grafana Dashboards","text":"<p>The dashboard created in Amazon Managed Grafana will enable you to visualize;  </p> <ol> <li> <p>Prometheus Ingestion Rate per workspace </p> </li> <li> <p>Prometheus Ingestion Rate and Real-time Cost per workspace    For real-time cost tracking, you will be using a <code>math expression</code> based on the pricing of <code>Metrics Ingested Tier</code> for the <code>First 2 billion samples</code> mentioned in the official AWS pricing document. Math operations take numbers and time series as input and change them to different numbers and time series and refer this document for further customization to fit your business requirements. </p> </li> <li> <p>Prometheus Active Series per workspace </p> </li> </ol> <p>A dashboard in Grafana is represented by a JSON object, which stores metadata of its dashboard. Dashboard metadata includes dashboard properties, metadata from panels, template variables, panel queries, etc.  </p> <p>You can access the JSON template of the above dashboard here. <p>With the preceding dashboard, you can now identify ingestion rate per workspace and monitor real-time cost per workspace based on the metrics ingestion rate for Amazon Managed Service for Prometheus. You can use other Grafana dashboard panels to build visuals to suit your requirements.</p>"},{"location":"guides/cost/cost-visualization/amazon-cloudwatch/","title":"Amazon CloudWatch","text":"<p>Amazon CloudWatch cost and usage visuals will allow you to gain insights into cost of individual AWS Accounts, AWS Regions, and all CloudWatch operations like GetMetricData, PutLogEvents, GetMetricStream, ListMetrics, MetricStorage, HourlyStorageMetering, and ListMetrics to name a few!  </p> <p>To visualize and analyze the CloudWatch cost and usage data, you need to create a custom Athena view. An Amazon Athena view is a logical table and it creates a subset of columns from the original CUR table to simplify the querying of data.</p> <ol> <li> <p>Before proceeding, make sure that you\u2019ve created the CUR (step #1) and deployed the AWS Conformation Template (step #2) mentioned in the Implementation overview.</p> </li> <li> <p>Now, Create a new Amazon Athena view by using the following query. This query fetches cost and usage of Amazon CloudWatch across all the AWS Accounts in your Organization.</p> <pre><code>CREATE OR REPLACE VIEW \"cloudwatch_cost\" AS \nSELECT\nline_item_usage_type\n, line_item_resource_id\n, line_item_operation\n, line_item_usage_account_id\n, month\n, year\n, \"sum\"(line_item_usage_amount) \"Usage\"\n, \"sum\"(line_item_unblended_cost) cost\nFROM\ndatabase.tablename #replace database.tablename with your database and table name\nWHERE (\"line_item_product_code\" = 'AmazonCloudWatch')\nGROUP BY 1, 2, 3, 4, 5, 6\n</code></pre> </li> </ol>"},{"location":"guides/cost/cost-visualization/amazon-cloudwatch/#create-amazon-quicksight-dashboard","title":"Create Amazon QuickSight dashboard","text":"<p>Now, let\u2019s create a QuickSight dashboard to visualize the cost and usage of Amazon CloudWatch.  </p> <ol> <li>On AWS Management Console, navigate to QuickSight service and then select your AWS Region from top right corner. Note that QuickSight Dataset should be in the same AWS Region as that of Amazon Athena table.</li> <li>Make sure that QuickSight can access Amazon S3 and AWS Athena.</li> <li>Create QuickSight Dataset by selecting the data-source as the Amazon Athena view that you created before. Use this procedure to schedule refreshing the Dataset on a daily basis.</li> <li>Create QuickSight Analysis.</li> <li>Create QuickSight Visuals to meet your needs. </li> <li>Format the Visual to meet your needs. </li> <li>Now, you can publish your dashboard from the Analysis.</li> <li>You can send the dashboard in report format to individuals or groups, either once or on a schedule.</li> </ol> <p>The following QuickSight dashboard shows Amazon CloudWatch cost and usage across all AWS Accounts in your AWS Organizations along with CloudWatch operations like GetMetricData, PutLogEvents, GetMetricStream, ListMetrics, MetricStorage, HourlyStorageMetering, and ListMetrics to name a few.</p> <p> </p> <p>With the preceding dashboard, you can now identify the cost of Amazon CloudWatch in the AWS accounts across your Organization. You can use other QuickSight visual types to build different dashboards to suit your requirements.</p>"},{"location":"guides/cost/cost-visualization/amazon-grafana/","title":"Amazon Managed Grafana","text":"<p>Amazon Managed Grafana cost and usage visuals will allow you to gain insights into cost of individual AWS Accounts, AWS Regions, specific Grafana Workspace instances and licensing cost of Admin, Editor, and Viewer users!  </p> <p>To visualize and analyze the cost and usage data, you need to create a custom Athena view.</p> <ol> <li> <p>Before proceeding, make sure that you\u2019ve created the CUR (step #1) and deployed the AWS Conformation Template (step #2) mentioned in the Implementation overview.</p> </li> <li> <p>Now, Create a new Amazon Athena view by using the following query. This query fetches cost and usage of Amazon Managed Grafana across all the AWS Accounts in your Organization.</p> <pre><code>CREATE OR REPLACE VIEW \"grafana_cost\" AS \nSELECT\nline_item_usage_type\n, line_item_resource_id\n, line_item_operation\n, line_item_usage_account_id\n, month\n, year\n, \"sum\"(line_item_usage_amount) \"Usage\"\n, \"sum\"(line_item_unblended_cost) cost\nFROM\ndatabase.tablename #replace database.tablename with your database and table name\nWHERE (\"line_item_product_code\" = 'AmazonGrafana')\nGROUP BY 1, 2, 3, 4, 5, 6\n</code></pre> </li> </ol> <p>Using Athena as a data source, you can build dashboards in either Amazon Managed Grafana or Amazon QuickSight to suit your business requirements. As well, you could directly run SQL queries against the Athena view that you created.</p>"},{"location":"guides/cost/cost-visualization/amazon-prometheus/","title":"Amazon Managed Service for Prometheus","text":"<p>Amazon Managed Service for Prometheus cost and usage visuals will allow you to gain insights into cost of individual AWS Accounts, AWS Regions, specific Prometheus Workspace instances along with Operations like RemoteWrite, Query, and HourlyStorageMetering!  </p> <p>To visualize and analyze the cost and usage data, you need to create a custom Athena view.</p> <ol> <li> <p>Before proceeding, make sure that you\u2019ve created the CUR (step #1) and deployed the AWS Conformation Template (step #2) mentioned in the Implementation overview.</p> </li> <li> <p>Now, Create a new Amazon Athena view by using the following query. This query fetches cost and usage of Amazon Managed Service for Prometheus across all the AWS Accounts in your Organization.</p> <pre><code>CREATE OR REPLACE VIEW \"prometheus_cost\" AS \nSELECT\nline_item_usage_type\n, line_item_resource_id\n, line_item_operation\n, line_item_usage_account_id\n, month\n, year\n, \"sum\"(line_item_usage_amount) \"Usage\"\n, \"sum\"(line_item_unblended_cost) cost\nFROM\ndatabase.tablename #replace database.tablename with your database and table name\nWHERE (\"line_item_product_code\" = 'AmazonPrometheus')\nGROUP BY 1, 2, 3, 4, 5, 6\n</code></pre> </li> </ol>"},{"location":"guides/cost/cost-visualization/amazon-prometheus/#create-amazon-managed-grafana-dashboard","title":"Create Amazon Managed Grafana dashboard","text":"<p>With Amazon Managed Grafana, you can add Athena as a data source by using the AWS data source configuration option in the Grafana workspace console. This feature simplifies adding Athena as a data source by discovering your existing Athena accounts and manages the configuration of the authentication credentials that are required to access Athena. For prerequisites associated with using the Athena data source, see Prerequisites.</p> <p>The following Grafana dashboard shows Amazon Managed Service for Prometheus cost and usage across all AWS Accounts in your AWS Organizations along with cost of individual Prometheus Workspace instances and the Operations like RemoteWrite, Query, and HourlyStorageMetering! </p> <p></p> <p>A dashboard in Grafana is represented by a JSON object, which stores metadata of its dashboard. Dashboard metadata includes dashboard properties, metadata from panels, template variables, panel queries, etc. Access the JSON template of the above dashboard here.</p> <p>With the preceding dashboard, you can now identify the cost and usage of Amazon Managed Service for Prometheus in the AWS accounts across your Organization. You can use other Grafana dashboard panels to build visuals to suit your requirements.</p>"},{"location":"guides/cost/cost-visualization/aws-xray/","title":"AWS X-Ray","text":"<p>AWS X-Ray cost and usage visuals will allow you to gain insights into cost of individual AWS Accounts, AWS Regions, and TracesStored!  </p> <p>To visualize and analyze the cost and usage data, you need to create a custom Athena view.</p> <ol> <li> <p>Before proceeding, make sure that you\u2019ve created the CUR (step #1) and deployed the AWS Conformation Template (step #2) mentioned in the Implementation overview.</p> </li> <li> <p>Now, Create a new Amazon Athena view by using the following query. This query fetches cost and usage of Amazon Managed Grafana across all the AWS Accounts in your Organization.</p> <pre><code>CREATE OR REPLACE VIEW \"xray_cost\" AS \nSELECT\nline_item_usage_type\n, line_item_resource_id\n, line_item_usage_account_id\n, month\n, year\n, \"sum\"(line_item_usage_amount) \"Usage\"\n, \"sum\"(line_item_net_unblended_cost) cost\nFROM\ndatabase.tablename #replace database.tablename with your database and table name \nWHERE (\"line_item_product_code\" = 'AWSXRay')\nGROUP BY 1, 2, 3, 4, 5\n</code></pre> </li> </ol> <p>Using Athena as a data source, you can build dashboards in either Amazon Managed Grafana or Amazon QuickSight to suit your business requirements. As well, you could directly run SQL queries against the Athena view that you created.</p>"},{"location":"guides/cost/cost-visualization/cost/","title":"AWS Observability services and Cost","text":"<p>As you invest in your Observability stack, it\u2019s important that you monitor the cost of your observability products on a regular basis. This allows you to ensure that you are only incurring the costs you need and that you are not overspending on resources you don't need.</p>"},{"location":"guides/cost/cost-visualization/cost/#aws-tools-for-cost-optimization","title":"AWS Tools for Cost Optimization","text":"<p>Most organizations\u2019 core focus lies on scaling their IT infrastructure on cloud, and usually are uncontrolled, unprepared, and unaware of their actual or forthcoming cloud spend. To help you track, report, and analyze costs over time, AWS provides several cost-optimization tools:</p> <p>AWS Cost Explorer \u2013 See patterns in AWS spending over time, project future costs, identify areas that need further inquiry, observe Reserved Instance utilization, observe Reserved Instance coverage, and receive Reserved Instance recommendations.</p> <p>AWS Cost and Usage Report(CUR)\u2013 Granular raw data files detailing your hourly AWS usage across accounts used for Do-It-Yourself (DIY) analysis. The AWS Cost and Usage Report has dynamic columns that populate depending on the services you use.</p>"},{"location":"guides/cost/cost-visualization/cost/#architecture-overview-visualizing-aws-cost-and-usage-report","title":"Architecture overview: Visualizing AWS Cost and Usage Report","text":"<p>You can build AWS cost and usage dashboards in Amazon Managed Grafana or Amazon QuickSight. The following architecture diagram illustrates both the solutions.</p> <p> Architecture diagram</p>"},{"location":"guides/cost/cost-visualization/cost/#cloud-intelligence-dashboards","title":"Cloud Intelligence Dashboards","text":"<p>The Cloud Intelligence Dashboards are a collection of Amazon QuickSight dashboards built on top of AWS Cost and Usage report (CUR). These dashboards work as your own cost management and optimization (FinOps) tool. You get in-depth, granular, and recommendation-driven dashboards that can help you get a detailed view of your AWS usage and costs.</p>"},{"location":"guides/cost/cost-visualization/cost/#implementation","title":"Implementation","text":"<ol> <li> <p>Create a CUR report with Amazon Athena integration enabled. During the initial configuration, it can take up to 24 hours for AWS to start delivering reports to your Amazon S3 bucket. Reports are delivered once a day. To streamline and automate integration of your Cost and Usage Reports with Athena, AWS provides an AWS CloudFormation template with several key resources along with the reports that you set up for Athena integration.</p> </li> <li> <p>Deploy the AWS CloudFormation template. This template includes an AWS Glue crawler, an AWS Glue database, and an AWS Lambda event. At this point, CUR data is made available through tables in Amazon Athena for you to query. </p> <ul> <li>Run Amazon Athena queries directly on your CUR data. To run Athena queries on your data, first use the Athena console to check whether AWS is refreshing your data and then run your query on the Athena console.</li> </ul> </li> <li> <p>Deploy Cloud Intelligence dashboards.</p> <ul> <li>For manual deployment, refer the AWS Well-Architected Cost Optimization lab. </li> <li>For automated deployment, refer the GitHub repo.</li> </ul> </li> </ol> <p>Cloud Intelligence dashboards are great for Finance teams, Executives, and IT managers. However, one common question that we get from customers is how to gain insights into organizational wide cost of individual AWS Observability products like Amazon CloudWatch, AWS X-Ray, Amazon Managed Service for Prometheus, and Amazon Managed Grafana.  </p> <p>In the next section, you will dive-deep into cost and usage of each of those products. Companies of any size can adopt this proactive approach to cloud cost optimization strategy and improve business efficiency through cloud cost analytics and data-driven decisions, without any performance impact or operational overhead.</p>"},{"location":"guides/cost/cost-visualization/reducing-cw-cost/","title":"Reducing CloudWatch cost","text":""},{"location":"guides/cost/cost-visualization/reducing-cw-cost/#getmetricdata","title":"GetMetricData","text":"<p>Typically <code>GetMetricData</code> is caused by calls from 3rd party Observability tools and/or cloud financial tools using the CloudWatch Metrics in their platform. </p> <ul> <li>Consider reducing the frequency with which the 3rd party tool is making requests. For example, reducing frequency from 1 min to 5 mins should result in a 1/5 (20%) of the cost.</li> <li>To identify the trend, consider turning off any data collection from 3rd party tools for a short while.</li> </ul>"},{"location":"guides/cost/cost-visualization/reducing-cw-cost/#cloudwatch-logs","title":"CloudWatch Logs","text":"<ul> <li>Find the top contributors using this knowledge center document.</li> <li>Reduce the logging level of top contributors unless deemed necessary.</li> <li>Find out if you are using 3rd party tooling for logging in addition to Cloud Watch.</li> <li>VPC Flow Log costs can add up quick if you have enabled it on every VPC and has a lot of traffic. If you still need it, consider delivering it to Amazon S3.</li> <li>See if logging is necessary on all AWS Lambda functions. If it\u2019s not, deny \u201clogs:PutLogEvents\u201d permissions in the Lambda role.</li> <li>CloudTrail logs are often a top contributor. Sending them to Amazon S3 and using Amazon Athena to query and Amazon EventBridge for alarms/notifications is cheaper.</li> </ul> <p>Refer this knowledge center article for further details.</p>"},{"location":"guides/databases/rds-and-aurora/","title":"Monitor Amazon RDS and Aurora databases","text":"<p>Monitoring is a critical part of maintaining the reliability, availability, and performance of Amazon RDS and Aurora database clusters. AWS provides several tools for monitoring health of your Amazon RDS and Aurora databases resources, detect issues before they become critical and optimize performance for consistent user experience.  This guide provides the observability best practices to ensure your databases are running smoothly. </p>"},{"location":"guides/databases/rds-and-aurora/#performance-guidelines","title":"Performance guidelines","text":"<p>As a best practice, you want to start with establishing a baseline performance for your workloads. When you set up a DB instance and run it with a typical workload, capture the average, maximum, and minimum values of all performance metrics. Do so at a number of different intervals (for example, one hour, 24 hours, one week, two weeks). This can give you an idea of what is normal. It helps to get comparisons for both peak and off-peak hours of operation. You can then use this information to identify when performance is dropping below standard levels.</p>"},{"location":"guides/databases/rds-and-aurora/#monitoring-options","title":"Monitoring Options","text":""},{"location":"guides/databases/rds-and-aurora/#amazon-cloudwatch-metrics","title":"Amazon CloudWatch metrics","text":"<p>Amazon CloudWatch is a critical tool for monitoring and managing your RDS and Aurora databases. It provides valuable insights into database performance and helps you identify and resolve issues quickly. Both Amazon RDS and Aurora database sends metrics to CloudWatch for each active database instance at 1 minute granularity. Monitoring is enabled by default and metrics are available for 15 days. RDS and Aurora publish instance-level metrics metrics to Amazon CloudWatch in the AWS/RDS namespace.</p> <p>Using CloudWatch Metrics, you can identify trends or patterns in your database performance, and use this information to optimize your configurations and improve your application's performance. Here are key metricsto monitor :</p> <ul> <li>CPU Utilization - Percentage of computer processing capacity used.</li> <li>DB Connections - The number of client sessions that are connected to the DB instance. Consider constraining database connections if you see high numbers of user connections in conjunction with decreases in instance performance and response time. The best number of user connections for your DB instance will vary based on your instance class and the complexity of the operations being performed. To determine the number of database connections, associate your DB instance with a parameter group.</li> <li>Freeable Memory - How much RAM is available on the DB instance, in megabytes. The red line in the Monitoring tab metrics is marked at 75% for CPU, Memory and Storage Metrics. If instance memory consumption frequently crosses that line, then this indicates that you should check your workload or upgrade your instance.</li> <li>Network throughput - The rate of network traffic to and from the DB instance in bytes per second.</li> <li>Read/Write Latency - The average time for a read or write operation in milliseconds.</li> <li>Read/Write IOPS - The average number of disk read or write operations per second.</li> <li>Free Storage Space - How much disk space is not currently being used by the DB instance, in megabytes. Investigate disk space consumption if space used is consistently at or above 85 percent of the total disk space. See if it is possible to delete data from the instance or archive data to a different system to free up space.</li> </ul> <p></p> <p>For troubleshooting performance related issues, first step is to tune the most used and expensive queries. Tune them to see if doing so lowers the pressure on system resources. For more information, see Tuning queries.</p> <p>If your queries are tuned and the issue still persists, consider upgrading your database instance classes. You can upgrade it to an instance with more resources (CPU, RAM, disk space, network bandwidth, I/O capacity).</p> <p>Then, you can set up alarms to alert when these metrics reach critical thresholds, and take action to resolve any issues as quickly as possible. </p> <p>For more information on CloudWatch metrics, refer Amazon CloudWatch metrics for Amazon RDS and Viewing DB instance metrics in the CloudWatch console and AWS CLI.</p>"},{"location":"guides/databases/rds-and-aurora/#cloudwatch-logs-insights","title":"CloudWatch Logs Insights","text":"<p>CloudWatch Logs Insights enables you to interactively search and analyze your log data in Amazon CloudWatch Logs. You can perform queries to help you more efficiently and effectively respond to operational issues. If an issue occurs, you can use CloudWatch Logs Insights to identify potential causes and validate deployed fixes.</p> <p>To publish logs from RDS or Aurora database cluster to CloudWatch, see Publish logs for Amazon RDS or Aurora for MySQL instances to CloudWatch</p> <p>For more information on monitoring RDS or Aurora logs with CloudWatch, see Monitoring Amazon RDS log file.</p>"},{"location":"guides/databases/rds-and-aurora/#cloudwatch-alarms","title":"CloudWatch Alarms","text":"<p>To identify when performance is degraded for your database clusters, you should monitor and alert on key performance metrics on a regular basis. Using Amazon CloudWatch alarms, you can watch a single metric over a time period that you specify. If the metric exceeds a given threshold, a notification is sent to an Amazon SNS topic or AWS Auto Scaling policy. CloudWatch alarms do not invoke actions simply because they are in a particular state. Rather the state must have changed and been maintained for a specified number of periods. Alarms invoke actions only when alarm change state occurs. Being in alarm state is not enough.</p> <p>To set a CloudWatch alarm -</p> <ul> <li>Navigate to AWS Management Console and open the Amazon RDS console at https://console.aws.amazon.com/rds/.</li> <li>In the navigation pane, choose Databases, and then choose a DB instance.</li> <li>Choose Logs &amp; events.</li> </ul> <p>In the CloudWatch alarms section, choose Create alarm.</p> <p></p> <ul> <li>For Send notifications, choose Yes, and for Send notifications to, choose New email or SMS topic.</li> <li>For Topic name, enter a name for the notification, and for With these recipients, enter a comma-separated list of email addresses and phone numbers.</li> <li>For Metric, choose the alarm statistic and metric to set.</li> <li>For Threshold, specify whether the metric must be greater than, less than, or equal to the threshold, and specify the threshold value.</li> <li>For Evaluation period, choose the evaluation period for the alarm. For consecutive period(s) of, choose the period during which the threshold must have been reached in order to trigger the alarm.</li> <li>For Name of alarm, enter a name for the alarm.</li> <li>Choose Create Alarm.</li> </ul> <p>The alarm appears in the CloudWatch alarms section.</p> <p>Take a look at this example to create an Amazon CloudWatch alarm for Multi-AZ DB cluster replica lag. </p>"},{"location":"guides/databases/rds-and-aurora/#database-audit-logs","title":"Database Audit Logs","text":"<p>Database Audit Logs provide a detailed record of all actions taken on your RDS and Aurora databases, enabling you to monitor for unauthorized access, data changes, and other potentially harmful activities. Here are some best practices for using Database Audit Logs:</p> <ul> <li>Enable Database Audit Logs for all of your RDS and Aurora instances, and configure them to capture all relevant data.</li> <li>Use a centralized log management solution, such as Amazon CloudWatch Logs or Amazon Kinesis Data Streams, to collect and analyze your Database Audit Logs.</li> <li>Monitor your Database Audit Logs regularly for suspicious activity, and take action to investigate and resolve any issues as quickly</li> </ul> <p>For more information on how to configure database audit logs, see Configuring an Audit Log to Capture database activities for Amazon RDS and Aurora.</p>"},{"location":"guides/databases/rds-and-aurora/#database-slow-query-and-error-logs","title":"Database Slow Query and Error Logs","text":"<p>Slow query logs help you find slow-performing queries in the database so you can investigate the reasons behind the slowness and tune the queries if needed. Error logs help you to find the query errors, which further helps you find the changes in the application due to those errors. </p> <p>You can monitor the slow query log and error log by creating a CloudWatch dashboard using Amazon CloudWatch Logs Insights (which enables you to interactively search and analyze your log data in Amazon CloudWatch Logs). </p> <p>To activate and monitor the error log, the slow query log, and the general log for an Amazon RDS, see Manage slow query logs and general logs for RDS MySQL. To activate slow query log for Aurora PostgreSQL, see Enable slow query logs for PostgreSQL.</p>"},{"location":"guides/databases/rds-and-aurora/#performance-insights-and-operating-system-metrics","title":"Performance Insights and operating-system metrics","text":""},{"location":"guides/databases/rds-and-aurora/#enhanced-monitoring","title":"Enhanced Monitoring","text":"<p>Enhanced Monitoring enables you to get fine-grain metrics in real time for the operating system (OS) that your DB instance runs on. </p> <p>RDS delivers the metrics from Enhanced Monitoring into your Amazon CloudWatch Logs account. By default, these metrics are stored for 30 days and stored in RDSOSMetrics Log group in Amazon CloudWatch. You have the option to choose a granularity between 1s to 60s. You can create custom metrics filters in CloudWatch from CloudWatch Logs and display the graphs on the CloudWatch dashboard.</p> <p></p> <p>Enhanced monitoring also include the OS level process list. Currently, Enhanced Monitoring is available for the following database engines:</p> <ul> <li>MariaDB</li> <li>Microsoft SQL Server</li> <li>MySQL</li> <li>Oracle</li> <li>PostgreSQL</li> </ul> <p>Different between CloudWatch and Enhanced Monitoring CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance. In contrast, Enhanced Monitoring gathers its metrics from an agent on the DB instance. A hypervisor creates and runs virtual machines (VMs). Using a hypervisor, an instance can support multiple guest VMs by virtually sharing memory and CPU. You might find differences between the CloudWatch and Enhanced Monitoring measurements, because the hypervisor layer performs a small amount of work. The differences can be greater if your DB instances use smaller instance classes. In this scenario, more virtual machines (VMs) are probably managed by the hypervisor layer on a single physical instance.</p> <p>To learn about all the metrics available with Enhanced Monitoring, please refer OS metrics in Enhanced Monitoring</p> <p></p>"},{"location":"guides/databases/rds-and-aurora/#performance-insights","title":"Performance Insights","text":"<p>Amazon RDS Performance Insights is a database performance tuning and monitoring feature that helps you quickly assess the load on your database, and determine when and where to take action. With the Performance Insights dashboard, you can visualize the database load on your db cluster and filter the load by waits, SQL statements, hosts, or users. It allows you to pin point on the root cause rather than chasing symptoms. Performance Insights uses lightweight data collection methods that do not impact the performance of your applications and makes it easy to see which SQL statements are causing the load and why.</p> <p>Performance Insights provides seven days of free performance history retention and you can extend that up to 2 years with a fees. You can enable Performance Insights from RDS management console or AWS CLI. Performance Insights also exposes a publicly available API to enable customers and third parties to integrate Performance Insights with their own custom tooling.</p> <p>Note</p> <p>Currently, RDS Performance Insights is available only for Aurora (PostgreSQL- and MySQL-compatible editions), Amazon RDS for PostgreSQL, MySQL, MariaDB, SQL Server and Oracle.</p> <p>DBLoad is the key metric which represents the average number of database active sessions. In Performance Insights, this data is queried as db.load.avg metric.</p> <p></p> <p>For more information on using Performance Insights with Aurora, refer: Monitoring DB load with Performance Insights on Amazon Aurora. </p>"},{"location":"guides/databases/rds-and-aurora/#open-source-observability-tools","title":"Open-source Observability Tools","text":""},{"location":"guides/databases/rds-and-aurora/#amazon-managed-grafana","title":"Amazon Managed Grafana","text":"<p>Amazon Managed Grafana is a fully managed service that makes it easy to visualize and analyze data from RDS and Aurora databases. </p> <p>The AWS/RDS namespace in Amazon CloudWatch includes the key metrics that apply to database entities running on Amazon RDS and Amazon Aurora.To visualize and track the health and potential performance issues of our RDS/Aurora databases in Amazon Managed Grafana, we can leverage CloudWatch data source. </p> <p></p> <p>As of now, only basic Performance Insights metrics are available in CloudWatch which is not sufficient to analyze database performance and identify bottlenecks in your database. To visualize RDS Performance Insight metrics in Amazon Managed Grafana and have a single pane of glass visibility, customers can use a custom lambda function to collect all the RDS Performance insights metrics and publish them in a custom CloudWatch metrics namespace. Once you have these metrics available in Amazon CloudWatch, you can visualize them in Amazon Managed Grafana.</p> <p>To deploy the custom lambda function to gather RDS Performance Insights metrics, clone the following GitHub repository and run the install.sh script.</p> <pre><code>$ git clone https://github.com/aws-observability/observability-best-practices.git\n$ cd sandbox/monitor-aurora-with-grafana\n\n$ chmod +x install.sh\n$ ./install.sh\n</code></pre> <p>Above script uses AWS CloudFormation to deploy a custom lambda function and an IAM role. Lambda function auto triggers every 10 mins to invoke RDS Performance Insights API and publish custom metrics to /AuroraMonitoringGrafana/PerformanceInsights custom namespace in Amazon CloudWatch.</p> <p></p> <p>For detailed step-by-step information on custom lambda function deployment and grafana dashboards, refer Performance Insights in Amazon Managed Grafana.</p> <p>By quickly identifying unintended changes in your database and notifying using alerts, you can take actions to minimize disruptions. Amazon Managed Grafana supports multiple notification channels such as SNS, Slack, PagerDuty etc. to which you can send alerts notifications. Grafana Alerting will show you more information on how to set up alerts in Amazon Managed Grafana.</p>"},{"location":"guides/databases/rds-and-aurora/#aiops-machine-learning-based-performance-bottlenecks-detection","title":"AIOps - Machine learning based performance bottlenecks detection","text":""},{"location":"guides/databases/rds-and-aurora/#amazon-devops-guru-for-rds","title":"Amazon DevOps Guru for RDS","text":"<p>With Amazon DevOps Guru for RDS, you can monitor your databases for performance bottlenecks and operational issues. It uses Performance Insights metrics, analyzes them using Machine Learning (ML) to provide database-specific analyses of performance issues, and recommends corrective actions. DevOps Guru for RDS can identify and analyze various performance-related database issues, such as over-utilization of host resources, database bottlenecks, or misbehavior of SQL queries, among others. When an issue or anomalous behavior is detected, DevOps Guru for RDS displays the finding on the DevOps Guru console and sends notifications using Amazon EventBridge or Amazon Simple Notification Service (SNS), allowing DevOps or SRE teams to take real-time action on performance and operational issues before they become customer-impacting outages.</p> <p>DevOps Guru for RDS establishes a baseline for the database metrics. Baselining involves analyzing the database performance metrics over a period of time to establish a normal behavior. Amazon DevOps Guru for RDS then uses ML to detect anomalies against the established baseline. If your workload pattern changes, then DevOps Guru for RDS establishes a new baseline that it uses to detect anomalies against the new normal. </p> <p>Note</p> <p>For new database instances, Amazon DevOps Guru for RDS takes up to 2 days to establish an initial baseline, because it requires an analysis of the database usage patterns and establishing what is considered a normal behavior.</p> <p></p> <p></p> <p>For more information on how to get started, please visit Amazon DevOps Guru for RDS to Detect, Diagnose, and Resolve Amazon Aurora-Related Issues using ML</p>"},{"location":"guides/databases/rds-and-aurora/#auditing-and-governance","title":"Auditing and Governance","text":""},{"location":"guides/databases/rds-and-aurora/#aws-cloudtrail-logs","title":"AWS CloudTrail Logs","text":"<p>AWS CloudTrail provides a record of actions taken by a user, role, or an AWS service in RDS. CloudTrail captures all API calls for RDS as events, including calls from the console and from code calls to RDS API operations. Using the information collected by CloudTrail, you can determine the request that was made to RDS, the IP address from which the request was made, who made the request, when it was made, and additional details. For more information, see Monitoring Amazon RDS API calls in AWS CloudTrail.</p> <p>For more information, refer Monitoring Amazon RDS API calls in AWS CloudTrail.</p>"},{"location":"guides/databases/rds-and-aurora/#references-for-more-information","title":"References for more information","text":"<p>Blog - Monitor RDS and Aurora databases with Amazon Managed Grafana</p> <p>Video - Monitor RDS and Aurora databases with Amazon Managed Grafana</p> <p>Blog - Monitor RDS and Aurora databases with Amazon CloudWatch</p> <p>Blog - Build proactive database monitoring for Amazon RDS with Amazon CloudWatch Logs, AWS Lambda, and Amazon SNS</p> <p>Official Doc - Amazon Aurora Monitoring Guide</p> <p>Hands-on Workshop - Observe and Identify SQL Performance Issues in Amazon Aurora</p>"},{"location":"guides/ec2/ec2-monitoring/","title":"Amazon EC2 Monitoring and Observability","text":""},{"location":"guides/ec2/ec2-monitoring/#introduction","title":"Introduction","text":"<p>Continuous Monitoring &amp; Observability increases agility, improves customer experience and reduces risk of the cloud environment. According to Wikipedia, Observability is a measure of how well internal states of a system can be inferred from the knowledge of its external outputs. The term observability itself originates from the field of control theory, where it basically means that you can infer the internal state of the components in a system by learning about the external signals/outputs it is producing.</p> <p>The difference between Monitoring and Observability is that Monitoring tells you whether a system is working or not, while Observability tells you why the system isn\u2019t working. Monitoring is usually a reactive measure whereas the goal of Observability is to be able to improve your Key Performance Indicators in a proactive manner. A system cannot be controlled or optimized unless it is observed. Instrumenting workloads through collection of metrics, logs, or traces and gaining meaningful insights &amp; detailed context using the right monitoring and observability tools help customers control and optimize the environment.</p> <p></p> <p>AWS enables customers to transform from monitoring to observability so that they can have full end-to-end service visibility. In this article we focus on Amazon Elastic Compute Cloud (Amazon EC2) and the best practices for improving the monitoring and observability of the service in AWS Cloud environment through AWS native and open-source tools.</p>"},{"location":"guides/ec2/ec2-monitoring/#amazon-ec2","title":"Amazon EC2","text":"<p>Amazon Elastic Compute Cloud (Amazon EC2) is a highly scalable compute platform in Amazon Web Services (AWS) Cloud. Amazon EC2 eliminates the need for up front hardware investment, so customers can develop and deploy applications faster while paying just for what they use. Some of the key features that EC2 provide are virtual computing environments called Instances, pre-configured templates of Instances called Amazon Machine Images, various configurations of resources like CPU, Memory, Storage and Networking capacity available as Instance Types.</p>"},{"location":"guides/ec2/ec2-monitoring/#monitoring-and-observability-using-aws-native-tools","title":"Monitoring and Observability using AWS Native Tools","text":""},{"location":"guides/ec2/ec2-monitoring/#amazon-cloudwatch","title":"Amazon CloudWatch","text":"<p>Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, and on-premises applications and infrastructure resources. CloudWatch collects monitoring and operational data in the form of logs, metrics, and events. It also provides a unified view of AWS resources, applications, and services that run on AWS and on-premises servers. CloudWatch helps you gain system-wide visibility into resource utilization, application performance, and operational health.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#unified-cloudwatch-agent","title":"Unified CloudWatch Agent","text":"<p>The Unified CloudWatch Agent is an open-source software under the MIT license which supports most  operating systems utilizing x86-64 and ARM64 architectures. The CloudWatch Agent helps collect system-level metrics from Amazon EC2 Instances &amp; on-premise servers in a hybrid environment across operating systems, retrieve custom metrics from applications or services and collect logs from Amazon EC2 instances and on-premises servers.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#installing-cloudwatch-agent-on-amazon-ec2-instances","title":"Installing CloudWatch Agent on Amazon EC2 Instances","text":""},{"location":"guides/ec2/ec2-monitoring/#command-line-install","title":"Command Line Install","text":"<p>The CloudWatch Agent can be installed through the command line. The required package for various architectures and various operating systems are available for download. Create the necessary IAM role which provides permissions for CloudWatch agent to read information from the Amazon EC2 instance and write it to CloudWatch. Once the required IAM role is created, you can install and run the CloudWatch agent on the required Amazon EC2 Instance. </p> <p>References</p> <p>Documentation: Installing the CloudWatch agent using the command line</p> <p>AWS Observability Workshop: Setup and install CloudWatch agent</p>"},{"location":"guides/ec2/ec2-monitoring/#installation-through-aws-systems-manager","title":"Installation through AWS Systems Manager","text":"<p>The CloudWatch Agent can also be installed through AWS Systems Manager. Create the necessary IAM role which provides permissions for CloudWatch agent to read information from the Amazon EC2 instance and write it to CloudWatch &amp; communicate with AWS Systems Manager. Before installing the CloudWatch agent on the EC2 instances, install or update the SSM agent on the required EC2 instances. The CloudWatch agent can be downloaded through the AWS Systems Manager. JSON Configuration file can be created to specify what metrics (including custom metrics), logs are to be collected. Once the required IAM role is created &amp; the configuration file is created, you can install and run the CloudWatch agent on the required Amazon EC2 Instances. </p> <p>References</p> <p>References: Documentation: Installing the CloudWatch agent using AWS Systems Manager</p> <p>AWS Observability Workshop: Install CloudWatch agent using AWS Systems Manager Quick Setup</p> <p>Related Blog Article: Amazon CloudWatch Agent with AWS Systems Manager Integration \u2013 Unified Metrics &amp; Log Collection for Linux &amp; Windows</p> <p>YouTube Video: Collect Metrics and Logs from Amazon EC2 instances with the CloudWatch Agent</p>"},{"location":"guides/ec2/ec2-monitoring/#installing-cloudwatch-agent-on-on-premise-servers-in-hybrid-environment","title":"Installing CloudWatch Agent on on-premise servers in hybrid environment.","text":"<p>In hybrid customer environments, where servers are on-premisis as well as in the cloud. A similar approach of can be taken to accomplish unified observability in Amazon CloudWatch. The CloudWatch agent can be directly downloaded from Amazon S3 or through AWS Systems Manager. Create an IAM User for the on-premise server to send data to Amazon CloudWatch. Install and Start the Agent on the on-premise servers. </p> <p>Reference</p> <p>Documentation: Installing the CloudWatch agent on on-premises servers</p>"},{"location":"guides/ec2/ec2-monitoring/#monitoring-of-amazon-ec2-instances-using-amazon-cloudwatch","title":"Monitoring of Amazon EC2 Instances using Amazon CloudWatch","text":"<p>A key aspect of maintaining the reliability, availability, and performance of your Amazon EC2 Instances and your applications is through continuous monitoring. With CloudWatch Agent installed on the required Amazon EC2 instances, monitoring the health of the instances and their performance is necessary to maintain a stable environment. As a baseline, items like CPU utilization, Network utilization, Disk performance, Disk Reads/Writes, Memory utilization, disk swap utilization, disk space utilization, page file utilization, and log collection of EC2 Instances are recommended.</p>"},{"location":"guides/ec2/ec2-monitoring/#basic-detailed-monitoring","title":"Basic &amp; Detailed Monitoring","text":"<p>Amazon CloudWatch collects and processes raw data from Amazon EC2 into readable near real-time metrics. By default, Amazon EC2 sends metric data to CloudWatch in 5-minute periods as Basic Monitoring for an instance. To send metric data for your instance to CloudWatch in 1-minute periods, detailed monitoring can be enabled on the instance.</p>"},{"location":"guides/ec2/ec2-monitoring/#automated-manual-tools-for-monitoring","title":"Automated &amp; Manual Tools for Monitoring","text":"<p>AWS provides two types of tools, automated and manual that help customers monitor their Amazon EC2 and report back when something is wrong. Some of these tools require a little configuration and a few require manual intervention.  Automated Monitoring tools include AWS System status checks, Instance status checks, Amazon CloudWatch alarms, Amazon EventBridge, Amazon CloudWatch Logs, CloudWatch agent, AWS Management Pack for Microsoft System Center Operations Manager. Manual monitoring tools include Dashboards which we\u2019ll look in detail a separate section below in this article.</p> <p>Reference</p> <p>Documentation: Automated and manual monitoring</p>"},{"location":"guides/ec2/ec2-monitoring/#metrics-from-amazon-ec2-instances-using-cloudwatch-agent","title":"Metrics from Amazon EC2 Instances using CloudWatch Agent","text":"<p>Metrics are the fundamental concept in CloudWatch. A metric represents a time-ordered set of data points that are published to CloudWatch. Think of a metric as a variable to monitor, and the data points as representing the values of that variable over time. For example, the CPU usage of a particular EC2 instance is one metric provided by Amazon EC2.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#default-metrics-using-cloudwatch-agent","title":"Default Metrics using CloudWatch Agent","text":"<p>Amazon CloudWatch collects metrics from Amazon EC2 instance which can be viewed through AWS Management Console, AWS CLI, or an API. The available metrics are data points which are covered for 5 minute interval through Basic Monitoring or at a 1 minute interval for detailed monitoring (if turned on).</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#custom-metrics-using-cloudwatch-agent","title":"Custom Metrics using CloudWatch Agent","text":"<p>Customers can also publish their own custom metrics to CloudWatch using the API or CLI through standard resolution of 1 minute granularity or high resolution granularity down to 1 sec interval. The unified CloudWatch agent supports retrieval of custom metrics through StatsD and collectd. </p> <p>Custom metrics from applications or services can be retrieved using the CloudWatch agent with StatsD protocol. StatsD is a popular open-source solution that can gather metrics from a wide variety of applications. StatsD is especially useful for instrumenting own metrics, which supports both Linux and Windows based servers.</p> <p>Custom metrics from applications or services can also be retrieved using the CloudWatch agent with the collectd protocol, which is a popular open-source solution supported only on Linux Servers with plugins that can gather system statistics for a wide variety of applications. By combining the system metrics that the CloudWatch agent can already collect with the additional metrics from collectd, you can better monitor, analyze, and troubleshoot your systems and applications.</p>"},{"location":"guides/ec2/ec2-monitoring/#additional-custom-metrics-using-cloudwatch-agent","title":"Additional Custom Metrics using CloudWatch Agent","text":"<p>The CloudWatch agent supports collecting custom metrics from your EC2 instances. A few popular examples are:</p> <ul> <li>Network performance metrics for EC2 instances running on Linux that use the Elastic Network Adapter (ENA).</li> <li>Nvidia GPU metrics from Linux servers.</li> <li>Process metrics using procstat plugin from individual processes on Linux &amp; Windows servers.</li> </ul>"},{"location":"guides/ec2/ec2-monitoring/#logs-from-amazon-ec2-instances-using-cloudwatch-agent","title":"Logs from Amazon EC2 Instances using CloudWatch Agent","text":"<p>Amazon CloudWatch Logs helps customers monitor and troubleshoot systems and applications in near real time using existing system, application and custom log files. To collect logs from Amazon EC2 Instances and on-premise servers to CloudWatch, the unified CloudWatch Agent needs to be installed. The latest unified CloudWatch Agent is recommended, since it can collect both logs and advanced metrics. It also supports a variety of operating systems. If the instance uses Instance Metadata Service Version 2 (IMDSv2) then the unified agent is required.</p> <p></p> <p>The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch Logs. Logs can be collected from Windows or Linux Servers and from both Amazon EC2 and on-premise servers. The CloudWatch agent configuration wizard can be used to setup the config JSON file which defines the setup of the CloudWatch agent.</p> <p></p> <p>Reference</p> <p>AWS Observability Workshop: Logs</p>"},{"location":"guides/ec2/ec2-monitoring/#amazon-ec2-instance-events","title":"Amazon EC2 Instance Events","text":"<p>An event indicates a change in your AWS environment. AWS resources and applications can generate events when their state changes. CloudWatch Events provides a near real-time stream of system events that describe changes to your AWS resources and applications. For example, Amazon EC2 generates an event when the state of an EC2 instance changes from pending to running. Customers can also generate custom application-level events and publish them to CloudWatch Events. </p> <p>Customers can monitor the status of Amazon EC2 Instances by viewing status checks and scheduled events. A status check provides the results from automated checks performed by Amazon EC2. These automated checks detect whether specific issues are affecting the instances. The status check information, together with the data provided by Amazon CloudWatch, gives detailed operational visibility into each of the instances.</p>"},{"location":"guides/ec2/ec2-monitoring/#amazon-eventbridge-rule-for-amazon-ec2-instance-events","title":"Amazon EventBridge Rule for Amazon EC2 Instance Events","text":"<p>Amazon CloudWatch Events can use Amazon EventBridge to automate system events to respond automatically for actions such as resource changes or issues. Events from AWS services including Amazon EC2 are delivered to CloudWatch Events in near real time and customers can create EventBridge rules to take appropriate actions when an event matches a rule.  Actions can be, Invoke an AWS Lambda function, Invoke Amazon EC2 Run Command, Relay the event to Amazon Kinesis Data Streams, Activate an AWS Step Functions state machine, Notify an Amazon SNS topic, Notify an Amazon SQS queue, piping to internal or external incident response application or SIEM tool.</p> <p>Reference</p> <p>AWS Observability Workshop: Incident Response - EventBridge Rule</p>"},{"location":"guides/ec2/ec2-monitoring/#amazon-cloudwatch-alarms-for-amazon-ec2-instances","title":"Amazon CloudWatch Alarms for Amazon EC2 Instances","text":"<p>Amazon CloudWatch alarms can watch a metric over a time period you specify, and perform one or more actions based on the value of the metric relative to a given threshold over a number of time periods. An alarm invokes actions only when the alarmchanges state. The action can be a notification sent to an Amazon Simple Notification Service (Amazon SNS) topic or Amazon EC2 Auto Scaling or take other appropriate actions like stop, terminate, reboot, or recover an EC2 instance.</p> <p></p> <p>Once the alarm is triggered an Email notification is sent to an SNS Topic as an action.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#monitoring-for-auto-scaling-instances","title":"Monitoring for Auto Scaling Instances","text":"<p>Amazon EC2 Auto Scaling helps customer ensure that you have the correct number of Amazon EC2 instances are available to handle the load for your application. Amazon EC2 Auto Scaling metrics collect information about Auto Scaling groups and are in the AWS/AutoScaling namespace. Amazon EC2 instance metrics representing CPU and other usage data from Auto Scaling instances are in the AWS/EC2 namespace. </p>"},{"location":"guides/ec2/ec2-monitoring/#dashboarding-in-cloudwatch","title":"Dashboarding in CloudWatch","text":"<p>Getting to know the inventory details of resources in AWS accounts, the resources performance and health checks is important for a stable resource management. Amazon CloudWatch dashboards are customizable home pages in the CloudWatch console that you can be used to monitor your resources in a single view, even those resources that are spread across different Regions. There are ways to get a good view and details of the Amazon EC2 Instances that are available</p>"},{"location":"guides/ec2/ec2-monitoring/#automatic-dashboards-in-cloudwatch","title":"Automatic Dashboards in CloudWatch","text":"<p>Automatic Dashboards are available in all AWS public regions which provides an aggregated view of the health and performance of all AWS resources including Amazon EC2 instances under CloudWatch. This helps customers quickly get started with monitoring, resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues. Automatic Dashboards are pre-built with AWS service recommended best practices, remain resource aware, and dynamically update to reflect the latest state of important performance metrics.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#custom-dashboards-in-cloudwatch","title":"Custom Dashboards in CloudWatch","text":"<p>With Custom Dashboards Customers can create as many additional dashboards as they want with different widgets and customize it accordingly . Dashboards can be configured for cross-region and cross account view and can be added to a favorites list.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#resource-health-dashboards-in-cloudwatch","title":"Resource Health Dashboards in CloudWatch","text":"<p>Resource Health in CloudWatch ServiceLens is a fully managed solution that customers can use to automatically discover, manage, and visualize the health and performance of Amazon EC2 hosts across their applications. Customers can visualize the health of their hosts by performance dimension such as CPU or memory, and slice and dice hundreds of hosts in a single view using filters such as instance type, instance state, or security groups. It enables a side-by-side comparison of a group of EC2 hosts and provides granular insights into an individual host.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#monitoring-and-observability-using-open-source-tools","title":"Monitoring And Observability using Open Source Tools","text":""},{"location":"guides/ec2/ec2-monitoring/#monitoring-of-amazon-ec2-instances-using-aws-distro-for-opentelemetry","title":"Monitoring of Amazon EC2 Instances using AWS Distro for OpenTelemetry","text":"<p>AWS Distro for OpenTelemetry (ADOT) is a secure, production-ready, AWS-supported distribution of the OpenTelemetry project. Part of the Cloud Native Computing Foundation, OpenTelemetry provides open source APIs, libraries, and agents to collect distributed traces and metrics for application monitoring. With AWS Distro for OpenTelemetry, customers can instrument applications just once to send correlated metrics and traces to multiple AWS and Partner monitoring solutions.</p> <p></p> <p>AWS Distro for OpenTelemetry (ADOT) provides a distributed monitoring framework that enables correlating data for monitoring application performance and health in an easy way which is critical for greater service visibility and maintenance.</p> <p>The key components of ADOT are SDKs, auto-instrumentation agents, collectors and exporters to send data to back-end services.</p> <p>OpenTelemetry SDK: To enable the collection of AWS resource-specific metadata, support to the OpenTelemetry SDKs for the X-Ray trace format and context. OpenTelemetry SDKs now correlate ingested trace and metrics data from AWS X-Ray and CloudWatch.</p> <p>Auto-instrumentation agent: Support in the OpenTelemetry Java auto-instrumentation agent are added for AWS SDK and AWS X-Ray trace data.</p> <p>OpenTelemetry Collector: The collector in the distribution is built using the upstream OpenTelemetry collector. Added AWS-specific exporters to the upstream collector to send data to AWS services including AWS X-Ray, Amazon CloudWatch and Amazon Managed Service for Prometheus. </p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#metrics-traces-through-adot-collector-amazon-cloudwatch","title":"Metrics &amp; Traces through ADOT Collector &amp; Amazon CloudWatch","text":"<p>AWS Distro for OpenTelemetry (ADOT) Collector along with the CloudWatch agent can be installed side-by-side on Amazon EC2 Instance and OpenTelemetry SDKs can be used to collect application traces &amp; metrics from your workloads running on Amazon EC2 Instances. </p> <p>To support OpenTelemetry metrics in Amazon CloudWatch, AWS EMF Exporter for OpenTelemetry Collector converts OpenTelemetry format metrics to CloudWatch Embedded Metric Format(EMF) which enables applications integrated in  OpenTelemetry metrics to be able to send high-cardinality application metrics to CloudWatch. The X-Ray exporter allows traces collected in an OTLP format to be exported to AWS X-ray.</p> <p></p> <p>ADOT Collector on Amazon EC2 can be installed through AWS CloudFormation or using AWS Systems Manager Distributor to collect application metrics.</p>"},{"location":"guides/ec2/ec2-monitoring/#monitoring-of-amazon-ec2-instances-using-prometheus","title":"Monitoring of Amazon EC2 Instances using Prometheus","text":"<p>Prometheus is a standalone open-source project and maintained independently for systems monitoring and alerting. Prometheus collects and stores metrics as time series data, i.e. metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels.</p> <p></p> <p>Prometheus is configured via command line flags and all the configuration details are maintained in the prometheus.yaml file. The 'scrape_config' section within the configuration file specifies the targets and parameters specifying how to scrape them. Prometheus Service Discovery (SD) is a methodology of finding endpoints to scrape for metrics. Amazon EC2 service discovery configurations allow retrieving scrape targets from AWS EC2 instances are configured in the <code>ec2_sd_config</code>.</p>"},{"location":"guides/ec2/ec2-monitoring/#metrics-through-prometheus-amazon-cloudwatch","title":"Metrics through Prometheus &amp; Amazon CloudWatch","text":"<p>The CloudWatch agent on EC2 instances can be installed &amp; configured with Prometheus to scrape metrics for monitoring in CloudWatch. This can be helpful to customers who prefer container workloads on EC2 and require custom metrics that are compatible with open source Prometheus monitoring. Installation of CloudWatch Agent can be done by following the steps explained in the earlier section above. The CloudWatch agent with Prometheus monitoring needs two configurations to scrape the Prometheus metrics. One is for the standard Prometheus configurations as documented in 'scrape_config' in the Prometheus documentation. The other is for the CloudWatch agent configuration.</p>"},{"location":"guides/ec2/ec2-monitoring/#metrics-through-prometheus-adot-collector","title":"Metrics through Prometheus &amp; ADOT Collector","text":"<p>Customers can choose to have an all open-source setup for their observability needs. For which, AWS Distro for OpenTelemetry (ADOT) Collector can be configured to scrape from a Prometheus-instrumented application and send the metrics to Prometheus Server. There are three OpenTelemetry components involved in this flow, that are the Prometheus Receiver, the Prometheus Remote Write Exporter, and the Sigv4 Authentication Extension. Prometheus Receiver receives metric data in Prometheus format. Prometheus Exporter exports data in Prometheus format. Sigv4 Authenticator extension provides Sigv4 authentication for making requests to AWS services.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#prometheus-node-exporter","title":"Prometheus Node Exporter","text":"<p>Prometheus Node Exporter is an open-source time series monitoring and alerting system for cloud environments. Amazon EC2 Instances can be instrumented with Node Exporter to collect and store node-level metrics as time-series data, recording information with a timestamp. Node exporter is a Prometheus exporter which can expose variety of host metrics via URL http://localhost:9100/metrics.</p> <p></p> <p>Once the metrics are created, they can be sent to Amazon Managed Prometheus.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#streaming-logs-from-amazon-ec2-instances-using-fluent-bit-plugin","title":"Streaming Logs from Amazon EC2 Instances using Fluent Bit Plugin","text":"<p>Fluent Bit is an open source and multi-platform log processor tool for handling data collection at scale, collecting &amp; aggregating diverse data that deal with various sources of information, variety of data formats, data reliability, security, flexible routing and multiple destinations.</p> <p></p> <p>Fluent Bit helps create an easy extension point for streaming logs from Amazon EC2 to AWS services including Amazon CloudWatch for log retention and analytics. The newly-launched Fluent Bit plugin can route logs to Amazon CloudWatch.</p>"},{"location":"guides/ec2/ec2-monitoring/#dashboarding-with-amazon-managed-grafana","title":"Dashboarding with Amazon Managed Grafana","text":"<p>Amazon Managed Grafana is a fully managed service based on the open source Grafana project, with rich, interactive &amp; secure data visualizations to help customers instantly query, correlate, analyze, monitor, and alarm on metrics, logs, and traces across multiple data sources. Customers can create interactive dashboards and share them with anyone in their organization with an automatically scaled, highly available, and enterprise-secure service. With Amazon Managed Grafana, customers can manage user and team access to dashboards across AWS accounts, AWS regions, and data sources.</p> <p></p> <p>Amazon Managed Grafana can be added with Amazon CloudWatch as a data source by using the AWS data source configuration option in the Grafana workspace console. This feature simplifies adding CloudWatch as a data source by discovering existing CloudWatch accounts and manage the configuration of the authentication credentials that are required to access CloudWatch. Amazon Managed Grafana also supports Prometheus data sources, i.e. both self-managed Prometheus servers and Amazon Managed Service for Prometheus workspaces as data sources.</p> <p>Amazon Managed Grafana comes with a variety of panels, makes it easy to construct the right queries and customize the display properties allowing customers to create the dashboards they need.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#conclusion","title":"Conclusion","text":"<p>Monitoring keeps you informed of whether a system is working properly. Observability lets you understand why the system is not working properly. Good observability allows you to answer the questions you didn't know that you needed to be aware of. Monitoring &amp; Observability paves way for measuring the internal states of a system which can be inferred from its outputs.</p> <p>Modern applications, those running on cloud in microservices, serverless and asynchronous architectures, generate large volumes of data in the form of metrics, logs, traces and events. Amazon CloudWatch along with open source tools such as Amazon Distro for OpenTelemetry, Amazon Managed Prometheus, and Amazon Managed Grafana, enable customers to collect, access, and correlate this data on a unified platform. Helping customers break down data silos so you can easily gain system-wide visibility and quickly resolve issues. </p>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/","title":"Operating the AWS Distro for OpenTelemetry (ADOT) Collector","text":"<p>The ADOT collector is a downstream distribution of the open-source OpenTelemetry Collector by CNCF.</p> <p>Customers can use the ADOT Collector to collect signals such as metrics and traces from different environments including on-prem, AWS and from other cloud providers.</p> <p>In order to operate the ADOT Collector in a real world environment and at scale, operators should monitor the collector health, and scale as needed. In this guide, you will learn about the actions one can take to operate the ADOT Collector in a production environment.</p>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#deployment-architecture","title":"Deployment architecture","text":"<p>Depending on the your requirements, there are a few deployment options that you might want to consider.</p> <ul> <li>No Collector</li> <li>Agent</li> <li>Gateway</li> </ul> <p>Tip</p> <p>Check out the OpenTelemetry documentation for additional information on these concepts.</p>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#no-collector","title":"No Collector","text":"<p>This option essentially skips the collector from the equation completely. If you are not aware, it is possible to make the API calls to destination services directly from the OTEL SDK and send the signals. Think about you making calls to the AWS X-Ray's PutTraceSegments API directly from your application process instead of sending the spans to an out-of-process agent such as the ADOT Collector.</p> <p>We strongly encourage you to take a look at the section in the upstream documentation for more specifics as there isn't any AWS specific aspect that changes the guidance for this approach.</p> <p></p>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#agent","title":"Agent","text":"<p>In this approach, you will run the collector in a distributed manner and collect signals into the destinations. Unlike the <code>No Collector</code> option, here we separate the concerns and decouple the application from having to use its resources to make remote API calls and instead communicate to a locally accessible agent.</p> <p>Essentially it will look like this below in an Amazon EKS environment running the collector as a Kubernetes sidecar:</p> <p></p> <p>In this above architecture, your scrape configuration shouldn't really have to make use of any service discovery mechanisms at all since you will be scraping the targets from <code>localhost</code> given that the collector is running in the same pod as the application container.</p> <p>The same architecture applies to collecting traces as well. You will simply have to create a OTEL pipeline as shown here</p>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#pros-and-cons","title":"Pros and Cons","text":"<ul> <li> <p>One argument advocating for this design is that you don't have to allocate extra-ordinary amount of resources (CPU, Memory) for the Collector to do its job since the targets are limited to localhost sources.</p> </li> <li> <p>The disadvantage of using this approach could be that, the number of varied configurations for the collector pod configuration is directly proportional to the number of applications you are running on the cluster. This means, you will have to manage CPU, Memory and other resource allocation individually for each Pod depending on the workload that is expected for the Pod. By not being careful with this, you might over or under-allocate resources for the Collector Pod that will result in either under-performing or locking up CPU cycles and Memory which could otherwise be used by other Pods in the Node.</p> </li> </ul> <p>You could also deploy the collector in other models such as Deployments, Daemonset, Statefulset etc based on your needs.</p>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#running-the-collector-as-a-daemonset-on-amazon-eks","title":"Running the collector as a Daemonset on Amazon EKS","text":"<p>You can choose to run the collector as a Daemonset in case you want to evenly distribute the load (scraping and sending the metrics to Amazon Managed Service for Prometheus workspace) of the collectors across the EKS Nodes.</p> <p></p> <p>Ensure you have the <code>keep</code> action that makes the collector only scrape targets from its own host/Node.</p> <p>See sample below for reference. Find more such configuration details here.</p> <pre><code>scrape_configs:\n    - job_name: kubernetes-apiservers\n    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n    kubernetes_sd_configs:\n    - role: endpoints\n    relabel_configs:\n    - action: keep\n        regex: $K8S_NODE_NAME\n        source_labels: [__meta_kubernetes_endpoint_node_name]\n    scheme: https\n    tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n        insecure_skip_verify: true\n</code></pre> <p>The same architecture can also be used for collecting traces. In this case, instead of the Collector reaching out to the endpoints to scrape Prometheus metrics, the trace spans will be sent to the Collector by the application pods.</p>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#pros-and-cons_1","title":"Pros and Cons","text":"<p>Advantages</p> <ul> <li>Minimal scaling concerns</li> <li>Configuring High-Availability is a challenge</li> <li>Too many copies of Collector in use</li> <li>Can be easy for Logs support</li> </ul> <p>Disadvantages</p> <ul> <li>Not the most optimal in terms of resource utilization</li> <li>Disproportionate resource allocation</li> </ul>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#running-the-collector-on-amazon-ec2","title":"Running the collector on Amazon EC2","text":"<p>As there is no side car approach in running the collector on EC2, you would be running the collector as an agent on the EC2 instance. You can set a static scrape configuration such as the one below to discover targets in the instance to scrape metrics from.</p> <p>The config below scrapes endpoints at ports <code>9090</code> and <code>8081</code> on localhost.</p> <p>Get a hands-on deep dive experience in this topic by going through our EC2 focused module in the One Observability Workshop.</p> <pre><code>global:\n  scrape_interval: 15s # By default, scrape targets every 15 seconds.\n\nscrape_configs:\n- job_name: 'prometheus'\n  static_configs:\n  - targets: ['localhost:9090', 'localhost:8081']\n</code></pre>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#running-the-collector-as-deployment-on-amazon-eks","title":"Running the collector as Deployment on Amazon EKS","text":"<p>Running the collector as a Deployment is particularly useful when you want to also provide High Availability for your collectors. Depending on the number of targets, metrics available to scrape etc the resources for the Collector should be adjusted to ensure the collector isn't starving and hence causing issues in signal collection.</p> <p>Read more about this topic in the guide here.</p> <p>The following architecture shows how a collector is deployed in a separate node outside of the workload nodes to collect metrics and traces.</p> <p></p> <p>To setup High-Availability for metric collection, read our docs that provide detailed instructions on how you can set that up</p>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#running-the-collector-as-a-central-task-on-amazon-ecs-for-metrics-collection","title":"Running the collector as a central task on Amazon ECS for metrics collection","text":"<p>You can use the ECS Observer extension to collect Prometheus metrics across different tasks in an ECS cluster or across clusters.</p> <p></p> <p>Sample collector configuration for the extension:</p> <pre><code>extensions:\n  ecs_observer:\n    refresh_interval: 60s # format is https://golang.org/pkg/time/#ParseDuration\n    cluster_name: 'Cluster-1' # cluster name need manual config\n    cluster_region: 'us-west-2' # region can be configured directly or use AWS_REGION env var\n    result_file: '/etc/ecs_sd_targets.yaml' # the directory for file must already exists\n    services:\n      - name_pattern: '^retail-.*$'\n    docker_labels:\n      - port_label: 'ECS_PROMETHEUS_EXPORTER_PORT'\n    task_definitions:\n      - job_name: 'task_def_1'\n        metrics_path: '/metrics'\n        metrics_ports:\n          - 9113\n          - 9090\n        arn_pattern: '.*:task-definition/nginx:[0-9]+'\n</code></pre>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#pros-and-cons_2","title":"Pros and Cons","text":"<ul> <li>An advantage in this model is that there are fewer collectors and configurations to manage yourself.</li> <li>When the cluster is rather large and there are thousands of targets to scrape, you will have to carefully design the architecture in such a way that the load is balanced across the collectors. Adding this to having to run near-clones of the same collectors for HA reasons should be done carefully in order to avoid operational issues.</li> </ul>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#gateway","title":"Gateway","text":""},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#managing-collector-health","title":"Managing Collector health","text":"<p>The OTEL Collector exposes several signals for us to keep tab of its health and performance. It is essential that the collector's health is closely monitored in order to take corrective actions such as,</p> <ul> <li>Scaling the collector horizontally</li> <li>Provisioning additional resources to the collector for it to function as desired</li> </ul>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#collecting-health-metrics-from-the-collector","title":"Collecting health metrics from the Collector","text":"<p>The OTEL Collector can be configured to expose metrics in Prometheus Exposition Format by simply adding the <code>telemetry</code> section to the <code>service</code> pipeline. The collector also can expose its own logs to stdout.</p> <p>More details on telemetry configuration can be found in the OpenTelemetry documentation here.</p> <p>Sample telemetry configuration for the collector.</p> <p><pre><code>service:\n  telemetry:\n    logs:\n      level: debug\n    metrics:\n      level: detailed\n      address: 0.0.0.0:8888\n</code></pre> Once configured, the collector will start exporting metrics such as this below at <code>http://localhost:8888/metrics</code>.</p> <pre><code># HELP otelcol_exporter_enqueue_failed_spans Number of spans failed to be added to the sending queue.\n# TYPE otelcol_exporter_enqueue_failed_spans counter\notelcol_exporter_enqueue_failed_spans{exporter=\"awsxray\",service_instance_id=\"523a2182-539d-47f6-ba3c-13867b60092a\",service_name=\"aws-otel-collector\",service_version=\"v0.25.0\"} 0\n\n# HELP otelcol_process_runtime_total_sys_memory_bytes Total bytes of memory obtained from the OS (see 'go doc runtime.MemStats.Sys')\n# TYPE otelcol_process_runtime_total_sys_memory_bytes gauge\notelcol_process_runtime_total_sys_memory_bytes{service_instance_id=\"523a2182-539d-47f6-ba3c-13867b60092a\",service_name=\"aws-otel-collector\",service_version=\"v0.25.0\"} 2.4462344e+07\n\n# HELP otelcol_process_memory_rss Total physical memory (resident set size)\n# TYPE otelcol_process_memory_rss gauge\notelcol_process_memory_rss{service_instance_id=\"523a2182-539d-47f6-ba3c-13867b60092a\",service_name=\"aws-otel-collector\",service_version=\"v0.25.0\"} 6.5675264e+07\n\n# HELP otelcol_exporter_enqueue_failed_metric_points Number of metric points failed to be added to the sending queue.\n# TYPE otelcol_exporter_enqueue_failed_metric_points counter\notelcol_exporter_enqueue_failed_metric_points{exporter=\"awsxray\",service_instance_id=\"d234b769-dc8a-4b20-8b2b-9c4f342466fe\",service_name=\"aws-otel-collector\",service_version=\"v0.25.0\"} 0\notelcol_exporter_enqueue_failed_metric_points{exporter=\"logging\",service_instance_id=\"d234b769-dc8a-4b20-8b2b-9c4f342466fe\",service_name=\"aws-otel-collector\",service_version=\"v0.25.0\"} 0\n</code></pre> <p>In the above sample output, you can see that the collector is exposing a metric called <code>otelcol_exporter_enqueue_failed_spans</code> showing the number of spans that were failed to get added to the sending queue. This metric is one to watch out to understand if the collector is having issues in sending trace data to the destination configured. In this case, you can see that the <code>exporter</code> label with value <code>awsxray</code> indicating the trace destination in use.</p> <p>The other metric <code>otelcol_process_runtime_total_sys_memory_bytes</code> is an indicator to understand the amount of memory being used by the collector. If this memory goes too close to the value in <code>otelcol_process_memory_rss</code> metric, that is an indication that the Collector is getting close to exhausting the allocated memory for the process and it might be time for you to take action such as allocating more memory for the collector to avoid issues.</p> <p>Likewise, you can see that there is another counter metric called <code>otelcol_exporter_enqueue_failed_metric_points</code> that indicates the number of metrics that failed to be sent to the remote destination</p>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#collector-health-check","title":"Collector health check","text":"<p>There is a liveness probe that the collector exposes in-order for you to check whether the collector is live or not. It is recommended to use that endpoint to periodically check the collector's availability.</p> <p>The <code>healthcheck</code> extension can be used to have the collector expose the endpoint. See sample configuration below:</p> <pre><code>extensions:\n  health_check:\n    endpoint: 0.0.0.0:13133\n</code></pre> <p>For the complete configuration options, refer to the GitHub repo here.</p> <pre><code>\u276f curl -v http://localhost:13133\n*   Trying 127.0.0.1:13133...\n* Connected to localhost (127.0.0.1) port 13133 (#0)\n&gt; GET / HTTP/1.1\n&gt; Host: localhost:13133\n&gt; User-Agent: curl/7.79.1\n&gt; Accept: */*\n&gt;\n* Mark bundle as not supporting multiuse\n&lt; HTTP/1.1 200 OK\n&lt; Date: Fri, 24 Feb 2023 19:09:22 GMT\n&lt; Content-Length: 0\n&lt;\n* Connection #0 to host localhost left intact\n</code></pre>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#setting-limits-to-prevent-catastrophic-failures","title":"Setting limits to prevent catastrophic failures","text":"<p>Given that resources (CPU, Memory) are finite in any environment, you should set limits to the collector components in-order to avoid failures due to unforeseen situations.</p> <p>It is particularly important when you are operating the ADOT Collector to collect Prometheus metrics. Take this scenario - You are in the DevOps team and are responsible for deploying and operating the ADOT Collector in an Amazon EKS cluster. Your application teams can simply drop their application Pods at will anytime of the day, and they expect the metrics exposed from their pods to be collected into an Amazon Managed Service for Prometheus workspace.</p> <p>Now it is your responsibility to ensure that this pipeline works without any hiccups. There are two ways to solve this problem at a high level:</p> <ul> <li>Scaling the collector infinitely (hence adding Nodes to the cluster if needed) to support this requirement</li> <li>Set limits on metric collection and advertise the upper threshold to the application teams</li> </ul> <p>There are pros and cons to both approaches. You can argue that you want to choose option 1, if you are fully committed to supporting your ever growing business needs not considering the costs or the overhead that it might bring in. While supporting the ever growing business needs infinitely might sound like <code>cloud is for infinite scalability</code> point of view, this can bring in a lot of operational overhead and might lead into much more catastrophical situations if not given infinite amount of time, and people resources to ensure continual uninterrupted operations, which in most cases is not practical.</p> <p>A much more pragmatic and frugal approach would be to choose option 2, where you are setting upper limits (and potentially increasing gradually based on needs progressively) at any given time to ensure the operational boundary is obvious.</p> <p>Here is an example of how you can do that with using Prometheus receiver in the ADOT Collector.</p> <p>In Prometheus scrape_config, you can set several limits for any particular scrape job. You could put limits on,</p> <ul> <li>The total body size of the scrape</li> <li>Limit number of labels to accept (the scrape will be discarded if this limit exceeds and you can see that in the Collector logs)</li> <li>Limit the number of targets to scrape</li> <li>..more</li> </ul> <p>You can see all available options in the Prometheus documentation.</p>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#limiting-memory-usage","title":"Limiting Memory usage","text":"<p>The Collector pipeline can be configured to use <code>memorylimiterprocessor</code> to limit the amount of memory the processor component will use. It is common to see customers wanting the Collector to do complex operations that require intense Memory and CPU requirements.</p> <p>While using processors such as <code>redactionprocessor,</code><code>filterprocessor,</code><code>spanprocessor,</code> are exciting and very useful, you should also remember that processors in general deal with data transformation tasks and it requires them to keep data in-memory in-order to complete the tasks. This can lead to a specific processor breaking the Collector entirely and also the Collector not having enough memory to expose its own health metrics.</p> <p>You can avoid this by limiting the amount of memory the Collector can use by making use of the  <code>memorylimiterprocessor.</code>. The recommendation for this is to provide buffer memory for the Collector to make use of for exposing health metrics and perform other tasks so the processors do not take all the allocated memory.</p> <p>For example, if your EKS Pod has a memory limit of <code>10Gi</code>, then set the <code>memorylimitprocessor</code> to less than <code>10Gi</code>, for example <code>9Gi</code> so the buffer of <code>1Gi</code> can be used to perform other operations such as exposing health metrics, receiver and exporter tasks.</p>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#backpressure-management","title":"Backpressure management","text":"<p>Some architecture patterns (Gateway pattern) such as the one shown below can be used to centralize some operational tasks such as (but not limited to) filtering out sensitive data out of signal data to maintain compliance requirements.</p> <p></p> <p>However, it is possible to overwhelm the Gateway Collector with too many such processing tasks that can cause issues. The recommended approach would be is to distribute the process/memory intense tasks between the individual collectors and the gateway so the workload is shared.</p> <p>For example, you could use the <code>resourceprocessor</code> to process resource attributes and use the <code>transformprocessor</code> to transform the signal data from within the individual Collectors as soon as the signal collection happens.</p> <p>Then you could use the <code>filterprocessor</code> to filter out certain parts of the signal data and use the <code>redactionprocessor</code> to redact sensitive information such as Credit Card numbers etc.</p> <p>The high-level architecture diagram would look like the one below:</p> <p></p> <p>As you might have observed already, the Gateway Collector can soon become a single point of failure. One obvious choice there is to spin up more than one Gateway Collector and proxy requests through a load balancer like AWS Application Load Balancer (ALB) as shown below.</p> <p></p>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#handling-out-of-order-samples-in-prometheus-metric-collection","title":"Handling out-of-order samples in Prometheus metric collection","text":"<p>Consider the following scenario in the architecture below:</p> <p></p> <ol> <li>Assume that metrics from ADOT Collector-1 in the Amazon EKS Cluster are sent to the Gateway cluster, which is being directed to the Gateway ADOT Collector-1</li> <li>In a moment, the metrics from the same ADOT Collector-1 (which is collecting the same targets, hence the same metric samples are being dealt with) is being sent to Gateway ADOT Collector-2</li> <li>Now if the Gateway ADOT Collector-2 happens to dispatch the metrics to Amazon Managed Service for Prometheus workspace first and then followed by the Gateway ADOT Collector-1 which contains older samples for the same metrics series, you will receive the <code>out of order sample</code> error from Amazon Managed Service for Prometheus.</li> </ol> <p>See example error below:</p> <pre><code>Error message:\n 2023-03-02T21:18:54.447Z        error   exporterhelper/queued_retry.go:394      Exporting failed. The error is not retryable. Dropping data.    {\"kind\": \"exporter\", \"data_type\": \"metrics\", \"name\": \"prometheusremotewrite\", \"error\": \"Permanent error: Permanent error: remote write returned HTTP status 400 Bad Request; err = %!w(&lt;nil&gt;): user=820326043460_ws-5f42c3b6-3268-4737-b215-1371b55a9ef2: err: out of order sample. timestamp=2023-03-02T21:17:59.782Z, series={__name__=\\\"otelcol_exporter_send_failed_metric_points\\\", exporter=\\\"logging\\\", http_scheme=\\\"http\\\", instance=\\\"10.195.158.91:28888\\\", \", \"dropped_items\": 6474}\ngo.opentelemetry.io/collector/exporter/exporterhelper.(*retrySender).send\n        go.opentelemetry.io/collector@v0.66.0/exporter/exporterhelper/queued_retry.go:394\ngo.opentelemetry.io/collector/exporter/exporterhelper.(*metricsSenderWithObservability).send\n        go.opentelemetry.io/collector@v0.66.0/exporter/exporterhelper/metrics.go:135\ngo.opentelemetry.io/collector/exporter/exporterhelper.(*queuedRetrySender).start.func1\n        go.opentelemetry.io/collector@v0.66.0/exporter/exporterhelper/queued_retry.go:205\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*boundedMemoryQueue).StartConsumers.func1\n        go.opentelemetry.io/collector@v0.66.0/exporter/exporterhelper/internal/bounded_memory_queue.go:61\n</code></pre>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#solving-out-of-order-sample-error","title":"Solving out of order sample error","text":"<p>You can solve the out of order sample error in this particular setup in a couple of ways:</p> <ul> <li>Use a sticky load balancer to direct requests from a particular source to go to the same target based on IP address.</li> </ul> <p>Refer to the link here for additional details.</p> <ul> <li> <p>As an alternate option, you can add an external label in the Gateway Collectors to distinguish the metric series so Amazon Managed Service for Prometheus considers these metrics are individual metric series and are not from the same.</p> <p>Warning</p> <p>Using this solution can will result in multiplying the metric series in proportion to the Gateway Collectors in the setup. This is might mean that you can overrun some limits such as <code>Active time series limits</code></p> </li> <li> <p>If you are deploying ADOT Collector as a Daemonset: make sure you are using <code>relabel_configs</code> to only keep samples from the same node where each ADOT Collector pod is running. Check the links below to learn more.</p> <ul> <li>Advanced Collector Configuration for Amazon Managed Prometheus - Expand the Click to View section, and look for the entried similar to the following:     <pre><code>    relabel_configs:\n    - action: keep\n      regex: $K8S_NODE_NAME\n</code></pre></li> <li>ADOT Add-On Advanced Configuration - Learn how to deploy ADOT Collector using the ADOT Add-On for EKS advanced configurations.</li> <li>ADOT Collector deployment strategies - Learn more about the different alternatives to deploy ADOT Collector at scale and the advantages of each approach.</li> </ul> </li> </ul>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#open-agent-management-protocol-opamp","title":"Open Agent Management Protocol (OpAMP)","text":"<p>OpAMP is a client/server protocol that supports communication over HTTP and over WebSockets. OpAMP is implemented in the OTel Collector and hence the OTel Collector can be used as a server as part of the control plane to manage other agents that support OpAMP, like the OTel Collector itself. The \"manage\" portion here involves being able to update configurations for collectors, monitoring health or even upgrading the Collectors.</p> <p>The details of this protocol is well documented in the upstream OpenTelemetry website.</p>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>It may become necessary to horizontally scale an ADOT Collector depending on your workload. The requirement to horizontally scale is entirely dependent on your use case, Collector configuration, and  telemetry throughput. </p> <p>Platform specific horizontal scaling techniques can be applied to a Collector as you would any other application while being cognizant of stateful, stateless, and scraper Collector components. </p> <p>Most collector components are <code>stateless</code>, meaning that they do not hold state in memory, and if they do it is not relevant for scaling purposes. Additional replicas of stateless Collectors can be scaled behind an  application load balancer.</p> <p><code>Stateful</code> Collector components are collector components that retain information in memory which is crucial for the operation of that component.</p> <p>Examples of stateful components in the ADOT Collector include but are not limited to:</p> <ul> <li>Tail Sampling Processor - requires all spans for a trace to make an accurate sampling decisions. Avanced sampling scaling techniques is documented on the ADOT developer portal. </li> <li>AWS EMF Exporter - performs cummulative to delta conversions on some metric types. This conversion requires the previous metric value to be stored in memory. </li> <li>Cummulative to Delta Processor - cummulative to delta conversion requires storing the previous metric value in memory. </li> </ul> <p>Collector components that are <code>scrapers</code> actively obtain telemetry data rather than passively receive it. Currently, the Prometheus receiver is the only scraper type component in the ADOT Collector. Horizontally scaling a collector configuration that contains a prometheus receiver will require splitting the scraping jobs per collector to ensure that no two Collectors scrape the same endpoint. Failure to do this may lead to Prometheus out of order sample errors. </p> <p>The process of and techniques of scaling collectors is documunted in greater detail in the upstream OpenTelemetry website. </p>"},{"location":"guides/operational/adot-at-scale/operating-adot-collector/#references","title":"References","text":"<ul> <li>https://opentelemetry.io/docs/collector/deployment/</li> <li>https://opentelemetry.io/docs/collector/management/</li> <li>https://opentelemetry.io/docs/collector/scaling/</li> <li>https://github.com/aws-observability/aws-otel-collector</li> <li>https://aws-observability.github.io/terraform-aws-observability-accelerator/</li> <li>https://catalog.workshops.aws/observability/en-US/aws-managed-oss/adot</li> <li>https://aws.amazon.com/blogs/opensource/setting-up-amazon-managed-grafana-cross-account-data-source-using-customer-managed-iam-roles/</li> <li>https://aws.amazon.com/blogs/opensource/set-up-cross-region-metrics-collection-for-amazon-managed-service-for-prometheus-workspaces/</li> </ul>"},{"location":"guides/operational/adot-at-scale/adot-java-spring/adot-java-spring/","title":"Instrumenting Java Spring Integration Applications","text":"<p>This article describes an approach for manually instrumenting Spring-Integration applications utilizing Open Telemetry and X-ray.</p> <p>The Spring-Integration framework is designed to enable the development of integration solutions typical of event-driven architectures and messaging-centric architectures. On the other hand, OpenTelemetry tends to be more focused on micro services architectures, in which services communicate and coordinate with each other using HTTP requests. Therefore this guide will provide an example of how to instrument Spring-Integration applications using manual instrumentation with the OpenTelemetry API.</p>"},{"location":"guides/operational/adot-at-scale/adot-java-spring/adot-java-spring/#background-information","title":"Background Information","text":""},{"location":"guides/operational/adot-at-scale/adot-java-spring/adot-java-spring/#what-is-tracing","title":"What is tracing?","text":"<p>The following quote from the OpenTelemetry documentation gives a good overview of what a trace's purpose is:</p> <p>Quote</p> <p>Traces give us the big picture of what happens when a request is made to an application. Whether your application is a monolith with a single database or a sophisticated mesh of services, traces are essential to understanding the full \u201cpath\u201d a request takes in your application.</p> <p>Given that one of the main benefits of tracing is end-to-end visibility of a request, it is important for traces to link properly all the way from the request origin to the backend. A common way of doing this in OpenTelemetry is to utilize nested spans. This works in a microservices architecture where the spans are passed from service to service until they reach the final destination. In a Spring Integration application, we need to create parent/child relationships between spans created both remotely AND locally.</p>"},{"location":"guides/operational/adot-at-scale/adot-java-spring/adot-java-spring/#tracing-utilizing-context-propagation","title":"Tracing Utilizing Context Propagation","text":"<p>We will demonstrate an approach using context propagation. Although this approach is traditionally used when you need to create parent/child relationship between spans created locally and in remote locations, it will be used for the case of the Spring Integration Application because it simplifies the code and will allow the application to scale: it will be possible to process messages in parallel in multiple threads and it will also be possible to scale horizontally in case we need to process messages in different hosts.</p> <p>Here is an overview of what is necessary to achieve this:</p> <ul> <li> <p>Create a <code>ChannelInterceptor</code> and register it as a <code>GlobalChannelInterceptor</code> so that it can capture messages being sent across all channels.</p> </li> <li> <p>In the <code>ChannelInterceptor</code>:</p> </li> <li>In the <code>preSend</code> method:<ul> <li>try to read the context from the previous message that is being generated upstream.This is where we are able to connect spans from upstream messages. If no context exists, a new trace is started (this is done by the OpenTelemetry SDK). </li> <li>Create a Span with a unique name that identifies that operation. This can be the name of the channel where this message is being processed.</li> <li>Save current context in the message.</li> <li>Store the context and scope in thread.local so that they can be closed afterwards.</li> <li>inject context in the message being sent downstream.</li> </ul> </li> <li>In the <code>afterSendCompletion</code>:<ul> <li>Restore the context and scope from thread.local</li> <li>Recreate the span from the context.</li> <li>Register any exceptions raised while processing the message.</li> <li>Close Scope.</li> <li>End Span.</li> </ul> </li> </ul> <p>This is a simplified description of what needs to be done. We are providing a functional sample application that uses the Spring-Integration framework. The code for this application can be found here.</p> <p>To view only the changes that were put in place to instrument the application, view this diff.</p>"},{"location":"guides/operational/adot-at-scale/adot-java-spring/adot-java-spring/#to-run-this-sample-application-use","title":"To run this sample application use:","text":"<pre><code># build and run\nmvn spring-boot:run\n# create sample input file to trigger flow\necho 'testcontent\\nline2content\\nlastline' &gt; /tmp/in/testfile.txt\n</code></pre> <p>To experiment with this sample application, you will need to have the ADOT collector running in the same machine as the application with a configuration similar to the following one:</p> <pre><code>receivers:\n  otlp:\n    protocols:\n      grpc: \n        endpoint: 0.0.0.0:4317\n      http:\n        endpoint: 0.0.0.0:4318\nprocessors:\n  batch/traces:\n    timeout: 1s\n    send_batch_size: 50\n  batch/metrics:\n    timeout: 60s\nexporters:\n  aws xray: region:us-west-2\n  aws emf:\n    region: us-west-2\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: [batch/traces]\n      exporters: [awsxray]\n    metrics:\n      receivers: [otlp]\n      processors: [batch/metrics]\n      exporters: [awsemf]\n</code></pre>"},{"location":"guides/operational/adot-at-scale/adot-java-spring/adot-java-spring/#results","title":"Results","text":"<p>If we run the sample application and then run the following command, this is what we get:</p> <pre><code>echo 'foo123\\nbar123\\nfoo1234' &gt; /tmp/in/testfile.txt\n</code></pre> <p></p> <p>We can see that the segments above match the workflow described in the sample application. Exceptions are expected when some of the messages were processed, therefore we can see that they are being properly registered and will allow us to troubleshoot them in X-Ray.</p>"},{"location":"guides/operational/adot-at-scale/adot-java-spring/adot-java-spring/#faq","title":"FAQ","text":""},{"location":"guides/operational/adot-at-scale/adot-java-spring/adot-java-spring/#how-do-we-create-nested-spans","title":"How do we create nested spans?","text":"<p>There are three mechanisms in OpenTelemetry that can be used to connect spans:</p>"},{"location":"guides/operational/adot-at-scale/adot-java-spring/adot-java-spring/#explicitly","title":"Explicitly","text":"<p>You need to pass the parent span to the place where the child span is created and link both of them using:</p> <pre><code>    Span childSpan = tracer.spanBuilder(\"child\")\n    .setParent(Context.current().with(parentSpan)) \n    .startSpan();\n</code></pre>"},{"location":"guides/operational/adot-at-scale/adot-java-spring/adot-java-spring/#implicitly","title":"Implicitly","text":"<p>The span context will be stored in thread.local under the hood. This method is indicated when you are sure that you are creating spans in the same thread.</p> <pre><code>    void parentTwo() {\n        Span parentSpan = tracer.spanBuilder(\"parent\").startSpan(); \n        try(Scope scope = parentSpan.makeCurrent()) {\n            childTwo(); \n        } finally {\n        parentSpan.end(); \n        }\n    }\n    void childTwo() {\n        Span childSpan = tracer.spanBuilder(\"child\")\n            // NOTE: setParent(...) is not required;\n            // `Span.current()` is automatically added as the parent \n            .startSpan();\n        try(Scope scope = childSpan.makeCurrent()) { \n            // do stuff\n        } finally {\n            childSpan.end();\n        } \n    }\n</code></pre>"},{"location":"guides/operational/adot-at-scale/adot-java-spring/adot-java-spring/#context-propagation","title":"Context Propagation","text":"<p>This method will store the context somewhere (HTTP headers or in a message) so that it can be transported to a remote location where the child span is created. It is not a strict requirement to be a remote location. This can be used in the same process as well.</p>"},{"location":"guides/operational/adot-at-scale/adot-java-spring/adot-java-spring/#how-are-opentelemetry-properties-translated-into-x-ray-properties","title":"How are OpenTelemetry properties translated into X-Ray properties?","text":"<p>Please see the following guide to view the relationship.</p>"},{"location":"guides/operational/alerting/amp-alertmgr/","title":"Amazon Managed Service for Prometheus Alert Manager","text":""},{"location":"guides/operational/alerting/amp-alertmgr/#introduction","title":"Introduction","text":"<p>Amazon Managed Service for Prometheus (AMP) supports two types of rules namely 'Recording rules' and 'Alerting rules', which can be imported from your existing Prometheus server and are evaluated at regular intervals.</p> <p>Alerting rules allow customers to define alert conditions based on PromQL and a threshold. When the value of the alerting rule exceeds threshold, a notification is sent to Alert manager in Amazon Managed Service for Prometheus which provides similar functionality to alert manager in standalone Prometheus. An alert is the outcome of an alerting rule in Prometheus when it is active.</p>"},{"location":"guides/operational/alerting/amp-alertmgr/#alerting-rules-file","title":"Alerting Rules File","text":"<p>An Alerting rule in Amazon Managed Service for Prometheus is defined by a rules file in YAML format, which follows the same format as a rules file in standalone Prometheus. Customers can have multiple rules files in an Amazon Managed Service for Prometheus workspace. A workspace is a logical space dedicated to the storage and querying of Prometheus metrics.</p> <p>A rules file typically has the following fields:</p> <pre><code>groups:\n  - name:\n  rules:\n  - alert:\n  expr:\n  for:\n  labels:\n  annotations:\n</code></pre> <pre><code>Groups: A collection of rules that are run sequentially at a regular interval\nName: Name of the group\nRules: The rules in a group\nAlert: Name of the alert\nExpr: The expression for the alert to trigger\nFor: Minimum duration for an alert\u2019s expression to be exceeding threshold before updating to a firing status\nLabels: Any additional labels attached to the alert\nAnnotations: Contextual details such as a description or link\n</code></pre> <p>A sample rule file looks like below</p> <pre><code>groups:\n  - name: test\n    rules:\n    - record: metric:recording_rule\n      expr: avg(rate(container_cpu_usage_seconds_total[5m]))\n  - name: alert-test\n    rules:\n    - alert: metric:alerting_rule\n      expr: avg(rate(container_cpu_usage_seconds_total[5m])) &gt; 0\n      for: 2m\n</code></pre>"},{"location":"guides/operational/alerting/amp-alertmgr/#alert-manager-configuration-file","title":"Alert Manager Configuration File","text":"<p>The Amazon Managed Service for Prometheus Alert Manager uses a configuration file in YAML format to set up the alerts (for the receiving service) that is in the same structure as an alert manager config file in standalone Prometheus. The configuration file consists of two key sections for alert manager and templating</p> <ol> <li> <p>template_files, contains the templates of annotations and labels in alerts exposed as the <code>$value</code>, <code>$labels</code>, <code>$externalLabels</code>, and <code>$externalURL</code> variables for convenience. The <code>$labels</code> variable holds the label key/value pairs of an alert instance. The configured external labels can be accessed via the <code>$externalLabels</code> variable. The <code>$value</code> variable holds the evaluated value of an alert instance. <code>.Value</code>, <code>.Labels</code>, <code>.ExternalLabels</code>, and <code>.ExternalURL</code> contain the alert value, the alert labels, the globally configured external labels, and the external URL (configured with <code>--web.external-url</code>) respectively.</p> </li> <li> <p>alertmanager_config, contains the alert manager configuration that uses the same structure as an alert manager config file in standalone Prometheus.</p> </li> </ol> <p>A sample alert manager configuration file having both template_files and alertmanager_config looks like below,</p> <pre><code>template_files:\n  default_template: |\n    {{ define \"sns.default.subject\" }}[{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}]{{ end }}\n    {{ define \"__alertmanager\" }}AlertManager{{ end }}\n    {{ define \"__alertmanagerURL\" }}{{ .ExternalURL }}/#/alerts?receiver={{ .Receiver | urlquery }}{{ end }}\nalertmanager_config: |\n  global:\n  templates:\n    - 'default_template'\n  route:\n    receiver: default\n  receivers:\n    - name: 'default'\n      sns_configs:\n      - topic_arn: arn:aws:sns:us-east-2:accountid:My-Topic\n        sigv4:\n          region: us-east-2\n        attributes:\n          key: severity\n          value: SEV2\n</code></pre>"},{"location":"guides/operational/alerting/amp-alertmgr/#key-aspects-of-alerting","title":"Key aspects of alerting","text":"<p>There are three important aspects to be aware of when creating Amazon Managed Service for Prometheus Alert Manager configuration file.</p> <ul> <li>Grouping: This helps collect similar alerts into a single notification, which is useful when the blast radius of failure or outage is large affecting many systems and several alerts fire simultaneously. This can also be used to group into categories (e.g., node alerts, pod alerts). The route block in the alert manager configuration file can be used to configure this grouping.</li> <li>Inhibition: This is a way to suppress certain notifications to avoid spamming similar alerts that are already active and fired. inhibit_rules block can be used to write inhibition rules.</li> <li>Silencing: Alerts can be muted for a specified duration, such as during a maintenance window or a planned outage. Incoming alerts are verified for matching all equality or regular expression before silencing the alert. PutAlertManagerSilences API can be used to create silencing.</li> </ul>"},{"location":"guides/operational/alerting/amp-alertmgr/#route-alerts-through-amazon-simple-notification-service-sns","title":"Route alerts through Amazon Simple Notification Service (SNS)","text":"<p>Currently Amazon Managed Service for Prometheus Alert Manager supports Amazon SNS as the only receiver. The key section in the alertmanager_config block is the receivers, which lets customers configure Amazon SNS to receive alerts. The following section can be used as a template for the receivers block.</p> <pre><code>- name: name_of_receiver\n  sns_configs:\n    - sigv4:\n        region: &lt;AWS_Region&gt;\n    topic_arn: &lt;ARN_of_SNS_topic&gt;\n    subject: somesubject\n    attributes:\n       key: &lt;somekey&gt;\n       value: &lt;somevalue&gt;\n</code></pre> <p>The Amazon SNS configuration uses the following template as default unless its explicitly overridden.</p> <pre><code>{{ define \"sns.default.message\" }}{{ .CommonAnnotations.SortedPairs.Values | join \" \" }}\n  {{ if gt (len .Alerts.Firing) 0 -}}\n  Alerts Firing:\n    {{ template \"__text_alert_list\" .Alerts.Firing }}\n  {{- end }}\n  {{ if gt (len .Alerts.Resolved) 0 -}}\n  Alerts Resolved:\n    {{ template \"__text_alert_list\" .Alerts.Resolved }}\n  {{- end }}\n{{- end }}\n</code></pre> <p>Additional Reference: Notification Template Examples</p>"},{"location":"guides/operational/alerting/amp-alertmgr/#routing-alerts-to-other-destinations-beyond-amazon-sns","title":"Routing alerts to other destinations beyond Amazon SNS","text":"<p>Amazon Managed Service for Prometheus Alert Manager can use Amazon SNS to connect to other destinations such as email, webhook (HTTP), Slack, PageDuty, and OpsGenie.</p> <ul> <li>Email A successful notification will result in an email received from Amazon Managed Service for Prometheus Alert Manager through Amazon SNS topic with the alert details as one of the targets.</li> <li>Amazon Managed Service for Prometheus Alert Manager can send alerts in JSON format, so that they can be processed downstream from Amazon SNS in AWS Lambda or in webhook-receiving endpoints.</li> <li>Webhook An existing Amazon SNS topic can be configured to output messages to a webhook endpoint. Webhooks are messages in serialized form-encoded JSON or XML formats, exchanged over HTTP between applications based on event driven triggers. This can be used to hook any existing SIEM or collaboration tools for alerting, ticketing or incident management systems.</li> <li>Slack Customers can integrate with Slack\u2019s email-to-channel integration where Slack can accept an email and forward it to a Slack channel, or use a Lambda function to rewrite the SNS notification to Slack.</li> <li>PagerDuty The template used in <code>template_files</code> block in the <code>alertmanager_config</code> definition can be customized to send the payload to PagerDuty as a destination of Amazon SNS.</li> </ul> <p>Additional Reference: Custom Alert manager Templates</p>"},{"location":"guides/operational/alerting/amp-alertmgr/#alert-status","title":"Alert status","text":"<p>Alerting rules define alert conditions based on expressions to send alerts to any notification service, whenever the set threshold is crossed. An example rule and its expression is shown below.</p> <pre><code>rules:\n- alert: metric:alerting_rule\n  expr: avg(rate(container_cpu_usage_seconds_total[5m])) &gt; 0\n  for: 2m\n</code></pre> <p>Whenever the alert expression results in one or more vector elements at a given point in time, the alert counts as active. The alerts take active (pending | firing) or resolved status.</p> <ul> <li>Pending: The time elapsed since threshold breach is less than the recording interval</li> <li>Firing: The time elapsed since threshold breach is more than the recording interval and Alert Manager is routing alerts.</li> <li>Resolved: The alert is no longer firing because the threshold is no longer breached.</li> </ul> <p>This can be manually verified by querying the Amazon Managed Service for Prometheus Alert Manager endpoint with ListAlerts API using awscurl command. A sample request is shown below.</p> <pre><code>awscurl https://aps-workspaces.us-east-1.amazonaws.com/workspaces/$WORKSPACE_ID/alertmanager/api/v2/alerts --service=\"aps\" -H \"Content-Type: application/json\"\n</code></pre>"},{"location":"guides/operational/alerting/amp-alertmgr/#amazon-managed-service-for-prometheus-alert-manager-rules-in-amazon-managed-grafana","title":"Amazon Managed Service for Prometheus Alert Manager rules in Amazon Managed Grafana","text":"<p>Amazon Managed Grafana (AMG) alerting feature allows customers to gain visibility into Amazon Managed Service for Prometheus Alert Manager alerts from their Amazon Managed Grafana workspace. Customers using the Amazon Managed Service for Prometheus workspaces to collect Prometheus metrics utilize the fully managed Alert Manager and Ruler features in the service to configure alerting and recording rules. With this feature, they can visualize all their alert and recording rules configured in their Amazon Managed Service for Prometheus workspace. Prometheus alerts view can be in Amazon Managed Grafana (AMG) console by checking the Grafana alerting checkbox in the Workspace configuration options tab. Once enabled, this will also migrate native Grafana alerts that were previously created in Grafana dashboards into a new Alerting page in the Grafana workspace.</p> <p>Reference: Announcing Prometheus Alert manager rules in Amazon Managed Grafana</p> <p></p>"},{"location":"guides/operational/alerting/amp-alertmgr/#recommended-alerts-for-a-baseline-monitoring","title":"Recommended alerts for a baseline monitoring","text":"<p>Alerting is a key aspect of robust monitoring and observability best practices. The alerting mechanism should strike a balance between alert fatigue and missing critical alerts. Here are some of the alerts that are recommended to improve the overall reliability of the workloads. Various teams in the organization look at monitoring their infrastructure and workloads from different perspectives and hence this could be expanded or changed based on the requirement and scenario &amp; certainly this is not a comprehensive list.</p> <ul> <li>Container Node is using more than certain (ex. 80%) allocated memory limit.</li> <li>Container Node is using more than certain (ex. 80%) allocated CPU limit.</li> <li>Container Node is using more than certain (ex. 90%) allocated disk space.</li> <li>Container in pod in namespace is using more than certain (ex. 80%) allocated CPU limit.</li> <li>Container in pod in namespace is using more than certain (ex. 80%) of memory limit.</li> <li>Container in pod in namespace had too many restarts.</li> <li>Persistent Volume in a namespace is using more than certain (max 75%) disk space.</li> <li>Deployment is currently having no active pods running</li> <li>Horizontal Pod Autoscaler (HPA) in namespace is running at max capacity</li> </ul> <p>The essential thing in setting up alerts for the above or any similar scenario will require the expression to be changed as needed. For example,</p> <pre><code>expr: |\n        ((sum(irate(container_cpu_usage_seconds_total{image!=\"\",container!=\"POD\", namespace!=\"kube-sys\"}[30s])) by (namespace,container,pod) /\nsum(container_spec_cpu_quota{image!=\"\",container!=\"POD\", namespace!=\"kube-sys\"} /\ncontainer_spec_cpu_period{image!=\"\",container!=\"POD\", namespace!=\"kube-sys\"}) by (namespace,container,pod) ) * 100)  &gt; 80\n      for: 5m\n</code></pre>"},{"location":"guides/operational/alerting/amp-alertmgr/#ack-controller-for-amazon-managed-service-for-prometheus","title":"ACK Controller for Amazon Managed Service for Prometheus","text":"<p>Amazon Managed Service for Prometheus AWS Controller for Kubernetes (ACK) controller is available for Workspace, Alert Manager and Ruler resources which lets customers take advantage of Prometheus using custom resource definitions (CRDs) and native objects or services that provide supporting capabilities without having to define any resources outside of Kubernetes cluster. The ACK controller for Amazon Managed Service for Prometheus can be used to manage all resources directly from the Kubernetes cluster that you\u2019re monitoring, allowing Kubernetes to act as your \u2018source of truth\u2019 for your workload\u2019s desired state. ACK is a collection of Kubernetes CRDs and custom controllers working together to extend the Kubernetes API and manage AWS resources.</p> <p>A snippet of alerting rules configured using ACK is shown below:</p> <pre><code>apiVersion: prometheusservice.services.k8s.aws/v1alpha1\nkind: RuleGroupsNamespace\nmetadata:\n  name: default-rule\nspec:\n  workspaceID: WORKSPACE-ID\n  name: default-rule\n  configuration: |\n    groups:\n    - name: example\n      rules:\n      - alert: HostHighCpuLoad\n        expr: 100 - (avg(rate(node_cpu_seconds_total{mode=\"idle\"}[2m])) * 100) &gt; 60\n        for: 5m\n        labels:\n          severity: warning\n          event_type: scale_up\n        annotations:\n          summary: Host high CPU load (instance {{ $labels.instance }})\n          description: \"CPU load is &gt; 60%\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: HostLowCpuLoad\n        expr: 100 - (avg(rate(node_cpu_seconds_total{mode=\"idle\"}[2m])) * 100) &lt; 30\n        for: 5m\n        labels:\n          severity: warning\n          event_type: scale_down\n        annotations:\n          summary: Host low CPU load (instance {{ $labels.instance }})\n          description: \"CPU load is &lt; 30%\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n</code></pre>"},{"location":"guides/operational/alerting/amp-alertmgr/#restricting-access-to-rules-using-iam-policy","title":"Restricting access to rules using IAM policy","text":"<p>Organizations require various teams to have their own rules to be created &amp; administered for their recording and alerting requirements. Rules management in Amazon Managed Service for Prometheus allows rules to be access controlled using AWS Identity and Access Management (IAM) policy so that each team can control their own set of rules &amp; alerts grouped by rulegroupnamespaces.</p> <p>The below image shows two example rulegroupnamespaces called devops and engg added into Rules management of Amazon Managed Service for Prometheus.</p> <p></p> <p>The below JSON is a sample IAM policy which restricts access to the devops rulegroupnamespace (shown above) with the Resource ARN specified. The notable actions in the below IAM policy are PutRuleGroupsNamespace and DeleteRuleGroupsNamespace which are restricted to the specified Resource ARN of the rulegroupsnamespace of AMP workspace. Once the policy is created, it can be assigned to any required user, group or role for desired access control requirement. The Action in the IAM policy can be modified/restricted as required based on IAM permissions for required &amp; allowable actions.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"VisualEditor0\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"aps:RemoteWrite\",\n        \"aps:DescribeRuleGroupsNamespace\",\n        \"aps:PutRuleGroupsNamespace\",\n        \"aps:DeleteRuleGroupsNamespace\"\n      ],\n      \"Resource\": [\n        \"arn:aws:aps:us-west-2:XXXXXXXXXXXX:workspace/ws-8da31ad6-f09d-44ff-93a3-xxxxxxxxxx\",\n        \"arn:aws:aps:us-west-2:XXXXXXXXXXXX:rulegroupsnamespace/ws-8da31ad6-f09d-44ff-93a3-xxxxxxxxxx/devops\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>The below awscli interaction shows an example of an IAM user having restricted access to a rulegroupsnamespace specified through Resource ARN (i.e. devops rulegroupnamespace) in IAM policy and how the same user is denied access to other resources (i.e. engg rulegroupnamespace) not having access.</p> <pre><code>$ aws amp describe-rule-groups-namespace --workspace-id ws-8da31ad6-f09d-44ff-93a3-xxxxxxxxxx --name devops\n{\n    \"ruleGroupsNamespace\": {\n        \"arn\": \"arn:aws:aps:us-west-2:XXXXXXXXXXXX:rulegroupsnamespace/ws-8da31ad6-f09d-44ff-93a3-xxxxxxxxxx/devops\",\n        \"createdAt\": \"2023-04-28T01:50:15.408000+00:00\",\n        \"data\": \"Z3JvdXBzOgogIC0gbmFtZTogZGV2b3BzX3VwZGF0ZWQKICAgIHJ1bGVzOgogICAgLSByZWNvcmQ6IG1ldHJpYzpob3N0X2NwdV91dGlsCiAgICAgIGV4cHI6IGF2ZyhyYXRlKGNvbnRhaW5lcl9jcHVfdXNhZ2Vfc2Vjb25kc190b3RhbFsybV0pKQogICAgLSBhbGVydDogaGlnaF9ob3N0X2NwdV91c2FnZQogICAgICBleHByOiBhdmcocmF0ZShjb250YWluZXJfY3B1X3VzYWdlX3NlY29uZHNfdG90YWxbNW1dKSkKICAgICAgZm9yOiA1bQogICAgICBsYWJlbHM6CiAgICAgICAgICAgIHNldmVyaXR5OiBjcml0aWNhbAogIC0gbmFtZTogZGV2b3BzCiAgICBydWxlczoKICAgIC0gcmVjb3JkOiBjb250YWluZXJfbWVtX3V0aWwKICAgICAgZXhwcjogYXZnKHJhdGUoY29udGFpbmVyX21lbV91c2FnZV9ieXRlc190b3RhbFs1bV0pKQogICAgLSBhbGVydDogY29udGFpbmVyX2hvc3RfbWVtX3VzYWdlCiAgICAgIGV4cHI6IGF2ZyhyYXRlKGNvbnRhaW5lcl9tZW1fdXNhZ2VfYnl0ZXNfdG90YWxbNW1dKSkKICAgICAgZm9yOiA1bQogICAgICBsYWJlbHM6CiAgICAgICAgc2V2ZXJpdHk6IGNyaXRpY2FsCg==\",\n        \"modifiedAt\": \"2023-05-01T17:47:06.409000+00:00\",\n        \"name\": \"devops\",\n        \"status\": {\n            \"statusCode\": \"ACTIVE\",\n            \"statusReason\": \"\"\n        },\n        \"tags\": {}\n    }\n}\n\n\n$ cat &gt; devops.yaml &lt;&lt;EOF\n&gt; groups:\n&gt;  - name: devops_new\n&gt;    rules:\n&gt;   - record: metric:host_cpu_util\n&gt;     expr: avg(rate(container_cpu_usage_seconds_total[2m]))\n&gt;   - alert: high_host_cpu_usage\n&gt;     expr: avg(rate(container_cpu_usage_seconds_total[5m]))\n&gt;     for: 5m\n&gt;     labels:\n&gt;            severity: critical\n&gt;  - name: devops\n&gt;    rules:\n&gt;    - record: container_mem_util\n&gt;      expr: avg(rate(container_mem_usage_bytes_total[5m]))\n&gt;    - alert: container_host_mem_usage\n&gt;      expr: avg(rate(container_mem_usage_bytes_total[5m]))\n&gt;      for: 5m\n&gt;      labels:\n&gt;        severity: critical\n&gt; EOF\n\n\n$ base64 devops.yaml &gt; devops_b64.yaml\n\n\n$ aws amp put-rule-groups-namespace --workspace-id ws-8da31ad6-f09d-44ff-93a3-xxxxxxxxxx --name devops --data file://devops_b64.yaml\n{\n    \"arn\": \"arn:aws:aps:us-west-2:XXXXXXXXXXXX:rulegroupsnamespace/ws-8da31ad6-f09d-44ff-93a3-xxxxxxxxxx/devops\",\n    \"name\": \"devops\",\n    \"status\": {\n        \"statusCode\": \"UPDATING\"\n    },\n    \"tags\": {}\n}\n</code></pre> <p><code>$ aws amp describe-rule-groups-namespace --workspace-id ws-8da31ad6-f09d-44ff-93a3-xxxxxxxxxx --name engg An error occurred (AccessDeniedException) when calling the DescribeRuleGroupsNamespace operation: User: arn:aws:iam::XXXXXXXXXXXX:user/amp_ws_user is not authorized to perform: aps:DescribeRuleGroupsNamespace on resource: arn:aws:aps:us-west-2:XXXXXXXXXXXX:rulegroupsnamespace/ws-8da31ad6-f09d-44ff-93a3-xxxxxxxxxx/engg</code></p> <p><code>$ aws amp put-rule-groups-namespace --workspace-id ws-8da31ad6-f09d-44ff-93a3-xxxxxxxxxx --name engg --data file://devops_b64.yaml An error occurred (AccessDeniedException) when calling the PutRuleGroupsNamespace operation: User: arn:aws:iam::XXXXXXXXXXXX:user/amp_ws_user is not authorized to perform: aps:PutRuleGroupsNamespace on resource: arn:aws:aps:us-west-2:XXXXXXXXXXXX:rulegroupsnamespace/ws-8da31ad6-f09d-44ff-93a3-xxxxxxxxxx/engg</code></p> <p><code>$ aws amp delete-rule-groups-namespace --workspace-id ws-8da31ad6-f09d-44ff-93a3-xxxxxxxxxx --name engg An error occurred (AccessDeniedException) when calling the DeleteRuleGroupsNamespace operation: User: arn:aws:iam::XXXXXXXXXXXX:user/amp_ws_user is not authorized to perform: aps:DeleteRuleGroupsNamespace on resource: arn:aws:aps:us-west-2:XXXXXXXXXXXX:rulegroupsnamespace/ws-8da31ad6-f09d-44ff-93a3-xxxxxxxxxx/engg</code></p> <p>The user permissions to use rules can also be restricted using an IAM policy (documentation sample).</p> <p>For more information customers can read the AWS Documentation, go through the AWS Observability Workshop on Amazon Managed Service for Prometheus Alert Manager.</p> <p>Additional Reference: Amazon Managed Service for Prometheus Is Now Generally Available with Alert Manager and Ruler</p>"},{"location":"guides/operational/business/key-performance-indicators/","title":"Key Performance Indicators","text":""},{"location":"guides/operational/business/key-performance-indicators/#10-understanding-kpis-golden-signals","title":"1.0 Understanding KPIs (\"Golden Signals\")","text":"<p>Organizations utilize key performance indicators (KPIs) a.k.a 'Golden Signals' that provide insight into the health or risk of the business and operations. Different parts of an organization would have unique KPIs that cater to measurement of their respective outcomes. For example, the product team of an eCommerce application would track the ability to process cart orders successfully as its KPI. An on-call operations team would measure their KPI as mean-time to detect (MTTD) an incident. For the financial team a KPI for cost of resources under budget is important. </p> <p>Service Level Indicators (SLIs), Service Level Objectives (SLOs), and Service Level Agreements (SLAs) are essential components of service reliability management. This guide outlines best practices for using Amazon CloudWatch and its features to calculate and monitor SLIs, SLOs, and SLAs, with clear and concise examples.</p> <ul> <li>SLI (Service Level Indicator): A quantitative measure of a service's performance.</li> <li>SLO (Service Level Objective): The target value for an SLI, representing the desired performance level.</li> <li>SLA (Service Level Agreement): A contract between a service provider and its users specifying the expected level of service.</li> </ul> <p>Examples of common SLIs:</p> <ul> <li>Availability: Percentage of time a service is operational</li> <li>Latency: Time taken to fulfill a request</li> <li>Error rate: Percentage of failed requests</li> </ul>"},{"location":"guides/operational/business/key-performance-indicators/#20-discover-customer-and-stakeholder-requirements-using-template-as-suggested-below","title":"2.0 Discover customer and stakeholder requirements (using template as suggested below)","text":"<ol> <li>Start with the top question: \u201cWhat is the business value or business problem in scope for the given workload (ex. Payment portal, eCommerce order placement, User registration, Data reports, Support portal etc)</li> <li>Break down the business value into categories such as User-Experience (UX); Business-Experience (BX); Operational-Experience (OpsX); Security-Experience(SecX); Developer-Experience (DevX)</li> <li>Derive core signals aka \u201cGolden Signals\u201d for each category; the top signals around UX &amp; BX will typically construe the business metrics</li> </ol> ID Initials Customer Business Needs Measurements Information Sources What does good look like? Alerts Dashboards Reports M1 Example External End User User Experience Response time (Page latency) Logs / Traces &lt; 5s for 99.9% No Yes No M2 Example Business Availability Successful RPS (Requests per second) Health Check &gt;85% in 5 min window Yes Yes Yes M3 Example Security Compliance Critical non-compliant resources Config data &lt;10 under 15 days No Yes Yes M4 Example Developers Agility Deployment time Deployment logs Always &lt; 10 min Yes No Yes M5 Example Operators Capacity Queue Depth App logs/metrics Always &lt; 10 Yes Yes Yes"},{"location":"guides/operational/business/key-performance-indicators/#21-golden-signals","title":"2.1 Golden Signals","text":"Category Signal Notes References UX Performance (Latency) See M1 in template Whitepaper: Availability and Beyond (Measuring latency) BX Availability See M2 in template Whitepaper: Avaiability and Beyond (Measuring availability) BX Business Continuity Plan (BCP) Amazon Resilience Hub (ARH) resilience score against defined RTO/RPO Docs: ARH user guide (Understanding resilience scores) SecX (Non)-Compliance See M3 in template Docs: AWS Control Tower user guide (Compliance status in the console) DevX Agility See M4 in template Docs: DevOps Monitoring Dashboard on AWS (DevOps metrics list) OpsX Capacity (Quotas) See M5 in template Docs: Amazon CloudWatch user guide (Visualizing your service quotas and setting alarms) OpsX Budget Anomalies Docs: 1. AWS Billing and Cost Management (AWS Cost Anomaly Detection)  2. AWS Budgets"},{"location":"guides/operational/business/key-performance-indicators/#30-top-level-guidance-tlg","title":"3.0 Top Level Guidance \u2018TLG\u2019","text":""},{"location":"guides/operational/business/key-performance-indicators/#31-tlg-general","title":"3.1 TLG General","text":"<ol> <li> <p>Work with business, architecture and security teams to help refine the business, compliance and governance requirements and ensure they accurately reflect the business needs. This includes establishing recovery-time and recovery-point targets (RTOs, RPOs). Formulate methods to measure requirements such as measuring availability and latency (ex. Uptime could allow a small percentage of faults over a 5 min window).</p> </li> <li> <p>Build an effective tagging strategy with purpose built schema that aligns to various business functional outcomes. This should especially cover operational observability and incident management.</p> </li> <li> <p>Where possible leverage dynamic thresholds for alarms (esp. for metrics that do not have baseline KPIs) using CloudWatch anomaly detection which provides machine learning algorithms to establish the baselines. When utilizing  AWS available services that publish CW metrics (or other sources like prometheus metrics) to configure alarms consider creating composite alarms to reduce alarm noise. Example: a composite alarm that comprises of a business metric indicative of availability (tracked by successful requests) and latency when configured to alarm when both drop below a critical threshold during deployments could be deterministic indicator of deployment bug.</p> </li> <li> <p>(NOTE: Requires AWS Business support or higher) AWS publishes events of interest using AWS Health service related to your resources in Personal Health Dashboard. Leverage AWS Health Aware (AHA) framework (that uses AWS Health) to ingest proactive and real-time alerts aggregated across your AWS Organization from a central account (such as a management account). These alerts can be sent to preferred communication platforms such as Slack and integrates with ITSM tools like ServiceNow and Jira. </p> </li> <li> <p>Leverage Amazon CloudWatch Application Insights to setup best monitors for resources and continuously analyze data for signs of problems with your applications. It also provides automated dashboards that show potential problems with monitored applications to quickly isolate/troubleshoot application/infrastructure issues. Leverage Container Insights to aggregate metrics and logs from containers and can be integrated seamlessly with CloudWatch Application Insights. </p> </li> <li> <p>Leverage AWS Resilience Hub to analyze applications against defined RTOs and RPOs. Validate if the availability, latency and business continuity requirements are met by using controlled experiments using tools like AWS Fault Injection Simulator. Conduct additional Well-Architected reviews and service specific deep-dives to ensure workloads are designed to meet business requirements following AWS best practices.</p> </li> <li> <p>For further details refer to other sections of AWS Observability Best Practices guidance, AWS Cloud Adoption Framework: Operations Perspective whitepaper and AWS Well-Architected Framework Operational Excellence Pillar whitepaper content on 'Understading workload health'.</p> </li> </ol>"},{"location":"guides/operational/business/key-performance-indicators/#32-tlg-by-domain-emphasis-on-business-metrics-ie-ux-bx","title":"3.2 TLG by Domain (emphasis on business metrics i.e. UX, BX)","text":"<p>Suitable examples are provided below using services such as CloudWatch (CW) (Ref: AWS Services that publish CloudWatch metrics documentation)</p>"},{"location":"guides/operational/business/key-performance-indicators/#321-canaries-aka-synthetic-transactions-and-real-user-monitoring-rum","title":"3.2.1 Canaries (aka Synthetic transactions) and Real-User Monitoring (RUM)","text":"<ul> <li>TLG: One of the easiest and most effective ways to understand client/customer experience is to simulate customer traffic with Canaries (Synthetic transactions) which regularly probes your services and records metrics.</li> </ul> AWS Service Feature Measurement Metric Example Notes CW Synthetics Availability SuccessPercent (Ex. SuccessPercent &gt; 90 or CW Anomaly Detection for 1min Period)**[Metric Math where m1 is SuccessPercent if Canaries run each weekday 7a-8a (CloudWatchSynthetics): ** <code>IF(((DAY(m1)&lt;6) AND (HOUR(m1)&gt;7 AND HOUR(m1)&lt;8)),m1)]</code> CW Synthetics Availability VisualMonitoringSuccessPercent (Ex. VisualMonitoringSuccessPercent &gt; 90 for 5 min Period for UI screenshot comparisons)**[Metric Math where m1 is SuccessPercent if Canaries run each weekday 7a-8a (CloudWatchSynthetics): ** <code>IF(((DAY(m1)&lt;6) AND (HOUR(m1)&gt;7 AND HOUR(m1)&lt;8)),m1)</code> If customer expects canary to match predetermined UI screenshot CW RUM Response Time Apdex Score (Ex. Apdex score:  NavigationFrustratedCount &lt; \u2018N\u2019 expected value)"},{"location":"guides/operational/business/key-performance-indicators/#322-api-frontend","title":"3.2.2 API Frontend","text":"AWS Service Feature Measurement Metric Example Notes CloudFront Availability Total error rate (Ex. [Total error rate] &lt; 10 or CW Anomaly Detection for 1min Period) Availability as a measure of error rate CloudFront (Requires turning on additional metrics) Peformance Cache hit rate (Ex.Cache hit rate &lt; 10 CW Anomaly Detection for 1min Period) Route53 Health checks (Cross region) Availability HealthCheckPercentageHealthy (Ex. [Minimum of HealthCheckPercentageHealthy] &gt; 90 or CW Anomaly Detection for 1min Period) Route53 Health checks Latency TimeToFirstByte (Ex. [p99 TimeToFirstByte] &lt; 100 ms or CW Anomaly Detection for 1min Period) API Gateway Availability Count (Ex. [(4XXError + 5XXError) / Count) * 100] &lt; 10 or CW Anomaly Detection for 1min Period) Availability as a measure of \"abandoned\" requests API Gateway Latency Latency (or IntegrationLatency i.e. backend latency) (Ex. p99 Latency &lt; 1 sec or CW Anomaly Detection for 1min Period) p99 will have greater tolerance than lower percentile like p90. (p50 is same as average) API Gateway Performance CacheHitCount (and Misses) (Ex. [CacheMissCount / (CacheHitCount + CacheMissCount)  * 100] &lt; 10 or CW Anomaly Detection for 1min Period) Performance as a measure of Cache (Misses) Application Load Balancer (ALB) Availability RejectedConnectionCount (Ex.[RejectedConnectionCount/(RejectedConnectionCount + RequestCount) * 100] &lt; 10 CW Anomaly Detection for 1min Period) Availability as a measure of rejected requests due to max connections breached Application Load Balancer (ALB) Latency TargetResponseTime (Ex. p99 TargetResponseTime &lt; 1 sec or CW Anomaly Detection for 1min Period) p99 will have greater tolerance than lower percentile like p90. (p50 is same as average)"},{"location":"guides/operational/business/key-performance-indicators/#323-serverless","title":"3.2.3 Serverless","text":"AWS Service Feature Measurement Metric Example Notes S3 Request metrics Availability AllRequests (Ex. [(4XXErrors + 5XXErrors) / AllRequests) * 100] &lt; 10 or CW Anomaly Detection for 1min Period) Availability as a measure of \"abandoned\" requests S3 Request metrics (Overall) Latency TotalRequestLatency (Ex. [p99 TotalRequestLatency] &lt; 100 ms or CW Anomaly Detection for 1min Period) DynamoDB (DDB) Availability ThrottledRequests (Ex. [ThrottledRequests] &lt; 100 or CW Anomaly Detection for 1min Period) Availability as a measure of \"throttled\" requests DynamoDB (DDB) Latency SuccessfulRequestLatency (Ex. [p99 SuccessfulRequestLatency] &lt; 100 ms or CW Anomaly Detection for 1min Period) Step Functions Availability ExecutionsFailed (Ex. ExecutionsFailed = 0)**[ex. Metric Math where m1 is ExecutionsFailed (Step function Execution) UTC time: <code>IF(((DAY(m1)&lt;6 OR ** ** DAY(m1)==7) AND (HOUR(m1)&gt;21 AND HOUR(m1)&lt;7)),m1)]</code> Assuming business flow that requests completion of step functions as a daily operation 9p-7a during weekdays (start of day business operations)"},{"location":"guides/operational/business/key-performance-indicators/#324-compute-and-containers","title":"3.2.4 Compute and Containers","text":"AWS Service Feature Measurement Metric Example Notes EKS Prometheus metrics Availability APIServer Request Success Ratio (ex. Prometheus metric like  APIServer Request Success Ratio) See best practices for monitoring EKS control plane metrics and EKS observability for details. EKS Prometheus metrics Performance apiserver_request_duration_seconds, etcd_request_duration_seconds apiserver_request_duration_seconds, etcd_request_duration_seconds ECS Availability Service RUNNING task count Service RUNNING task count See ECS CW metrics documentation ECS Performance TargetResponseTime (ex.  [p99 TargetResponseTime] &lt; 100 ms or CW Anomaly Detection for 1min Period) See ECS CW metrics documentation EC2 (.NET Core) CW Agent Performance Counters Availability (ex. ASP.NET Application Errors Total/Sec &lt; 'N') (ex. ASP.NET Application Errors Total/Sec &lt; 'N') See EC2 CW Application Insights documentation"},{"location":"guides/operational/business/key-performance-indicators/#325-databases-rds","title":"3.2.5 Databases (RDS)","text":"AWS Service Feature Measurement Metric Example Notes RDS Aurora Performance Insights (PI) Availability Average active sessions (Ex. Average active serssions with CW Anomaly Detection for 1min Period) See RDS Aurora CW PI documentation RDS Aurora Disaster Recovery (DR) AuroraGlobalDBRPOLag (Ex. AuroraGlobalDBRPOLag &lt; 30000 ms for 1min Period) See RDS Aurora CW documentation RDS Aurora Performance Commit Latency, Buffer Cache Hit Ratio, DDL Latency, DML Latency (Ex. Commit Latency with CW Anomaly Detection for 1min Period) See RDS Aurora CW PI documentation RDS (MSSQL) PI Performance SQL Compilations (Ex. SQL Compliations &gt; 'M' for 5 min Period) See RDS CW PI documentation"},{"location":"guides/operational/business/key-performance-indicators/#40-using-amazon-cloudwatch-and-metric-math-for-calculating-slis-slos-and-slas","title":"4.0 Using Amazon CloudWatch and Metric Math for Calculating SLIs, SLOs, and SLAs","text":""},{"location":"guides/operational/business/key-performance-indicators/#41-amazon-cloudwatch-and-metric-math","title":"4.1 Amazon CloudWatch and Metric Math","text":"<p>Amazon CloudWatch provides monitoring and observability services for AWS resources. Metric Math allows you to perform calculations using CloudWatch metric data, making it an ideal tool for calculating SLIs, SLOs, and SLAs.</p>"},{"location":"guides/operational/business/key-performance-indicators/#411-enabling-detailed-monitoring","title":"4.1.1 Enabling Detailed Monitoring","text":"<p>Enable Detailed Monitoring for your AWS resources to get 1-minute data granularity, allowing for more accurate SLI calculations.</p>"},{"location":"guides/operational/business/key-performance-indicators/#412-organizing-metrics-with-namespaces-and-dimensions","title":"4.1.2 Organizing Metrics with Namespaces and Dimensions","text":"<p>Use Namespaces and Dimensions to categorize and filter metrics for easier analysis. For example, use Namespaces to group metrics related to a specific service, and Dimensions to differentiate between various instances of that service.</p>"},{"location":"guides/operational/business/key-performance-indicators/#42-calculating-slis-with-metric-math","title":"4.2 Calculating SLIs with Metric Math","text":""},{"location":"guides/operational/business/key-performance-indicators/#421-availability","title":"4.2.1 Availability","text":"<p>To calculate availability, divide the number of successful requests by the total number of requests:</p> <pre><code>availability = 100 * (successful_requests / total_requests)\n</code></pre> <p>Example:</p> <p>Suppose you have an API Gateway with the following metrics: - <code>4XXError</code>: Number of 4xx client errors - <code>5XXError</code>: Number of 5xx server errors - <code>Count</code>: Total number of requests</p> <p>Use Metric Math to calculate the availability:</p> <pre><code>availability = 100 * ((Count - 4XXError - 5XXError) / Count)\n</code></pre>"},{"location":"guides/operational/business/key-performance-indicators/#422-latency","title":"4.2.2 Latency","text":"<p>To calculate average latency, use the <code>SampleCount</code> and <code>Sum</code> statistics provided by CloudWatch:</p> <pre><code>average_latency = Sum / SampleCount\n</code></pre> <p>Example:</p> <p>Suppose you have a Lambda function with the following metric: - <code>Duration</code>: Time taken to execute the function</p> <p>Use Metric Math to calculate the average latency:</p> <pre><code>average_latency = Duration.Sum / Duration.SampleCount\n</code></pre>"},{"location":"guides/operational/business/key-performance-indicators/#423-error-rate","title":"4.2.3 Error Rate","text":"<p>To calculate the error rate, divide the number of failed requests by the total number of requests:</p> <pre><code>error_rate = 100 * (failed_requests / total_requests)\n</code></pre> <p>Example:</p> <p>Using the API Gateway example from before:</p> <pre><code>error_rate = 100 * ((4XXError + 5XXError) / Count)\n</code></pre>"},{"location":"guides/operational/business/key-performance-indicators/#44-defining-and-monitoring-slos","title":"4.4 Defining and Monitoring SLOs","text":""},{"location":"guides/operational/business/key-performance-indicators/#441-setting-realistic-targets","title":"4.4.1 Setting Realistic Targets","text":"<p>Define SLO targets based on user expectations and historical performance data. Set achievable targets to ensure a balance between service reliability and resource utilization.</p>"},{"location":"guides/operational/business/key-performance-indicators/#442-monitoring-slos-with-cloudwatch","title":"4.4.2 Monitoring SLOs with CloudWatch","text":"<p>Create CloudWatch Alarms to monitor your SLIs and notify you when they approach or breach SLO targets. This enables you to proactively address issues and maintain service reliability.</p>"},{"location":"guides/operational/business/key-performance-indicators/#443-reviewing-and-adjusting-slos","title":"4.4.3 Reviewing and Adjusting SLOs","text":"<p>Periodically review your SLOs to ensure they remain relevant as your service evolves. Adjust targets if necessary and communicate any changes to stakeholders.</p>"},{"location":"guides/operational/business/key-performance-indicators/#45-defining-and-measuring-slas","title":"4.5 Defining and Measuring SLAs","text":""},{"location":"guides/operational/business/key-performance-indicators/#451-setting-realistic-targets","title":"4.5.1 Setting Realistic Targets","text":"<p>Define SLA targets based on historical performance data and user expectations. Set achievable targets to ensure a balance between service reliability and resource utilization.</p>"},{"location":"guides/operational/business/key-performance-indicators/#452-monitoring-and-alerting","title":"4.5.2 Monitoring and Alerting","text":"<p>Set up CloudWatch Alarms to monitor SLIs and notify you when they approach or breach SLA targets. This enables you to proactively address issues and maintain service reliability.</p>"},{"location":"guides/operational/business/key-performance-indicators/#453-regularly-reviewing-slas","title":"4.5.3 Regularly Reviewing SLAs","text":"<p>Periodically review SLAs to ensure they remain relevant as your service evolves. Adjust targets if necessary and communicate any changes to stakeholders.</p>"},{"location":"guides/operational/business/key-performance-indicators/#46-measuring-sla-or-slo-performance-over-a-set-period","title":"4.6 Measuring SLA or SLO Performance Over a Set Period","text":"<p>To measure SLA or SLO performance over a set period, such as a calendar month, use CloudWatch metric data with custom time ranges.</p> <p>Example:</p> <p>Suppose you have an API Gateway with an SLO target of 99.9% availability. To measure the availability for the month of April, use the following Metric Math expression:</p> <pre><code>availability = 100 * ((Count - 4XXError - 5XXError) / Count)\n</code></pre> <p>Then, configure the CloudWatch metric data query with a custom time range:</p> <ul> <li>Start Time: <code>2023-04-01T00:00:00Z</code></li> <li>End Time: <code>2023-04-30T23:59:59Z</code></li> <li>Period: <code>2592000</code> (30 days in seconds)</li> </ul> <p>Finally, use the <code>AVG</code> statistic to calculate the average availability over the month. If the average availability is equal to or greater than the SLO target, you have met your objective.</p>"},{"location":"guides/operational/business/key-performance-indicators/#50-summary","title":"5.0 Summary","text":"<p>Key Performance Indicators (KPIs) a.k.a 'Golden Signals' must align to business and stake-holder requirements. Calculating SLIs, SLOs, and SLAs using Amazon CloudWatch and Metric Math is crucial for managing service reliability. Follow the best practices outlined in this guide to effectively monitor and maintain the performance of your AWS resources. Remember to enable Detailed Monitoring, organize metrics with Namespaces and Dimensions, use Metric Math for SLI calculations, set realistic SLO and SLA targets, and establish monitoring and alerting systems with CloudWatch Alarms. By applying these best practices, you can ensure optimal service reliability, better resource utilization, and improved customer satisfaction.</p>"},{"location":"guides/operational/business/monitoring-for-business-outcomes/","title":"Why should you do observability?","text":"<p>See Developing an Observability Strategy on YouTube</p>"},{"location":"guides/operational/business/monitoring-for-business-outcomes/#what-really-matters","title":"What really matters?","text":"<p>Everything that you do at work should align to your organization's mission. All of us that are employed work to fulfill our organization's mission and towards its vision. At Amazon, our mission states that:</p> <p>Amazon strives to be Earth\u2019s most customer-centric company, Earth\u2019s best employer, and Earth\u2019s safest place to work.</p> <p>\u2014 About Amazon</p> <p>In IT, every project, deployment, security measure or optimization should work towards a business outcome. It seems obvious, but you should not do anything that does not add value to the business. As ITIL puts it:</p> <p>Every change should deliver business value.</p> <p>\u2014 ITIL Service Transition, AXELOS, 2011, page 44. \u2014 See Change Management in the Cloud AWS Whitepaper</p> <p>Mission and business value are important because they should inform everything that you do. There are many benefits to observability, these include:</p> <ul> <li>Better availability</li> <li>More reliability</li> <li>Understanding of application health and performance</li> <li>Better collaboration</li> <li>Proactive detection of issues</li> <li>Increase customer satisfaction</li> <li>Reduce time to market</li> <li>Reduce operational costs</li> <li>Automation</li> </ul> <p>All of these benefits have one thing in common, they all deliver business value, either directly to the customer or indrectly to the organization. When thinking about observability, everything should come back to thinking about whether or not your application is delivering business value.</p> <p>This means that observability should be measuring things that contribute towards delivering business value, focusing on business outcomes and when they are at risk: you should think about what your customers want and what they need.</p>"},{"location":"guides/operational/business/monitoring-for-business-outcomes/#where-do-i-start","title":"Where do I start?","text":"<p>Now that you know what matters, you need to think about what you need to measure. At Amazon, we start with the customer and work backwards from their needs:</p> <p>We are internally driven to improve our services, adding benefits and features, before we have to. We lower prices and increase value for customers before we have to. We invent before we have to.</p> <p>\u2014 Jeff Bezos, 2012 Shareholder Letter</p> <p>Let's take a simple example, using an e-commerce site. First, think about what you want as a customer when you are buying products online, it may not be the same for everyone, but you probably care about things like:</p> <ul> <li>Delivery</li> <li>Price</li> <li>Security</li> <li>Page Speed</li> <li>Search (can you find the product you are looking for?)</li> </ul> <p>Once you know what your customers care about, you can start to measure them and how they affect your business outcomes. Page speed directly impacts your conversion rate and search engine ranking. A 2017 study showed that more than half (53%) of mobile users abandon a page if it takes more than 3 seconds to load. There are of course, many studies that show the importance of page speed, and it is an obvious metric to measure, but you need to measure it and take action because it has a measureable impact on conversion and you can use that data to make improvements.</p>"},{"location":"guides/operational/business/monitoring-for-business-outcomes/#working-backwards","title":"Working backwards","text":"<p>You cannot be expected to know everything that you customers care about. If you are reading this, you are probably in a technical role. You need to talk to the stakeholders in your organisation, this isn't always easy, but it is vital to ensuring that you are measuring what's important. </p> <p>Let's continue with the e-commerce example. This time, consider search: it may be obvious that customers need to be able to search for a product in order to buy it, but did you know that according to a Forrester Research report, 43% of visitors navigate immediately to the search box and searches are 2-3 times more likely to convert compared to non-searchers. Search is really important, it has to work well and you need to monitor it - maybe you discover that particular searches are yeilding no results and that you need to move from naive pattern matching to natural language processing. This is an example of monitoring for a business outcome and then acting to improve the customer experience.</p> <p>At Amazon:</p> <p>We strive to deeply understand customers and work backwards from their pain points to rapidly develop innovations that create meaningful solutions in their lives.</p> <p>\u2014 Daniel Slater - Worldwide Lead, Culture of Innovation, AWS in Elements of Amazon\u2019s Day 1 Culture</p> <p>We start with the customer and work backwards from their needs. This isn't the only approach to success in business, but it is a good approach to observability. Work with stakeholders to understand what's important to your customers and then work backwards from there.</p> <p>As an added benefit, if you collect metrics that are important to your customers and stakeholders, you can visualize these in near real-time dashboards and avoid having to create reports or answer questions such as \"how long is it taking to load the landing page?\" or \"how much is it costing to run the website?\" - stakeholders and executives should be able to self serve this information.</p> <p>These are the kind of high level metrics that really matter for your application and they are also almost always the best indicator that there is an issue. For example: an alert indicating that there are fewer orders than you would normally expect in a given time period tells you that there is probably an issue that is impacting customers; an alert indicating that a volume on a server is nearly full or that you have a high number of 5xx errors for a particular service may be something that requires fixing, but you still have to understand customer impact and then prioritize accordingly - this can take time.</p> <p>Issues that impact customers are easy to identify when you are measuring these high level business metrics. These metrics are the what is happening. Other metrics and other forms of observability such as tracing and logs are the why is this happening, which will lead you to what you can do to fix it or improve it.</p>"},{"location":"guides/operational/business/monitoring-for-business-outcomes/#what-to-observe","title":"What to observe","text":"<p>Now you have an idea of what matters to your customers, you can identify Key Performance Indicators (KPIs). These are your high level metrics that will tell you if business outcomes are at risk. You also need to gather information from many different sources that may impact those KPIs, this is where you need to start thinking about metrics that could impact those KPIs. As was discussed earlier, the number of 5xx errors, does not indicate impact, but it could have an effect on your KPIs. Work your way backwards from what will impact business outcomes to things that may impact business outcomes. </p> <p>Once you know what you need to collect, you need to identify the sources of information that will provide you with the metrics you can use to measure KPIs and related metrics that may impact those KPIs. This is the basis of what you observe.</p> <p>This data is likely to come from Metrics, Logs and Traces. Once you have this data, you can use it to alert when outcomes are at risk.</p> <p>You can then evaluate the impact and attempt to rectify the issue. Almost always, this data will tell you that there\u2019s a problem, before an isolated technical metric (such as cpu or memory) does.</p> <p>You can use observability reactively to fix an issue impacting business outcomes or you can use the data proactively to do something like improve your customer's search experience.</p>"},{"location":"guides/operational/business/monitoring-for-business-outcomes/#conclusion","title":"Conclusion","text":"<p>Whilst CPU, RAM, Disk Space and other technical metrics are important for scaling, performance, capacity and cost \u2013 they don\u2019t really tell you how your application is doing and don\u2019t give any insight in to customer experience.</p> <p>Your customers are what\u2019s important and it\u2019s their experience that you should be monitoring.</p> <p>That\u2019s why you should work backwards from your customers\u2019 requirements, working with your stakeholders and establish KPIs and metrics that matter.</p>"},{"location":"guides/operational/business/sla-percentile/","title":"Percentiles are important","text":"<p>Percentiles are important in monitoring and reporting because they provide a more detailed and accurate view of data distribution compared to just relying on averages. An average can sometimes hide important information, such as outliers or variations in the data, that can significantly impact performance and user experience. Percentiles, on the other hand, can reveal these hidden details and give a better understanding of how the data is distributed.</p> <p>In Amazon CloudWatch, percentiles can be used to monitor and report on various metrics, such as response times, latency, and error rates, across your applications and infrastructure. By setting up alarms on percentiles, you can get alerted when specific percentile values exceed thresholds, allowing you to take action before they impact more customers.</p> <p>To use percentiles in CloudWatch, choose your metric in All metrics in the CloudWatch console and use an existing metric and set the statistic to p99, you can then edit the value after the p to whichever percentile you would like. You can then view percentile graphs, add them to CloudWatch dashboards and set alarms on these metrics. For example, you could set an alarm to notify you when the 95th percentile of response times exceeds a certain threshold, indicating that a significant percentage of users are experiencing slow response times.</p> <p>The histogram below was created in Amazon Managed Grafana using a CloudWath Logs Insights query from CloudWatch RUM logs. The query used was:</p> <pre><code>fields @timestamp, event_details.duration\n| filter event_type = \"com.amazon.rum.performance_navigation_event\"\n| sort @timestamp desc\n</code></pre> <p>The histogram plots the page load time in milliseconds. With this view, it's possible to clearly see the outliers. This data is hidden if average is used.</p> <p></p> <p>The same data shown in CloudWatch using the average value indicates that pages are taking under two seconds to load. You can see from the histogram above, that most pages are actually taking less than a second and we have outliers.</p> <p></p> <p>Using the same data again with a percentile (p99) indicates that there is an issue, the CloudWatch graph now shows that 99 percent of page loads are taking less than 23 seconds.</p> <p></p> <p>To make this easier to visualize, the graphs below compare the average value to the 99th percentile. In this case, the target page load time is two seconds, it is possible to use alternative CloudWatch statistics and metric math to make other calculations. In this case Percentile rank (PR) is used with the statistic PR(:2000) to show that 92.7% of page loads are happening within the target of 2000ms.</p> <p></p> <p>Using percentiles in CloudWatch can help you gain deeper insights into your system's performance, detect issues early, and improve your customer's experience by identifying outliers that would otherwise be hidden.</p>"},{"location":"guides/operational/gitops-with-amg/gitops-with-amg/","title":"Using GitOps and Grafana Operator with Amazon Managed Grafana","text":""},{"location":"guides/operational/gitops-with-amg/gitops-with-amg/#how-to-use-this-guide","title":"How to use this guide","text":"<p>This Observability best practices guide is meant for developers and architects who want to understand how to use grafana-operator as a Kubernetes operator on your Amazon EKS cluster to create and manage the lifecycle of Grafana resources and Grafana dashboards in Amazon Managed Grafana in a Kubernetes native way.</p>"},{"location":"guides/operational/gitops-with-amg/gitops-with-amg/#introduction","title":"Introduction","text":"<p>Customers use Grafana as an observability platform for open source analytics and monitoring solution. We have seen customers running their workloads in Amazon EKS want to shift their focus towards workload gravity and rely on Kubernetes-native controllers to deploy and manage the lifecycle of external resources such as Cloud resources. We have seen customers installing AWS Controllers for Kubernetes (ACK) to create, deploy and manage AWS services. Many customers these days opt to offload the Prometheus and Grafana implementations to managed services and in case of AWS these services are Amazon Managed Service for Prometheus and Amazon Managed Grafana for monitoring their workloads.</p> <p>One common challenge customers face while using Grafana is, in creating and managing the lifecycle of Grafana resources and Grafana dashboards in external Grafana instances such as Amazon Managed Grafana from their Kubernetes cluster. Customers face  challenges in finding ways to completely automate and manage infrastructure and application deployment of their whole system using Git based workflows which also includes creating of Grafana resources in Amazon Managed Grafana. In this Observability best practices guide, we will focus on the following topics:</p> <ul> <li>Introduction on Grafana Operator - A Kubernetes operator to manage external Grafana instances from your Kubernetes cluster </li> <li>Introduction to GitOps - Automated mechanisms to create and manage your infrastructure using Git based workflows</li> <li>Using Grafana Operator on Amazon EKS to manage Amazon Managed Grafana</li> <li>Using GitOps with Flux on Amazon EKS to manage Amazon Managed Grafana</li> </ul>"},{"location":"guides/operational/gitops-with-amg/gitops-with-amg/#introduction-on-grafana-operator","title":"Introduction on Grafana Operator","text":"<p>The grafana-operator is a Kubernetes operator built to help you manage your Grafana instances inside Kubernetes. Grafana Operator makes it possible for you to manage and create Grafana dashboards, datasources etc. declaratively between multiple instances in an easy and scalable way. The Grafana operator now supports managing resources such as dashboards, datasources etc hosted on external environments like Amazon Managed Grafana. This ultimately enables us to use GitOps mechanisms using CNCF projects such as Flux to create and manage the lifecyle of resources in Amazon Managed Grafana from Amazon EKS cluster.</p>"},{"location":"guides/operational/gitops-with-amg/gitops-with-amg/#introduction-to-gitops","title":"Introduction to GitOps","text":""},{"location":"guides/operational/gitops-with-amg/gitops-with-amg/#what-is-gitops-and-flux","title":"What is GitOps and Flux","text":"<p>GitOps is a software development and operations methodology that uses Git as the source of truth for deployment configurations. It involves keeping the desired state of an application or infrastructure in a Git repository and using Git-based workflows to manage and deploy changes. GitOps is a way of managing application and infrastructure deployment so that the whole system is described declaratively in a Git repository. It is an operational model that offers you the ability to manage the state of multiple Kubernetes clusters leveraging the best practices of version control, immutable artifacts, and automation. </p> <p>Flux is a GitOps tool that automates the deployment of applications on Kubernetes. It works by continuously monitoring the state of a Git repository and applying any changes to a cluster. Flux integrates with various Git providers such as GitHub, GitLab, and Bitbucket. When changes are made to the repository, Flux automatically detects them and updates the cluster accordingly.</p>"},{"location":"guides/operational/gitops-with-amg/gitops-with-amg/#advantages-of-using-flux","title":"Advantages of using Flux","text":"<ul> <li>Automated deployments: Flux automates the deployment process, reducing manual errors and freeing up developers to focus on other tasks.</li> <li>Git-based workflow: Flux leverages Git as a source of truth, which makes it easier to track and revert changes.</li> <li>Declarative configuration: Flux uses Kubernetes manifests to define the desired state of a cluster, making it easier to manage and track changes.</li> </ul>"},{"location":"guides/operational/gitops-with-amg/gitops-with-amg/#challenges-in-adopting-flux","title":"Challenges in adopting Flux","text":"<ul> <li>Limited customization: Flux only supports a limited set of customizations, which may not be suitable for all use cases.</li> <li>Steep learning curve: Flux has a steep learning curve for new users and requires a deep understanding of Kubernetes and Git.</li> </ul>"},{"location":"guides/operational/gitops-with-amg/gitops-with-amg/#using-grafana-operator-on-amazon-eks-to-manage-resources-in-amazon-managed-grafana","title":"Using Grafana Operator on Amazon EKS to manage resources in Amazon Managed Grafana","text":"<p>As discussed in previous section, Grafana Operator enables us to use our Kubernetes cluster to create and manage the lifecyle of resources in Amazon Managed Grafana in a Kubernetes native way.  The below architecture diagram shows the demonstration of Kubernetes cluster as a control plane with using Grafana Operator to setup an identity with AMG, adding Amazon Managed Service for Prometheus as a data source and creating dashboards on Amazon Managed Grafana from Amazon EKS cluster in a Kubernetes native way.</p> <p></p> <p>Please refer to our post on Using Open Source Grafana Operator on your Kubernetes cluster to manage Amazon Managed Grafana for detailed demonstration of how to deploy the above solution on your Amazon EKS cluster.</p>"},{"location":"guides/operational/gitops-with-amg/gitops-with-amg/#using-gitops-with-flux-on-amazon-eks-to-manage-resources-in-amazon-managed-grafana","title":"Using GitOps with Flux on Amazon EKS to manage resources in Amazon Managed Grafana","text":"<p>As discussed above, Flux automates the deployment of applications on Kubernetes. It works by continuously monitoring the state of a Git repository such as GitHub and when changes are made to the repository, Flux automatically detects them and updates the cluster accordingly. Please reference the below architecture where we will be demonstrating how to use Grafana Operator from your Kubernetes cluster and GitOps mechanisms using Flux to add Amazon Managed Service for Prometheus as a data source and create dashboards in Amazon Managed Grafana in a Kubernetes native way. </p> <p></p> <p>Please refer to our One Observability Workshop module - GitOps with Amazon Managed Grafana. This module sets up required \"Day 2\" operational tooling such as the following on your EKS cluster:</p> <ul> <li>External Secrets Operator is installed successfully to read Amazon Managed Grafana secrets from AWS Secret Manager</li> <li>Prometheus Node Exporterto measure various machine resources such as memory, disk and CPU utilization</li> <li>Grafana Operator to use our Kubernetes cluster to create and manage the lifecyle of resources in Amazon Managed Grafana in a Kubernetes native way. </li> <li>Flux to automate the deployment of applications on Kubernetes using GitOps mechanisms.</li> </ul>"},{"location":"guides/operational/gitops-with-amg/gitops-with-amg/#conclusion","title":"Conclusion","text":"<p>In this section of Observability best practices guide, we learned about using Grafana Operator and GitOps with Amazon Managed Grafana. We started with learning about GitOps and Grafana Operator. Then we focussed on how to use Grafana Operator on Amazon EKS to manage resources in Amazon Managed Grafana and on how to use GitOps with Flux on Amazon EKS to manage resources in Amazon Managed Grafana to setup an identity with AMG, adding AWS data sources on Amazon Managed Grafana from Amazon EKS cluster in a Kubernetes native way.</p>"},{"location":"guides/partners/databricks/","title":"Databricks Monitoring and Observability Best Practices in AWS","text":"<p>Databricks is a platform for managing data analytics and AI/ML workloads. This guide aim at supporting customers running Databricks on AWS with monitoring these workloads using AWS Native services for observability or OpenSource Managed Services.</p>"},{"location":"guides/partners/databricks/#why-monitor-databricks","title":"Why monitor Databricks","text":"<p>Operation teams managing Databricks clusters benefit from having an integrated, customized dashboard to track workload status, errors, performance bottlenecks; alerting on unwanted behaviour, such as total resource usage over time, or percentual amount of errors; and centralized logging, for root cause analysis, as well as extracting additional customized metrics.</p>"},{"location":"guides/partners/databricks/#what-to-monitor","title":"What to monitor","text":"<p>Databricks run Apache Spark in its cluster instances, which has native features to expose metrics. These metrics will give information regarding drivers, workers, and the workloads being executed in the cluster.</p> <p>The instances running Spark will have additional useful information about storage, CPU, memory, and networking. It\u00b4s important to understand what external factors could be affecting the performance of a Databricks cluster. In the case of clusters with numerous instances, understanding bottlenecks and general health is important as well.</p>"},{"location":"guides/partners/databricks/#how-to-monitor","title":"How to monitor","text":"<p>To install collectors and it's dependencies, Databricks init scripts will be needed. These are scripts that are runned in each instance of a Databricks cluster at boot time.</p> <p>Databricks cluster permissions will also need permission to send metrics and logs using instance profiles.</p> <p>Finally, it's a best practice to configure metrics namespace in Databricks cluster Spark configuration, replacing <code>testApp</code> with a proper reference to the cluster.</p> <p> Figure 1: example of metrics namespace Spark configuration</p>"},{"location":"guides/partners/databricks/#key-parts-of-a-good-observability-solution-for-databricks","title":"Key parts of a good Observability solution for DataBricks","text":"<p>1) Metrics: Metrics are numbers that describe activity or a particular process measured over a period of time. Here are different types of metrics on Databricks:</p> <p>System resource-level metrics, such as CPU, memory, disk, and network. Application Metrics using Custom Metrics Source, StreamingQueryListener, and QueryExecutionListener, Spark Metrics exposed by MetricsSystem.</p> <p>2) Logs: Logs are a representation of serial events that have happened, and they tell a linear story about them. Here are different types of logs on Databricks:</p> <ul> <li>Event logs</li> <li>Audit logs</li> <li>Driver logs: stdout, stderr, log4j custom logs (enable structured logging)</li> <li>Executor logs: stdout, stderr, log4j custom logs (enable structured logging)</li> </ul> <p>3) Traces: Stack traces provide end-to-end visibility, and they show the entire flow through stages. This is useful when you must debug to identify which stages/codes cause errors/performance issues.</p> <p>4) Dashboards: Dashboards provide a great summary view of an application/service\u2019s golden metrics.</p> <p>5) Alerts: Alerts notify engineers about conditions that require attention.</p>"},{"location":"guides/partners/databricks/#aws-native-observability-options","title":"AWS Native Observability options","text":"<p>Native solutions, such as Ganglia UI and Log Delivery, are great solutions for collecting system metrics and querying Apache Spark\u2122 metrics. However, some areas can be improved:</p> <ul> <li>Ganglia doesn\u2019t support alerts.</li> <li>Ganglia doesn\u2019t support creating metrics derived from logs (e.g., ERROR log growth rate).</li> <li>You can\u2019t use custom dashboards to track SLO (Service Level Objectives) and SLI (Service Level Indicators) related to data-correctness, data-freshness, or end-to-end latency, and then visualize them with ganglia.</li> </ul> <p>Amazon CloudWatch is a critical tool for monitoring and managing your Databricks clusters on AWS. It provides valuable insights into cluster performance and helps you identify and resolve issues quickly. Integrating Databricks with CloudWatch and enabling structured logging can help improve those areas. CloudWatch Application Insights can help you automatically discover the fields contained in the logs, and CloudWatch Logs Insights provides a purpose-built query language for faster debugging and analysis.</p> <p> Figure 2: Databricks CloudWatch Architecture</p> <p>For more informaton on how to use CloudWatch to monitor Databricks, see: How to Monitor Databricks with Amazon CloudWatch</p>"},{"location":"guides/partners/databricks/#open-source-software-observability-options","title":"Open-source software observability options","text":"<p>Amazon Managed Service for Prometheus is a Prometheus-compatible monitoring managed, serverless service, that will be responsible for storing metrics, and managing alerts created on top of these metrics. Prometheus is a popular open source monitoring technology, being the second project belonging to the Cloud Native Computing Foundation, right after Kubernetes.</p> <p>Amazon Managed Grafana is a managed service for Grafana. Grafana is an open source technology for time-series data visualization, commonly used for observability. We can use Grafana to visualize data from several sources, such as Amazon Managed Service for Prometheus, Amazon CloudWatch, and many others. It will be used to visualize Databricks metrics and alerts.</p> <p>AWS Distro for OpenTelemetry is the AWS-supported distribution of OpenTelemetry project, which provides open source standards, libraries, and services for collecting traces and metrics. Through OpenTelemetry, we can collect several different observability data formats, such as Prometheus or StatsD, enrich this data, and send it to several destinations, such as CloudWatch or Amazon Managed Service for Prometheus.</p>"},{"location":"guides/partners/databricks/#use-cases","title":"Use cases","text":"<p>While AWS Native services will deliver the observability needed to manage Databricks clusters, there are some scenarios where using Open Source managed services is the best choice.</p> <p>Both Prometheus and Grafana are very popular technologies, and are already being used in many companies. AWS Open Source services for observability will allow operations teams to use the same existing infrastructure, the same query language, and existing dashboards and alerts to monitor Databricks workloads, without the heavy lifting of managing these services infrastructure, scalability, and performance.</p> <p>ADOT is the best alternative for teams that need to send metrics and traces to different destinations, such as CloudWatch and Prometheus, or work with different types of data sources, such as OTLP and StatsD.</p> <p>Finally, Amazon Managed Grafana supports many different Data Sources, including CloudWatch and Prometheus, and help correlate data for teams that decide on using more than one tool, allowing for the creation of templates that will enable observability for all Databricks Clusters, and a powerful API that allow its provisioning and configuration through Infrastructure as Code.</p> <p> Figure 3: Databricks Open Source Observability Architecture</p> <p>To observe metrics from a Databricks cluster using AWS Managed Open Source Services for Observability, you will need an Amazon Managed Grafana workspace for visualizing both metrics and alerts, and an Amazon Managed Service for Prometheus workspace, configured as a datasource in the Amazon Managed Grafana workspace.</p> <p>There are two important kind of metrics that must be collected: Spark and node metrics.</p> <p>Spark metrics will bring information such as current number of workers in the cluster, or executors; shuffles, that happen when nodes exchenge data during processing; or spills, when data go from RAM to disk and from disk to RAM. To expose these metrics, Spark native Prometheus - available since version 3.0 - must be enabled through Databricks management console, and configured through a <code>init_script</code>.</p> <p>To keep track of node metrics, such as disk usage, CPU time, memory, storage performance, we use the <code>node_exporter</code>, that can be used without any further configuration, but should only expose important metrics.</p> <p>An ADOT Collector must be installed in each node of the cluster, scraping the metrics exposed by both Spark and the <code>node_exporter</code>, filtering these metrics, injecting metadata such as <code>cluster_name</code>, and sending these metrics to the Prometheus workspace.</p> <p>Both the ADOT Collector and the <code>node _exporter</code> must be installed and configured through a <code>init_script</code>.</p> <p>The Databricks cluster must be configured with an IAM Role with permission to write metrics in the Prometheus workspace.</p>"},{"location":"guides/partners/databricks/#best-practices","title":"Best Practices","text":""},{"location":"guides/partners/databricks/#prioritize-valuable-metrics","title":"Prioritize valuable metrics","text":"<p>Spark and node_exporter both expose several metrics, and several formats for the same metrics. Without filtering which metrics are useful for monitoring and incident response, the mean time to detect problems increase, costs with storing samples increase, valuable information will be harder to be found and understood. Using OpenTelemetry processors, it is possible to filter and keep only valuable metrics, or filter out metrics that doesn't make sense; aggregate and calculate metrics before sending them to AMP.</p>"},{"location":"guides/partners/databricks/#avoid-alerting-fatigue","title":"Avoid alerting fatigue","text":"<p>Once valuable metrics are being ingested into AMP, it's essential to configure alerts. However, alerting on every resource usage burst may cause alerting fatigue, that is when too much noise will decrease the confidence in alerts severity, and leave important events undetected. AMP alerting rules group feature should be use to avoid ambiqguity, i.e., several connected alerts generating separated notifications. Also, alerts should receive the proper severity, and it should reflect business priorities.</p>"},{"location":"guides/partners/databricks/#reuse-amazon-managed-grafana-dashboards","title":"Reuse Amazon Managed Grafana dashboards","text":"<p>Amazon Managed Grafana leverages Grafana native templating feature, which allow the creation for dashboards for all existing and new Databricks clusters. It removes the need of manually creating and keeping visualizations for each cluster. To use this feature, its important to have the correct labels in the metrics to group these metrics per cluster. Once again, it's possible with OpenTelemetry processors.</p>"},{"location":"guides/partners/databricks/#references-and-more-information","title":"References and More Information","text":"<ul> <li>Create Amazon Managed Service for Prometheus workspace</li> <li>Create Amazon Managed Grafana workspace</li> <li>Configure Amazon Managed Service for Prometheus datasource</li> <li>Databricks Init Scripts</li> </ul>"},{"location":"guides/serverless/aws-native/lambda-based-observability/","title":"AWS Lambda based Serverless Observability","text":"<p>In the world of distributed systems and serverless computing, achieving observability is the key to ensuring application reliability and performance. It involves more than traditional monitoring. By leveraging AWS observability tools like Amazon CloudWatch and AWS X-Ray, you can gain insights into your serverless applications, troubleshoot issues, and optimize application performance. In this guide, we will learn essential concepts, tools and best practices to implement Observability of your Lambda based serverless application.</p> <p>The first step before you implement observability for your infrastructure or application is to determine your key objectives. It could be enhanced user experience, increased developer productivity, meeting service level objectives (SLOs), increasing business revenue or any other specific objective depending on your application type. So, clearly define these key objectives and establish how you would measure them. Then work backwards from there to design your observability strategy. Refer to \u201cMonitor what matters\u201d to learn more.</p>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#pillars-of-observability","title":"Pillars of Observability","text":"<p>There are three main pillars to observability:</p> <ul> <li>Logs: Timestamped records of discrete events that happened within an application or system, such as a failure, an error, or a state transformation</li> <li>Metrics: Numeric data measured at various time intervals (time series data); SLIs (request rate, error rate, duration, CPU%, etc.)</li> <li>Traces: A trace represents a single user\u2019s journey across multiple applications and systems (usually microservices)</li> </ul> <p>AWS offers both Native and Open source tools to facilitate logging, monitoring metrics, and tracing to obtain actionable insights for your AWS Lambda application.</p>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#logs","title":"Logs","text":"<p>In this section of the observability best practices guide, we will deep dive on to following topics:</p> <ul> <li>Unstructured vs structured logs</li> <li>CloudWatch Logs Insights</li> <li>Logging correlation Id</li> <li>Code Sample using Lambda Powertools</li> <li>Log visualization using CloudWatch Dashboards</li> <li>CloudWatch Logs Retention</li> </ul> <p>Logs are discrete events that have occurred within your application. These can include events like failures, errors, execution path or something else. Logs can be recorded in unstructured, semi-structured, or structured formats.</p>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#unstructured-vs-structured-logs","title":"Unstructured vs structured logs","text":"<p>We often see developers start with simple log messages within their application using <code>print</code> or <code>console.log</code> statements. These are difficult to parse and analyze programmatically at scale, particularly in a AWS Lambda based applications that can generate many lines of log messages across different log groups. As a result, consolidating these logs in CloudWatch becomes challenging and hard to analyze. You would need to do text match or regular expressions to find relevant information in the logs. Here\u2019s is an example of what unstructured logging looks like:</p> <pre><code>[2023-07-19T19:59:07Z]  INFO  Request started\n[2023-07-19T19:59:07Z]  INFO  AccessDenied: Could not access resource\n[2023-07-19T19:59:08Z]  INFO  Request finished\n</code></pre> <p>As you can see, the log messages lack a consistent structure, making it challenging to get useful insights from it. Also, it is hard to add contextual information to it.</p> <p>Whereas structured logging is a way to log information in a consistent format, often in JSON, that allows logs to be treated as data rather than text, which makes querying and filtering simple. It gives developers the ability to efficiently store, retrieve, and analyze the logs programmatically. It also facilitates better debugging. Structured logging provides a simpler way to modify the verbosity of logs across different environments through log levels. Pay attention to logging levels. Logging too much will increase costs and decrease application throughput. Ensure personal identifiable information is redacted before logging. Here\u2019s is an example of what structured logging looks like:</p> <pre><code>{\n   \"correlationId\": \"9ac54d82-75e0-4f0d-ae3c-e84ca400b3bd\",\n   \"requestId\": \"58d9c96e-ae9f-43db-a353-c48e7a70bfa8\",\n   \"level\": \"INFO\",\n   \"message\": \"AccessDenied\",\n   \"function-name\": \"demo-observability-function\",\n   \"cold-start\": true\n}\n</code></pre> <p><code>Prefer structured and centralized logging into CloudWatch logs</code> to emit operational information about transactions, correlation identifiers across different components, and business outcomes from your application. </p>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#cloudwatch-logs-insights","title":"CloudWatch Logs Insights","text":"<p>Use CloudWatch Logs Insights, which can automatically discover fields in JSON formatted logs. In addition, JSON logs can be extended to log custom metadata specific to your application that can be used to search, filter, and aggregate your logs.</p>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#logging-correlation-id","title":"Logging correlation Id","text":"<p>For example, for an http request coming in from API Gateway, the correlation Id is set at the <code>requestContext.requestId</code> path, which can be easily extracted and logged in the downstream Lambda functions using Lambda powertools. Distributed systems often involve multiple services and components working together to handle a request. So, logging correlation Id and passing them to downstream systems becomes crucial for end-to-end tracing and debugging. A correlation Id is a unique identifier assigned to a request at the very beginning. As the request moves through different services, the correlation Id is included in the logs, allowing you to trace the entire path of the request. You can either manually insert correlation Id to your AWS Lambda logs or use tools like AWS Lambda powertools to easily grab the correlation Id the from API Gateway and log it along with your application logs. For example, for an http request correlation Id could be a request-id which can be initiated at API Gateway and then passed on to your backend services like Lambda functions.</p>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#code-sample-using-lambda-powertools","title":"Code Sample using Lambda Powertools","text":"<p>As a best practice, generate a correlation Id as early as possible in the request lifecycle, preferably at the entry point of your serverless application, such as API Gateway or application load balancer. Use UUIDs, or request id or any other unique attribute which can used to track the request across distributed systems. Pass the correlation id along with each request either as part of the custom header, body or metadata. Ensure that correlation Id is included in all the log entries and traces in your downstream services. </p> <p>You can either manually capture and include correlation Id as part of your Lambda function logs or use tools like AWS Lambda Powertools. With Lambda Powertools, you can easily grab the correlation Id from predefined request path mapping for supported upstream services and automatically add it alongside your application logs. Also, ensure that correlation Id is added to all your error messages to easily debug and identify the root cause in case of failures and tie it back to the original request.</p> <p>Let's look at the code sample to demostrate structured logging with correlation id and viewing it in CloudWatch for below serverless architecture:</p> <p></p> <pre><code>// Initializing Logger\nLogger log = LogManager.getLogger();\n\n// Uses @Logger annotation from Lambda Powertools, which takes optional parameter correlationIdPath to extract correlation Id from the API Gateway header and inserts correlation_id to the Lambda function logs in a structured format.\n@Logging(correlationIdPath = \"/headers/path-to-correlation-id\")\npublic APIGatewayProxyResponseEvent handleRequest(final APIGatewayProxyRequestEvent input, final Context context) {\n  ...\n  // The log statement below will also have additional correlation_id\n  log.info(\"Success\")\n  ...\n}\n</code></pre> <p>In this example, a Java based Lambda function is using Lambda Powertools library to log <code>correlation_id</code> coming in from the api gateway request.</p> <p>Sample CloudWatch logs for the code sample:</p> <pre><code>{\n   \"level\": \"INFO\",\n   \"message\": \"Success\",\n   \"function-name\": \"demo-observability-function\",\n   \"cold-start\": true,\n   \"lambda_request_id\": \"52fdfc07-2182-154f-163f-5f0f9a621d72\",\n   \"correlation_id\": \"&lt;correlation_id_value&gt;\"\n}_\n</code></pre>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#log-visualization-using-cloudwatch-dashboards","title":"Log visualization using CloudWatch Dashboards","text":"<p>Once you log the data in structured JSON format, CloudWatch Logs Insights then automatically discovers values in JSON output and parses the messages as fields. CloudWatch Logs insights provides purpose-built SQL-like query language to search and filter multiple log streams. You can perform queries over multiple log groups using glob and regular expressions pattern matching. In addition, you can also write your custom queries and save them to re-run it again without having to re-create them each time.</p> <p> In CloudWatch logs insights, you can generate visualizations like line charts, bar charts, and stacked area charts from your queries with one or more aggregation functions. You can then easily add these visualization to the CloudWatch Dashboards. Sample dashboard below shows percentile report of Lambda function\u2019s execution duration. Such dashboards will quickly give you insights on where you should focus on improve application performance. Average latency is a good metrics to look at but <code>you should aim to optimize for p99 and not the average latency.</code> </p> <p> To send (platform, function and extensions) logs to locations other than CloudWatch, you could use Lambda Telemetry API with Lambda Extensions. A number of partner solutions provide Lambda layers which use the Lambda Telemetry API and make integration with their systems easier.</p> <p>To make the best use of CloudWatch logs insights, think about what data you must be ingesting into your logs in the form of structured logging, which will then help better monitor the health of your application.</p>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#cloudwatch-logs-retention","title":"CloudWatch Logs Retention","text":"<p>By default all messages that are written to stdout in your Lambda function are saved to an Amazon CloudWatch log stream. Lambda function's execution role should have permission to create CloudWatch log streams and write log events the streams. It is important to be aware that CloudWatch is billed by the amount of data ingested, and the storage used. Therefore, reducing the amount of logging will help you minimize the associated cost. <code>By default CloudWatch logs are kept indefinitely and never expire. It is recommended to configure log retention policy to reduce log-storage costs</code>, and apply it across all your log groups. You might want differing retention policies per environment. Log retention can be configured manually in the AWS console but to ensure consistency and best practices, you should configure it as part of your Infrastructure as Code (IaC) deployments. Below is a sample CloudFormation template that demonstrates how to configuring Log Retention for Lambda function:</p> <pre><code>Resources:\n  Function:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: .\n      Runtime: python3.8\n      Handler: main.handler\n      Tracing: Active\n\n  # Explicit log group that refers to the Lambda function\n  LogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub \"/aws/lambda/${Function}\"\n      # Explicit retention time\n      RetentionInDays: 7\n</code></pre> <p>In this example, we created a Lambda function and corresponding log group. The <code>RetentionInDays</code> property is set to 7 days, meaning that logs in this log group will be retained for 7 days before they are automatically deleted, thus helping to control log storage cost.</p>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#metrics","title":"Metrics","text":"<p>In this section of Observability best practices guide, we will deep dive on to following topics:</p> <ul> <li>Monitor and alert on out-of-the-box metrics</li> <li>Publish custom metrics</li> <li>Use embedded-metrics to auto generate metrics from your logs</li> <li>Use CloudWatch Lambda Insights to monitor system-level metrics</li> <li>Creating CloudWatch Alarms </li> </ul>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#monitor-and-alert-on-out-of-the-box-metrics","title":"Monitor and alert on out-of-the-box metrics","text":"<p>Metrics are numeric data measured at various time intervals (time series data) and service-level indicators (request rate, error rate, duration, CPU, etc.). AWS services provide a number of out-of-the-box standard metrics to help monitor the operational health of your application. Establish key metrics applicable for your application and use them for monitor performance of your application. Examples of key metrics may include function errors, queue depth, failed state machine executions, and api response times.</p> <p>One challenge with out-of-the-box metrics is knowing how to analyze them in a CloudWatch dashboard. For example, when looking at Concurrency, do I look at max, average, or percentile? And the right statistics to monitor differs for each metric.</p> <p>As best practices, for Lambda function\u2019s <code>ConcurrentExecutions</code> metrics look at the <code>Count</code> statistics to check if it is getting close to the account and regional limit or close to the Lambda reserved concurrency limit if applicable.  For <code>Duration</code> metric, which indicates how long your function takes to process an event, look at the <code>Average</code> or <code>Max</code> statistic. For measuring the latency of your API, look at the <code>Percentile</code> statistics for API Gateway\u2019s <code>Latency</code> metrics. P50, P90, and P99 are much better methods of monitoring latency over averages.</p> <p>Once you know what metrics to monitor, configure alerts on these key metrics to engage you when components of your application are unhealthy. For Example</p> <ul> <li>For AWS Lambda, alert on Duration, Errors, Throttling, and ConcurrentExecutions. For stream-based invocations, alert on IteratorAge. For Asynchronous invocations, alert on DeadLetterErrors.</li> <li>For Amazon API Gateway, alert on IntegrationLatency, Latency, 5XXError, 4XXError</li> <li>For Amazon SQS, alert on ApproximateAgeOfOldestMessage, ApproximateNumberOfMessageVisible</li> <li>For AWS Step Functions, alert on ExecutionThrottled, ExecutionsFailed, ExecutionsTimedOut</li> </ul>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#publish-custom-metrics","title":"Publish custom metrics","text":"<p>Identify key performance indicators (KPIs) based on desired business and customer outcomes for your application. Evaluate KPIs to determine application success and operational health. Key metrics may vary depending on the type of application, but examples include site visited, orders placed, flights purchased, page load time, unique visitors etc.</p> <p>One way to publish custom metrics to AWS CloudWatch is by calling CloudWatch metrics SDK\u2019s <code>putMetricData</code> API. However, <code>putMetricData</code> API call is synchronous. It will increase the duration of your Lambda function and it can potentially block other API calls in your application, leading to performance bottlenecks. Also, longer execution duration of your Lambda function will attribute towards higher cost. Additionally you are charged for both the number of custom metrics that are sent to CloudWatch and the number of API calls (i.e. PutMetricData API calls) that are made. </p> <p><code>A more efficient and cost-effective way to publish custom metrics is with</code> CloudWatch Embedded Metrics Format (EMF). The CloudWatch Embedded Metric format allows you to generate custom metrics <code>asynchronously</code> as logs written to CloudWatch logs, resulting in improved performance of your application at a lower cost. With EMF, you can embed custom metrics alongside detailed log event data, and CloudWatch automatically extracts these custom metrics so that you can visualize and set alarm on them as you would do out-of-the-box metrics. By sending logs in the embedded metric format, you can query it using CloudWatch Logs Insights, and you only pay for the query, not the cost of the metrics.</p> <p>To achieve this, you can generate the logs using EMF specification, and send them to CloudWatch using <code>PutLogEvents</code> API. To simplify the process, there are two client libraries that support the creation of metrics in the EMF format.</p> <ul> <li>Low level client libraries (aws-embedded-metrics)</li> <li>Lambda Powertools Metrics.</li> </ul>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#use-cloudwatch-lambda-insights-to-monitor-system-level-metrics","title":"Use CloudWatch Lambda Insights to monitor system-level metrics","text":"<p>CloudWatch Lambda insights provides you system-level metrics, including CPU time, memory usage, disk utilization, and network performance. Lambda Insights also collects, aggregates, and summarizes diagnostic information, such as <code>cold starts</code> and Lambda worker shutdowns. Lambda Insights leverages CloudWatch Lambda extension, which is packaged as a Lambda layer. Once enabled, it collects system-level metrics and emits a single performance log event to CloudWatch Logs for every invocation of that Lambda function in the embedded metrics format. </p> <p>Note</p> <p>CloudWatch Lambda Insights is not enabled by default and needs to be turned on per Lambda function. </p> <p>You can enable it via AWS console or via  Infrastructure as Code (IaC). Here is an example of how to enable it using the AWS serverless application model (SAM). You add <code>LambdaInsightsExtension</code> extension Layer to your Lambda function, and also add managed IAM policy <code>CloudWatchLambdaInsightsExecutionRolePolicy</code>, which gives permissions to your Lambda function to create log stream and call <code>PutLogEvents</code> API to be able to write logs to it.</p> <pre><code>// Add LambdaInsightsExtension Layer to your function resource\nResources:\n  MyFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      Layers:\n        - !Sub \"arn:aws:lambda:${AWS::Region}:580247275435:layer:LambdaInsightsExtension:14\"\n\n// Add IAM policy to enable Lambda function to write logs to CloudWatch\nResources:\n  MyFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      Policies:\n        - `CloudWatchLambdaInsightsExecutionRolePolicy`\n</code></pre> <p>You can then use CloudWatch console to view these system-level performance metrics under Lambda Insights.</p> <p></p>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#creating-cloudwatch-alarms","title":"Creating CloudWatch Alarms","text":"<p>Creating CloudWatch Alarms and take necessary actions when metrics go off is a critical part of observability. Amazon CloudWatch alarms are used to alert you or automate remediation actions when application and infrastructure metrics exceed static or dynamically set thresholds. </p> <p>To set up an alarm for a metric, you select a threshold value that triggers a set of actions. A fixed threshold value is known as a static threshold. For instance, you can configure an alarm on <code>Throttles</code> metrics from Lambda function to activate if it exceeds 10% of the time within a 5-min period. This could potentially mean that Lambda function has reached its max concurrency for your account and region.</p> <p>In a serverless application, it is common to send an alert using SNS (Simple Notification Service). This enables users to receive alerts via email, SMS, or other channels. Additionally, you can subscribe a Lambda function to the SNS topic, allowing it to auto remediate any issues which caused the alarm to go off. </p> <p>For example, Let\u2019s say you have a Lambda function A, which is polling an SQS queue and calling a downstream service. If downstream service is down and not responding, Lambda function will continue to poll from SQS and try calling downstream service with failures. While you can monitor these errors and generate a CloudWatch alarm using SNS to notify appropriate team, you can also call another Lambda function B (via SNS subscription), which can disable the event-source-mapping for the Lambda function A and thus stopping it from polling SQS queue, until the downstream service is back up and running.</p> <p>While setting up alarms on an individual metric is good, sometimes monitoring multiple metrics becomes necessary to better understand the operational health and performance of your application. In such a scenario, you should setup alarms based on multiple metrics using metric math expression. </p> <p>For example, if you want to monitor AWS Lambda errors but allow a small number of errors without triggering your alarm, y you can create an error rate expression in the form of a percentage. i.e. ErrorRate = errors / invocation * 100, then create an alarm to send an alert if the ErrorRate goes above 20% within the configured evaluation period.</p>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#tracing","title":"Tracing","text":"<p>In this section of Observability best practices guide, we will deep dive on to following topics:</p> <ul> <li>Introduction to distributed tracing and AWS X-Ray</li> <li>Apply appropriate sampling rule</li> <li>Use X-Ray SDK to trace interaction with other services</li> <li>Code Sample for tracing integrated services using X-Ray SDK</li> </ul>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#introduction-to-distributed-tracing-and-aws-x-ray","title":"Introduction to distributed tracing and AWS X-Ray","text":"<p>Most serverless applications consist of multiple microservices, each using multiple AWS services. Due to the nature of serverless architectures, it\u2019s crucial to have distributed tracing. For effective performance monitoring and error tracking, it is important to trace the transaction across entire application flow, from the source caller through all the downstream services. While it\u2019s possible to achieve this using individual service\u2019s logs, it\u2019s faster and more efficient to use a tracing tool like AWS X-Ray. See Instrumenting your application with AWS X-Ray for more information.</p> <p>AWS X-Ray enables you to trace requests as it flows through the involved microservices. X-Ray Service maps enables you to understand different integration points and identify any performance degradation of your application. You can quickly isolate which component of you application is causing errors, throttling or having latency issues with just few clicks. Under the service graph, you can also individual traces to pinpoint the exact duration taken by each microservice.</p> <p></p> <p><code>As a best practice, create custom subsegments in your code for downstream calls</code> or any specific functionality that requires monitoring. For instance, you can create a subsegment to monitor a call to an external HTTP API, or an SQL database query.</p> <p>For example, To create a custom subsegment for a function that makes calls to downstream services, use the <code>captureAsyncFunc</code> function (in node.js)</p> <pre><code>var AWSXRay = require('aws-xray-sdk');\n\napp.use(AWSXRay.express.openSegment('MyApp'));\n\napp.get('/', function (req, res) {\n  var host = 'api.example.com';\n\n  // start of the subsegment\n  AWSXRay.captureAsyncFunc('send', function(subsegment) {\n    sendRequest(host, function() {\n      console.log('rendering!');\n      res.render('index');\n\n      // end of the subsegment\n      subsegment.close();\n    });\n  });\n});\n</code></pre> <p>In this example, the application creates a custom subsegment named <code>send</code> for calls to the <code>sendRequest</code> function. <code>captureAsyncFunc</code> passes a subsegment that you must close within the callback function when the asynchronous calls that it makes are complete.</p>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#apply-appropriate-sampling-rule","title":"Apply appropriate sampling rule","text":"<p>AWS X-Ray SDK does not trace all requests by default. It applies a conservative sampling rule to provide a representative sample of the requests without incurring high cost. However, you can customize the default sampling rule or disable sampling altogether and start tracing all your requests based on your specific requirements. </p> <p>It\u2019s important to note that AWS X-Ray is not intended to be used as an audit or compliance tool. You should consider having <code>different sampling rate for different type of application</code>. For instance, high-volume read-only calls, like background polling, or health checks can be sampled at a lower rate while still providing enough data to identify any potential issues that may arise. You may also want to have <code>different sampling rate per environment</code>. For instance, in your development environment, you may want all your requests to be traced to troubleshoot any errors or performance issues easily, whereas for production environment you may have lower number of traces. <code>You should also keep in mind that extensive tracing can result in increased cost</code>. For more information about sampling rules, see Configuring sampling rules in the X-Ray console.</p>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#use-x-ray-sdk-to-trace-interaction-with-other-aws-services","title":"Use X-Ray SDK to trace interaction with other AWS services","text":"<p>While X-Ray tracing can be easily enabled for services like AWS Lambda and Amazon API Gateway, with just few clicks or few lines on your IaC tool, other services require additional steps to instrument their code. Here is the complete list of AWS Services integrated with X-Ray. </p> <p>To instrument calls to the services which are not integrated with X-Ray, such as DynamoDB, you can capture traces by wrapping AWS SDK calls with the AWS X-Ray SDK. For instance, when using node.js, you can follow below code example to capture all AWS SDK calls:</p>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#code-sample-for-tracing-integrated-services-using-x-ray-sdk","title":"Code sample for tracing integrated services using X-Ray SDK","text":"<pre><code>//... FROM (old code)\nconst AWS = require('aws-sdk');\n\n//... TO (new code)\nconst AWSXRay = require('aws-xray-sdk-core');\nconst AWS = AWSXRay.captureAWS(require('aws-sdk'));\n...\n</code></pre> <p>Note</p> <p>To instrument individual clients wrap your AWS SDK client in a call to <code>AWSXRay.captureAWSClient</code>.  Do not use both <code>captureAWS</code> and <code>captureAWSClient</code> together. This will lead to duplicate traces.</p>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#additional-resources","title":"Additional Resources","text":"<p>CloudWatch Logs Insights</p> <p>CloudWatch Lambda Insights</p> <p>Embedded Metrics Library</p>"},{"location":"guides/serverless/aws-native/lambda-based-observability/#summary","title":"Summary","text":"<p>In this observability best practice guide for AWS Lambda based serverless application, we highlighted critical aspects such as logging, metrics and tracing using Native AWS services such as Amazon CloudWatch and AWS X-Ray. We recommended using AWS Lambda Powertools library to easily add observability best practices to your application. By adopting these best practices, you can unlock valuable insights into your serverless application, enabling faster error detection and performance optimization.</p> <p>For further deep dive, we would highly recommend you to practice AWS Native Observability module of AWS One Observability Workshop.</p>"},{"location":"guides/serverless/oss/lambda-based-observability-adot/","title":"AWS Lambda based Serverless Observability with OpenTelemetry","text":"<p>This guide covers the best practices on configuring observability for Lambda based serverless applications using managed open-source tools and technologies together with the native AWS monitoring services such as AWS X-Ray, and Amazon CloudWatch. We will cover tools such as AWS Distro for OpenTelemetry (ADOT), AWS X-Ray, and Amazon Managed Service for Prometheus (AMP) and how you can use these tools to gain actionable insights into your serverless applications, troubleshoot issues, and optimize application performance.</p>"},{"location":"guides/serverless/oss/lambda-based-observability-adot/#key-topics-covered","title":"Key topics covered","text":"<p>In this section of the observability best practices guide, we will deep dive on to following topics:</p> <ul> <li>Introduction to AWS Distro for OpenTelemetry (ADOT) and ADOT Lambda Layer</li> <li>Auto-instrumentation Lambda function using ADOT Lambda Layer</li> <li>Custom configuration support for ADOT Collector</li> <li>Integration with Amazon Managed Service for Prometheus (AMP)</li> <li>Pros and cons of using ADOT Lambda Layer</li> <li>Managing cold start latency when using ADOT</li> </ul>"},{"location":"guides/serverless/oss/lambda-based-observability-adot/#introduction-to-aws-distro-for-opentelemetry-adot","title":"Introduction to AWS Distro for OpenTelemetry (ADOT)","text":"<p>AWS Distro for OpenTelemetry (ADOT) is a secure, production-ready, AWS-supported distribution of the Cloud Native Computing Foundation (CNCF) OpenTelemetry (OTel) project. Using ADOT, you can instrument your applications just once and send correlated metrics and traces to multiple monitoring solutions.</p> <p>AWS's managed OpenTelemetry Lambda Layer utilizes OpenTelemetry Lambda Layer  to export telemetry data asynchronously from AWS Lambda. It provides plug-and-play user experience by wrapping an AWS Lambda function, and by packaging the OpenTelemetry runtime specific SDK, trimmed down version of ADOT collector together with an out-of-the-box configuration for auto-instrumenting AWS Lambda functions. ADOT Lambda Layer collector components, such as Receivers, Exporters, and Extensions support integration with Amazon CloudWatch, Amazon OpenSearch Service, Amazon Managed Service for Prometheus, AWS X-Ray, and others. Find the complete list here. ADOT also supports integrations with partner solutions.</p> <p>ADOT Lambda Layer supports both auto-instrumentation (for Python, NodeJS, and Java) as well as custom instrumentation for any specific set of libraries and SDKs. With auto-instrumentation, by default, the Lambda Layer is configured to export traces to AWS X-Ray. For custom instrumentation, you will need to include the corresponding library instrumentation from the respective OpenTelemetry runtime instrumentation repository and modify your code to initialize it in your function.</p>"},{"location":"guides/serverless/oss/lambda-based-observability-adot/#auto-instrumentation-using-adot-lambda-layer-with-aws-lambda","title":"Auto-instrumentation using ADOT Lambda Layer with AWS Lambda","text":"<p>You can easily enable auto-instrumentation of Lambda function using ADOT Lambda Layer without any code changes. Let\u2019s take an example of adding ADOT Lambda layer to your existing Java based Lambda function and view execution logs and traces in CloudWatch.</p> <ol> <li>Choose the ARN of the Lambda Layer based on the <code>runtime</code>, <code>region</code> and the <code>arch type</code> as per the documentation. Make sure you use the Lambda Layer in the same region as your Lambda function. For example, Lambda Layer for java auto-instrumentation would be <code>arn:aws:lambda:us-east-1:901920570463:layer:aws-otel-java-agent-x86_64-ver-1-28-1:1</code></li> <li>Add Layer to your Lambda function either via Console of IaC of your choice. <ul> <li>With AWS Console, follow the instructions to add Layer to your Lambda function. Under Specify an ARN paste the layer ARN selected above.</li> <li>With IaC option, SAM template for Lambda function would look like this: <pre><code>Layers:\n- !Sub arn:aws:lambda:${AWS::Region}:901920570463:layer:aws-otel-java-agent-arm64-ver-1-28-1:1\n</code></pre></li> </ul> </li> <li>Add an environment variable <code>AWS_LAMBDA_EXEC_WRAPPER=/opt/otel-handler</code> for Node.js or Java, and <code>AWS_LAMBDA_EXEC_WRAPPER=/opt/otel-instrument</code> for Python to your Lambda function.</li> <li>Enable Active Tracing for your Lambda function. <code>Note</code> that by default, the layer is configured to export traces to AWS X-Ray. Make sure your Lambda function\u2019s execution role has the required AWS X-Ray permissions. For more on AWS X-Ray permissions for AWS Lambda, see the AWS Lambda documentation.<ul> <li><code>Tracing: Active</code></li> </ul> </li> <li>Example SAM template with Lambda Layer configuration, Environment Variable, and X-Ray tracing would look something like this: <pre><code>Resources:\n  ListBucketsFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      Handler: com.example.App::handleRequest\n      ...\n      ProvisionedConcurrencyConfig:\n        ProvisionedConcurrentExecutions: 1\n      Policies:\n        - AWSXrayWriteOnlyAccess\n        - AmazonS3ReadOnlyAccess\n      Environment:\n        Variables:\n          AWS_LAMBDA_EXEC_WRAPPER: /opt/otel-handler\n      Tracing: Active\n      Layers:\n        - !Sub arn:aws:lambda:${AWS::Region}:901920570463:layer:aws-otel-java-agent-amd64-ver-1-28-1:1\n      Events:\n        HelloWorld:\n          Type: Api\n          Properties:\n            Path: /listBuckets\n            Method: get\n</code></pre></li> <li>Testing and Visualizing traces in AWS X-Ray Invoke your Lambda function either directly or via an API (if an API is configured as a trigger). For example, invoking Lambda function via API (using <code>curl</code>) would generate logs as below: <pre><code>curl -X GET https://XXXXXX.execute-api.us-east-1.amazonaws.com/Prod/listBuckets\n</code></pre> Lambda function logs:</li> </ol> <pre><code>\nINIT_START Runtime Version: java:11.v23 Runtime Version ARN: arn:aws:lambda:us-east-1::runtime:d8547b2b4b9c3a87d99a42a8197838256f11e0afe5f7ce2d58e4574f4e6cf50d\n{level:info,msg:Launching OpenTelemetry Lambda extension,version:v0.32.0}\n{level:info,logger:telemetryAPI.Listener,msg:Listening for requests,address:sandbox:53612}\n{level:info,logger:telemetryAPI.Client,msg:Subscribing,baseURL:http://127.0.0.1:9001/2022-07-01/telemetry}\n{level:info,logger:telemetryAPI.Client,msg:Subscription success,response:\\OK\\}\n{level:info,caller:service/telemetry.go:84,msg:Setting up own telemetry...}\n{level:info,caller:service/telemetry.go:201,msg:Serving Prometheus metrics,address:localhost:8888,level:Basic}\n{level:info,caller:exporter@v0.82.0/exporter.go:275,msg:Development component. May change in the future.,kind:exporter,data_type:metrics,name:logging}\n{level:info,caller:service/service.go:132,msg:Starting aws-otel-lambda...,Version:v0.32.0,NumCPU:2}\n{level:info,caller:extensions/extensions.go:30,msg:Starting extensions...}\n{level:info,caller:otlpreceiver@v0.82.0/otlp.go:83,msg:Starting GRPC server,kind:receiver,name:otlp,data_type:traces,endpoint:localhost:4317}\n{level:info,caller:otlpreceiver@v0.82.0/otlp.go:101,msg:Starting HTTP server,kind:receiver,name:otlp,data_type:traces,endpoint:localhost:4318}\n{level:info,caller:service/service.go:149,msg:Everything is ready. Begin running and processing data.}\nPicked up JAVA_TOOL_OPTIONS: -javaagent:/opt/opentelemetry-javaagent.jar\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\n[otel.javaagent 2023-09-24 15:28:16:862 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 1.28.0-adot-lambda1-aws\nEXTENSION Name: collector State: Ready Events: [INVOKE, SHUTDOWN]\nSTART RequestId: ed8f8444-3c29-40fe-a4a1-aca7af8cd940 Version: 3\n...\nEvent### {resource: /listBuckets,path: /listBuckets/,httpMethod: GET,headers: {Accept=*/*, CloudFront-Forwarded-Proto=https, CloudFront-Is-Desktop-Viewer=true, CloudFront-Is-Mobile-Viewer=false, CloudFront-Is-SmartTV-Viewer=false, CloudFront-Is-Tablet-Viewer=false, CloudFront-Viewer-ASN=7224, CloudFront-Viewer-Country=US, Host=yf0ukeu0sg.execute-api.us-east-1.amazonaws.com, User-Agent=curl/8.1.2, Via=2.0 31341771a4bfa40d7b1f61883ffb56c6.cloudfront.net (CloudFront), X-Amz-Cf-Id=ooZO2eehCAwGzPW0nq6NKOwKLQGBkmhtuFCzsRhyCISbnex6c45Jcw==, X-Amzn-Trace-Id=Root=1-65105691-384f7da75714148655fa631b, X-Forwarded-For=15.248.0.124, 15.158.50.42, X-Forwarded-Port=443, X-Forwarded-Proto=https},multiValueHeaders: {Accept=[*/*], CloudFront-Forwarded-Proto=[https], CloudFront-Is-Desktop-Viewer=[true], CloudFront-Is-Mobile-Viewer=[false], CloudFront-Is-SmartTV-Viewer=[false], CloudFront-Is-Tablet-Viewer=[false], CloudFront-Viewer-ASN=[7224], CloudFront-Viewer-Country=[US], Host=[yf0ukeu0sg.execute-api.us-east-1.amazonaws.com], User-Agent=[curl/8.1.2], Via=[2.0 31341771a4bfa40d7b1f61883ffb56c6.cloudfront.net (CloudFront)], X-Amz-Cf-Id=[ooZO2eehCAwGzPW0nq6NKOwKLQGBkmhtuFCzsRhyCISbnex6c45Jcw==], X-Amzn-Trace-Id=[Root=1-65105691-384f7da75714148655fa631b], X-Forwarded-For=[15.248.0.124, 15.158.50.42], X-Forwarded-Port=[443], X-Forwarded-Proto=[https]},requestContext: {accountId: 681808143105,resourceId: 7xxz9z,stage: Prod,requestId: 56c8c9ad-3d8d-4f20-aed1-bcef03b5e8e4,identity: {sourceIp: 15.248.0.124,userAgent: curl/8.1.2,},resourcePath: /listBuckets,httpMethod: GET,apiId: yf0ukeu0sg,path: /Prod/listBuckets/,},isBase64Encoded: false}\n...\nEND RequestId: ed8f8444-3c29-40fe-a4a1-aca7af8cd940\nREPORT RequestId: ed8f8444-3c29-40fe-a4a1-aca7af8cd940 Duration: 5144.38 ms Billed Duration: 5145 ms Memory Size: 1024 MB Max Memory Used: 345 MB Init Duration: 27769.64 ms\nXRAY TraceId: 1-65105691-384f7da75714148655fa631b SegmentId: 2c52a147021ebd20 Sampled: true\n</code></pre> <p>As you can see from the logs, OpenTelemetry Lambda extension starts listening and instrumenting Lambda functions using opentelemetry-javaagent and generates traces in AWS X-Ray.</p> <p>To view the traces from the above Lambda function invocation, navigate to the AWS X-Ray console and select the trace id under Traces. You should see a Trace Map along with Segments Timeline as below: </p>"},{"location":"guides/serverless/oss/lambda-based-observability-adot/#custom-configuration-support-for-adot-collector","title":"Custom configuration support for ADOT Collector","text":"<p>The ADOT Lambda Layer combines both OpenTelemetry SDK and the ADOT Collector components. The configuration of the ADOT Collector follows the OpenTelemetry standard. By default, the ADOT Lambda Layer uses config.yaml, which exports telemetry data to AWS X-Ray. However, ADOT Lambda Layer also supports other exporters, which enables you to send metrics and traces to other destinations. Find the complete list of available components supported for custom configuration here.</p>"},{"location":"guides/serverless/oss/lambda-based-observability-adot/#integration-with-amazon-managed-service-for-prometheus-amp","title":"Integration with Amazon Managed Service for Prometheus (AMP)","text":"<p>You can use custom collector configuration to export metrics from your Lambda function to Amazon Managed Prometheus (AMP).</p> <ol> <li>Follow the steps from auto-instrumentation above, to configure Lambda Layer, set Environment variable <code>AWS_LAMBDA_EXEC_WRAPPER</code>.</li> <li>Follow the instructions to create Amazon Manager Prometheus workspace in your AWS account, where your Lambda function will be sending metrics to. Make a note of the <code>Endpoint - remote write URL</code> from the AMP workspace. You would need that to be configured on ADOT collector configuration.</li> <li> <p>Create a custom ADOT collector configuration file (say <code>collector.yaml</code>) in your Lambda function's root directory with details of AMP endpoint remote write URL from previous step. You can also load the configuration file from S3 bucket. Sample ADOT collector configuration file: <pre><code>#collector.yaml in the root directory\n#Set an environemnt variable 'OPENTELEMETRY_COLLECTOR_CONFIG_FILE' to '/var/task/collector.yaml'\n\nextensions:\n  sigv4auth:\n    service: \"aps\"\n    region: \"&lt;workspace_region&gt;\"\n\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n      http:\n\nexporters:\n  logging:\n  prometheusremotewrite:\n    endpoint: \"&lt;workspace_remote_write_url&gt;\"\n    namespace: test\n    auth:\n      authenticator: sigv4auth\n\nservice:\n  extensions: [sigv4auth]\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [awsxray]\n    metrics:\n      receivers: [otlp]\n      exporters: [logging, prometheusremotewrite]\n</code></pre> Prometheus Remote Write Exporter can also be configured with retry, and timeout settings. For more information see the documentation. <code>Note</code> Service value for <code>sigv4auth</code> extension should be <code>aps</code> (amazon prometheus service). Also, Make sure your Lambda function execution role has the required AMP permissions. For more information on permissions and policies required on AMP for AWS Lambda, see the AWS Managed Service for Prometheus documentation.</p> </li> <li> <p>Add an environment variable <code>OPENTELEMETRY_COLLECTOR_CONFIG_FILE</code> and set value to the path of configuration file. E.g.  /var/task/<code>&lt;path to config file&gt;</code>.yaml. This will tell the Lambda Layer extension where to find the collector configuration. <pre><code>Function:\n    Type: AWS::Serverless::Function\n    Properties:\n      ...\n      Environment:\n        Variables:\n          OPENTELEMETRY_COLLECTOR_CONFIG_FILE: /var/task/collector.yaml\n</code></pre></p> </li> <li>Update your Lambda function code to add metrics using OpenTelemetry Metrics API. Check out examples here. <pre><code>// get meter\nMeter meter = GlobalOpenTelemetry.getMeterProvider()\n    .meterBuilder(\"aws-otel\")\n    .setInstrumentationVersion(\"1.0\")\n    .build();\n\n// Build counter e.g. LongCounter\nLongCounter counter = meter\n    .counterBuilder(\"processed_jobs\")\n    .setDescription(\"Processed jobs\")\n    .setUnit(\"1\")\n    .build();\n\n// It is recommended that the API user keep a reference to Attributes they will record against\nAttributes attributes = Attributes.of(stringKey(\"Key\"), \"SomeWork\");\n\n// Record data\ncounter.add(123, attributes);\n</code></pre></li> </ol>"},{"location":"guides/serverless/oss/lambda-based-observability-adot/#pros-and-cons-of-using-adot-lambda-layer","title":"Pros and Cons of using ADOT Lambda Layer","text":"<p>If you intend to send traces to AWS X-Ray from Lambda function, you can either use X-Ray SDK or  AWS Distro for OpenTelemetry (ADOT) Lambda Layer. While X-Ray SDK supports easy instrumentation of various AWS services, it can only send traces to X-Ray. Whereas, ADOT collector, which is included as part of the Lambda Layer supports large number of library instrumentations for each language. You can use it to collect and send metrics and traces to AWS X-Ray and other monitoring solutions, such as Amazon CloudWatch, Amazon OpenSearch Service, Amazon Managed Service for Prometheus and other partner solutions.</p> <p>However, due to the flexibility ADOT offers, your Lambda function may require additional memory and can experience notable impact on cold start latency. So, if you are optimizing your Lambda function for low-latency and do not need advanced features of OpenTelemetry, using AWS X-Ray SDK over ADOT might be more suitable. For detailed comparison and guidance on choosing the right tracing tool, refer to AWS docs on choosing between ADOT and X-Ray SDK.</p>"},{"location":"guides/serverless/oss/lambda-based-observability-adot/#managing-cold-start-latency-when-using-adot","title":"Managing cold start latency when using ADOT","text":"<p>ADOT Lambda Layer for Java is agent-based, which means that when you enable auto-instrumentation, Java Agent will try to instrument all the OTel supported libraries. This will increase the Lambda function cold start latency significantly. So, we recommend that you only enable auto-instrumentation for the libraries/frameworks that are used by your application.</p> <p>To enable only specific instrumentations, you can use the following environment variables:</p> <ul> <li><code>OTEL_INSTRUMENTATION_COMMON_DEFAULT_ENABLED</code>: when set to false, disables auto-instrumentation in the Layer, requiring each instrumentation to be enabled individually.</li> <li><code>OTEL_INSTRUMENTATION_&lt;NAME&gt;_ENABLED</code>: set to true to enable auto-instrumentation for a specific library or framework. Replace  by the instrumentation that you want to enable. For the list of available instrumentations, see Suppressing specific agent instrumentation. <p>For example, to only enable auto-instrumentation for Lambda and the AWS SDK, you would set the following environment variables: <pre><code>OTEL_INSTRUMENTATION_COMMON_DEFAULT_ENABLED=false\nOTEL_INSTRUMENTATION_AWS_LAMBDA_ENABLED=true\nOTEL_INSTRUMENTATION_AWS_SDK_ENABLED=true\n</code></pre></p>"},{"location":"guides/serverless/oss/lambda-based-observability-adot/#additional-resources","title":"Additional Resources","text":"<ul> <li>OpenTelemetry</li> <li>AWS Distro for OpenTelemetry (ADOT)</li> <li>ADOT Lambda Layer</li> </ul>"},{"location":"guides/serverless/oss/lambda-based-observability-adot/#summary","title":"Summary","text":"<p>In this observability best practice guide for AWS Lambda based serverless application using Open Source technologies, we covered AWS Distro for OpenTelemetry (ADOT) and Lambda Layer and how you can use it instrument your AWS Lambda functions. We covered how you can easily enable auto-instrumentation as well as customize the ADOT collector with simple configuration to send observability signals to multiple destinations. We highlighted pros and cons of using ADOT and how it can impact cold start latency for your Lambda function and also recommended best practices to manage cold-start times. By adopting these best practices, you can instrument your applications just once to send logs, metrics and traces to multiple monitoring solutions in a vendor agnostic way.</p> <p>For further deep dive, we would highly recommend you to practice AWS managed open-source Observability module of AWS One Observability Workshop.</p>"},{"location":"guides/signal-collection/emf/","title":"CloudWatch Embedded Metric Format","text":""},{"location":"guides/signal-collection/emf/#introduction","title":"Introduction","text":"<p>CloudWatch Embedded Metric Format (EMF) enables customers to ingest complex high-cardinality application data to Amazon CloudWatch in the form of logs and generate actionable metrics. With Embedded Metric Format customers do not have to rely on complex architecture or need to use any third party tools to gain insights into their environments. Although this feature can be used in all environments, it\u2019s particularly useful in workloads that have ephemeral resources like AWS Lambda functions or containers in Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS), or Kubernetes on EC2. Embedded Metric Format lets customers easily create custom metrics without having to instrument or maintain separate code, while gaining powerful analytical capabilities on log data.</p>"},{"location":"guides/signal-collection/emf/#how-embedded-metric-format-emf-logs-work","title":"How Embedded Metric Format (EMF) logs work","text":"<p>Compute environments like Amazon EC2, On-premise Servers, containers in Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS), or Kubernetes on EC2 can generate &amp; send Embedded Metric Format (EMF) logs through the CloudWatch Agent to Amazon CloudWatch.</p> <p>AWS Lambda allows customers to easily generate custom metrics without requiring any custom code, making blocking network calls or relying on any third party software to generate and ingest Embedded Metric Format (EMF) logs to Amazon CloudWatch.</p> <p>Customers can embed custom metrics alongside detailed log event data asynchronously without requiring to provide special header declaration while publishing structured logs aligning the EMF specification. CloudWatch automatically extracts the custom metrics so that customers can visualize &amp; set alarm for real-time incident detection. The detailed log events and high-cardinality context associated with the extracted metrics can be queried using CloudWatch Logs Insights to provide deep insights into the root causes of operational events.</p> <p>The Amazon CloudWatch output plugin for Fluent Bit allows customers to ingest metrics &amp; logs data into the Amazon CloudWatch service that includes support for Embedded Metric Format (EMF).</p> <p></p>"},{"location":"guides/signal-collection/emf/#when-to-use-embedded-metric-format-emf-logs","title":"When to use Embedded Metric Format (EMF) logs","text":"<p>Traditionally, monitoring has been structured into three categories. The first category is the classic health check of an application. The second category is 'metrics', through which customers instrument their application using models like counters, timers, and gauges. The third category is 'logs', that are invaluable for the overall observability of the application. Logs provide customers continuous information about how their application is behaving. Now, customers have a way to significantly improve the way they can observe their application, without having to make sacrifices in data granularity or richness by unifying and simplifying all the instrumentation of their application while gaining incredible analytical capabilities through Embedded Metric Format (EMF) logs.</p> <p>Embedded Metric Format (EMF) logs is ideal for environments which generate high cardinality application data, that can be part of the EMF logs without having to increase metric dimensions. This still allows customers to slice and dice the application data by querying EMF logs through CloudWatch Logs Insights and CloudWatch Metrics Insights without needing to put every attribute as a metric dimension.</p> <p>Customers aggregating telemetry data from millions of Telco or IoT devices require insights into their devices performance and ability to quickly deep dive into unique telemetry that the devices report. They also need to troubleshoot problems easier &amp; faster without requiring to dig through humongous data to provide a quality service. By using Embedded Metric Format (EMF) logs customers can accomplish large scale observability by combining metrics and logs into single entity and improve troubleshooting with cost efficiency and better performance.</p>"},{"location":"guides/signal-collection/emf/#generating-embedded-metric-format-emf-logs","title":"Generating Embedded Metric Format (EMF) logs","text":"<p>The following methods can be used to generate Embedded metric format logs</p> <ol> <li> <p>Generate and send the EMF logs through an agent (like CloudWatch or Fluent-Bit or Firelens) using open-sourced client libraries.</p> </li> <li> <p>Open-sourced client libraries are available in the following languages which can be used to create EMF logs</p> <ul> <li>Node.Js</li> <li>Python</li> <li>Java</li> <li>C#</li> </ul> </li> <li> <p>EMF logs can be generated using AWS Distro for OpenTelemetry (ADOT). ADOT is a secure, production-ready, AWS-supported distribution of the OpenTelemetry project part of the Cloud Native Computing Foundation (CNCF). OpenTelemetry is an open-source initiative that provides APIs, libraries, and agents to collect distributed traces, logs and metrics for application monitoring &amp; removes boundaries and restrictions between vendor-specific formats. There are two components required for this, an OpenTelemetry compliant data source and ADOT Collector enabled for use with CloudWatch EMF logs.</p> </li> <li> <p>Manually constructed logs conforming to defined specification in JSON format, can be sent through CloudWatch agent or PutLogEvents API to CloudWatch.</p> </li> </ol>"},{"location":"guides/signal-collection/emf/#viewing-embedded-metric-format-logs-in-cloudwatch-console","title":"Viewing Embedded Metric Format logs in CloudWatch console","text":"<p>After generating the Embedded Metric Format (EMF) logs that extract metrics customers can view them in CloudWatch console under Metrics. Embedded metrics have the dimensions that are specified while generating the logs. Embedded metrics generated using client libraries have ServiceType, ServiceName, LogGroup as default dimensions.</p> <ul> <li>ServiceName: The name of the service is overridden, however for services where the name cannot be inferred (e.g. Java process running on EC2) a default value of Unknown is used if not explicitly set.</li> <li>ServiceType: The type of the service is overridden, however for services where the type cannot be inferred (e.g. Java process running on EC2) a default value of Unknown is used if not explicitly set.</li> <li>LogGroupName: Customers can optionally configure the destination log group that metrics should be delivered to, for agent-based platforms. This value is passed from the library to the agent in the Embedded Metric payload. If a LogGroup is not provided, the default value will be derived from the service name: -metrics</li> <li>LogStreamName: Customers can optionally configure the destination log stream that metrics should be delivered to, for agent-based platforms. This value will be passed from the library to the agent in the Embedded Metric payload. If a LogStreamName is not provided, the default value will be derived by the agent (this will likely be the hostname).</li> <li>NameSpace: Overrides the CloudWatch namespace. If not set, a default value of aws-embedded-metrics is used.</li> </ul> <p>A sample EMF logs looks like below in the CloudWatch Console logs</p> <pre><code>2023-05-19T15:20:39.391Z 238196b6-c8da-4341-a4b7-0c322e0ef5bb INFO\n{\n    \"LogGroup\": \"emfTestFunction\",\n    \"ServiceName\": \"emfTestFunction\",\n    \"ServiceType\": \"AWS::Lambda::Function\",\n    \"Service\": \"Aggregator\",\n    \"AccountId\": \"XXXXXXXXXXXX\",\n    \"RequestId\": \"422b1569-16f6-4a03-b8f0-fe3fd9b100f8\",\n    \"DeviceId\": \"61270781-c6ac-46f1-baf7-22c808af8162\",\n    \"Payload\": {\n        \"sampleTime\": 123456789,\n        \"temperature\": 273,\n        \"pressure\": 101.3\n    },\n    \"executionEnvironment\": \"AWS_Lambda_nodejs18.x\",\n    \"memorySize\": \"256\",\n    \"functionVersion\": \"$LATEST\",\n    \"logStreamId\": \"2023/05/19/[$LATEST]f3377848231140c185570caa9f97abc8\",\n    \"_aws\": {\n        \"Timestamp\": 1684509639390,\n        \"CloudWatchMetrics\": [\n            {\n                \"Dimensions\": [\n                    [\n                        \"LogGroup\",\n                        \"ServiceName\",\n                        \"ServiceType\",\n                        \"Service\"\n                    ]\n                ],\n                \"Metrics\": [\n                    {\n                        \"Name\": \"ProcessingLatency\",\n                        \"Unit\": \"Milliseconds\"\n                    }\n                ],\n                \"Namespace\": \"aws-embedded-metrics\"\n            }\n        ]\n    },\n    \"ProcessingLatency\": 100\n}\n</code></pre> <p>For the same EMF log, the extracted metrics looks like below, which can be queried in CloudWatch Metrics.</p> <p></p> <p>Customers can query the detailed log events associated with the extracted metrics using CloudWatch Logs Insights to get deep insights into the root causes of operational events. One of the benefits of extracting metrics from EMF logs is that the customers can filter logs by the unique metric (metric name plus unique dimension set) and metric values, to get context on the events that contributed to the aggregated metric value.</p> <p>For the same EMF logs discussed above, an example query having ProcessingLatency as a metric and Service as a dimension to get an impacted request id or device id is shown below as sample query in CloudWatch Logs Insights.</p> <pre><code>filter ProcessingLatency &lt; 200 and Service = \"Aggregator\"\n| fields @requestId, @ingestionTime, @DeviceId\n</code></pre> <p></p>"},{"location":"guides/signal-collection/emf/#alarms-on-metrics-created-with-emf-logs","title":"Alarms on metrics created with EMF logs","text":"<p>Creating alarms on metrics generated by EMF follows the same pattern as creating alarms on any other metrics. The key thing to note here is, EMF metric generation depends on log publishing flow, because the CloudWatch Logs process the EMF logs &amp; transform the metrics. So it\u2019s important to publish logs in a timely manner so that the metric datapoints are created within the period of time in which alarms are evaluated.</p> <p>For the same EMF logs discussed above, an example alarm is created and shown below using the ProcessingLatency metric as a datapoint with a threshold.</p> <p></p>"},{"location":"guides/signal-collection/emf/#latest-features-of-emf-logs","title":"Latest features of EMF Logs","text":"<p>Customers can send EMF logs to CloudWatch Logs using PutLogEvents API and may optionally include the HTTP header <code>x-amzn-logs-format: json/emf</code> to instruct CloudWatch Logs that the metrics should be extracted, it is no longer necessary.</p> <p>Amazon CloudWatch supports high resolution metric extraction with up to 1 second granularity from structured logs using Embedded Metric Format (EMF). Customers can provide an optional StorageResolution parameter within EMF specification logs with a value of 1 or 60 (default) to indicate the desired resolution (in seconds) of the metric. Customers can publish both standard resolution (60 seconds) and high resolution (1 second) metrics via EMF, enabling granular visibility into their applications\u2019 health and performance.</p> <p>Amazon CloudWatch provides enhanced visibility into errors in Embedded Metric Format (EMF) with two error metrics (EMFValidationErrors &amp; EMFParsingErrors). This enhanced visibility helps customers quickly identify and remediate errors when leveraging EMF, thereby simplifying the instrumentation process .</p> <p>With the increased complexity of managing modern applications, customers need more flexibility when defining and analyzing custom metrics. Hence the maximum number of metric dimensions has been increased from 10 to 30. Customers can create custom metrics using EMF logs with up to 30 dimensions.</p>"},{"location":"guides/signal-collection/emf/#additional-references","title":"Additional References:","text":"<ul> <li>One Observability Workshop on Embedded Metric Format with an AWS Lambda function sample using NodeJS Library.</li> <li>Serverless Observability Workshop on Async metrics using Embedded Metrics Format (EMF)</li> <li>Java code sample using PutLogEvents API to send EMF logs to CloudWatch Logs</li> <li>Blog article: Lowering costs and focusing on our customers with Amazon CloudWatch embedded custom metrics</li> </ul>"},{"location":"recipes/","title":"Recipes","text":"<p>In here you will find curated guidance, how-to's, and links to other resources that help with the application of observability (o11y) to various use cases. This includes managed services such as Amazon Managed Service for Prometheus and Amazon Managed Grafana as well as agents, for example OpenTelemetry and Fluent Bit. Content here is not resitricted to AWS tools alone though, and many open source projects are referenced here.</p> <p>We want to address the needs of both developers and infrastructure folks equally, so many of the recipes \"cast a wide net\". We encourge you to explore and find the solutions that work best for what you are seeking to accomplish.</p> <p>Info</p> <p>The content here is derived from actual customer engagement by our Solutions Architects, Professional Services, and feedback from other customers. Everything you will find here has been implemented by our actual customers in their own environments.</p> <p>The way we think about the o11y space is as follows: we decompose it into six dimensions you can then combine to arrive at a specific solution:</p> dimension examples Destinations Prometheus \u00b7 Grafana \u00b7 OpenSearch \u00b7 CloudWatch \u00b7 Jaeger Agents ADOT \u00b7 Fluent Bit \u00b7 CW agent \u00b7 X-Ray agent Languages Java \u00b7 Python \u00b7 .NET \u00b7 JavaScript \u00b7 Go \u00b7 Rust Infra &amp; databases RDS \u00b7 DynamoDB \u00b7 MSK Compute unit Batch \u00b7 ECS \u00b7 EKS \u00b7 AEB \u00b7 Lambda \u00b7 AppRunner Compute engine Fargate \u00b7 EC2 \u00b7 Lightsail <p>Example solution requirement</p> <p>I need a logging solution for a Python app I'm running on EKS on Fargate with the goal to store the logs in an S3 bucket for further consumption</p> <p>One stack that would fit this need is the following:</p> <ol> <li>Destination: An S3 bucket for further consumption of data</li> <li>Agent: FluentBit to emit log data from EKS</li> <li>Language: Python</li> <li>Infra &amp; DB: N/A</li> <li>Compute unit: Kubernetes (EKS)</li> <li>Compute engine: EC2</li> </ol> <p>Not every dimension needs to be specified and sometimes it's hard to decide where to start. Try different paths and compare the pros and cons of certain recipes.</p> <p>To simplify navigation, we're grouping the six dimension into the following categories:</p> <ul> <li>By Compute: covering compute engines and units</li> <li>By Infra &amp; Data: covering infrastructure and databases</li> <li>By Language: covering languages</li> <li>By Destination: covering telemetry and analytics</li> <li>Tasks: covering anomaly detection, alerting, troubleshooting, and more</li> </ul> <p>Learn more about dimensions \u2026</p>"},{"location":"recipes/#how-to-use","title":"How to use","text":"<p>You can either use the top navigation menu to browse to a specific index page, starting with a rough selection. For example, <code>By Compute</code> -&gt; <code>EKS</code> -&gt; <code>Fargate</code> -&gt; <code>Logs</code>.</p> <p>Alternatively, you can search the site pressing <code>/</code> or the <code>s</code> key:</p> <p></p> <p>License</p> <p>All recipes published on this site are available via the MIT-0 license, a modification to the usual MIT license that removes the requirement for attribution.</p>"},{"location":"recipes/#how-to-contribute","title":"How to contribute","text":"<p>Start a discussion on what you plan to do and we take it from there.</p>"},{"location":"recipes/#learn-more","title":"Learn more","text":"<p>The recipes on this site are a good practices collection. In addition, there are a number of places where you can learn more about the status of open source projects we use as well as about the managed services from the recipes, so check out:</p> <ul> <li>observability @ aws, a playlist of AWS folks talking about   their projects and services.</li> <li>AWS observability workshops, to try out the offerings in a   structured manner.</li> <li>The AWS monitoring and observability homepage with pointers   to case studies and partners.</li> </ul>"},{"location":"recipes/aes/","title":"Amazon OpenSearch Service","text":"<p>Amazon OpenSearch Service (AOS), successor to Amazon Elasticsearch Service, makes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is an open source, distributed  search and analytics suite derived from Elasticsearch. It offers the latest  versions of OpenSearch, support for 19 versions of Elasticsearch (1.5 to 7.10 versions), and visualization capabilities powered by OpenSearch Dashboards and Kibana  (1.5 to 7.10 versions). </p> <p>Check out the following recipes:</p> <ul> <li>AOS tutorial: a quick start guide</li> <li>Get started with AOS: T-shirt-size your domain</li> <li>Getting started with AOS</li> <li>Log Analytics with AOS</li> <li>Getting started with Open Distro for Elasticsearch</li> <li>Know your data with Machine Learning</li> <li>Send CloudTrail Logs to AOS</li> <li>Searching DynamoDB Data with AOS</li> <li>Getting Started with Trace Analytics in AOS</li> </ul>"},{"location":"recipes/alerting/","title":"Alerting","text":"<p>This section has a selection of recipes for various alerting systems and scenarios.</p> <ul> <li>Build proactive database monitoring for RDS with CW Logs, Lambda, and SNS</li> </ul>"},{"location":"recipes/amg/","title":"Amazon Managed Grafana","text":"<p>Amazon Managed Grafana is a fully managed service based on open  source Grafana, enabling you to analyze your metrics, logs, and traces without having to provision servers, configure and update software, or do the heavy  lifting involved in securing and scaling Grafana in production. You can create, explore, and share observability dashboards with your team, connecting to multiple data sources.</p> <p>Check out the following recipes:</p>"},{"location":"recipes/amg/#basics","title":"Basics","text":"<ul> <li>Getting Started</li> <li>Using Terraform for automation</li> </ul>"},{"location":"recipes/amg/#authentication-and-access-control","title":"Authentication and Access Control","text":"<ul> <li>Direct SAML integration with identity providers</li> <li>Integrating identity providers (OneLogin, Ping Identity, Okta, and Azure AD) to SSO</li> <li>Integrating Google authentication via SAMLv2</li> <li>Setting up Amazon Managed Grafana cross-account data source using customer managed IAM roles</li> <li>Fine-grained access control in Amazon Managed Grafana using Grafana Teams</li> </ul>"},{"location":"recipes/amg/#data-sources-and-visualizations","title":"Data sources and Visualizations","text":"<ul> <li>Using Athena in Amazon Managed Grafana</li> <li>Using Redshift in Amazon Managed Grafana</li> <li>Viewing custom metrics from statsd with Amazon Managed Service for Prometheus and Amazon Managed Grafana</li> <li>Setting up cross-account data source using customer managed IAM roles</li> </ul>"},{"location":"recipes/amg/#others","title":"Others","text":"<ul> <li>Monitoring hybrid environments</li> <li>Managing Grafana and Loki in a regulated multitenant environment</li> <li>Monitoring Amazon EKS Anywhere using Amazon Managed Service for Prometheus and Amazon Managed Grafana</li> <li>Workshop for Getting Started</li> </ul>"},{"location":"recipes/amp/","title":"Amazon Managed Service for Prometheus","text":"<p>Amazon Managed Service for Prometheus (AMP) is a Prometheus-compatible monitoring service that makes it easy to monitor containerized applications at scale.  With AMP, you can use the Prometheus query language (PromQL) to monitor the performance of containerized workloads without having to manage the underlying  infrastructure required to manage the ingestion, storage, and querying of operational metrics.</p> <p>Check out the following recipes:</p> <ul> <li>Getting Started with AMP</li> <li>Using ADOT in EKS on EC2 to ingest to AMP and visualize in AMG</li> <li>Setting up cross-account ingestion into AMP</li> <li>Metrics collection from ECS using AMP</li> <li>Configuring Grafana Cloud Agent for AMP</li> <li>Set up cross-region metrics collection for AMP workspaces</li> <li>Best practices for migrating self-hosted Prometheus on EKS to AMP</li> <li>Workshop for Getting Started with AMP</li> <li>Exporting Cloudwatch Metric Streams via Firehose and AWS Lambda to Amazon Managed Service for Prometheus</li> <li>Terraform as Infrastructure as a Code to deploy Amazon Managed Service for Prometheus and configure Alert manager</li> <li>Monitor Istio on EKS using Amazon Managed Prometheus and Amazon Managed Grafana</li> <li>Monitoring Amazon EKS Anywhere using Amazon Managed Service for Prometheus and Amazon Managed Grafana</li> <li>Introducing Amazon EKS Observability Accelerator</li> <li>Installing the Prometheus mixin dashboards with AMP and Amazon Managed Grafana</li> <li>Auto-scaling Amazon EC2 using Amazon Managed Service for Prometheus and alert manager</li> </ul>"},{"location":"recipes/anomaly-detection/","title":"Anomaly Detection","text":"<p>This section contains recipes for anomaly detection.</p> <ul> <li>Enabling Anomaly Detection for a CloudWatch Metric</li> </ul>"},{"location":"recipes/apprunner/","title":"AWS App Runner","text":"<p>AWS App Runner is a fully managed service that makes it easy for developers to quickly deploy containerized web applications and APIs, at scale and with no prior infrastructure experience required. Start with your source code or a container image. App Runner builds and deploys the web application automatically, load balances traffic with encryption, scales to meet your traffic needs, and makes it easy for your services to communicate with other AWS services and applications that run in a private Amazon VPC. With App Runner, rather than thinking about servers or scaling, you have more time to focus on your applications.</p> <p>Check out the following recipes:</p>"},{"location":"recipes/apprunner/#general","title":"General","text":"<ul> <li>Container Day - Docker Con | How Developers can get to production web applications at scale easily</li> <li>AWS Blog | Centralized observability for AWS App Runner services</li> <li>AWS Blog | Observability for AWS App Runner VPC networking</li> <li>AWS Blog | Controlling and monitoring AWS App Runner applications with Amazon EventBridge</li> </ul>"},{"location":"recipes/apprunner/#logs","title":"Logs","text":"<ul> <li>Viewing App Runner logs streamed to CloudWatch Logs</li> </ul>"},{"location":"recipes/apprunner/#metrics","title":"Metrics","text":"<ul> <li>Viewing App Runner service metrics reported to CloudWatch</li> </ul>"},{"location":"recipes/apprunner/#traces","title":"Traces","text":"<ul> <li>Getting Started with AWS X-Ray tracing for App Runner using AWS Distro for OpenTelemetry</li> <li>Containers from the Couch | AWS App Runner X-Ray Integration</li> <li>AWS Blog | Tracing an AWS App Runner service using AWS X-Ray with OpenTelemetry</li> <li>AWS Blog | Enabling AWS X-Ray tracing for AWS App Runner service using AWS Copilot CLI</li> </ul>"},{"location":"recipes/cw/","title":"Amazon CloudWatch","text":"<p>Amazon CloudWatch (CW) is a monitoring and observability service built  for DevOps engineers, developers, site reliability engineers (SREs), and IT managers. CloudWatch collects monitoring and operational data in the form of logs, metrics,  and events, providing you with a unified view of AWS resources, applications,  and services that run on AWS and on-premises servers.</p> <p>Check out the following recipes:</p> <ul> <li>Build proactive database monitoring for RDS with CW Logs, Lambda, and SNS</li> <li>Implementing CloudWatch-centric observability for Kubernetes-native developers in EKS</li> <li>Create Canaries via CW Synthetics</li> <li>Cloudwatch Logs Insights for Quering Logs</li> <li>Lambda Insights</li> <li>Anomaly Detection via CloudWatch</li> <li>Metrics Alarms via CloudWatch</li> <li>Choosing container logging options to avoid backpressure</li> <li>Introducing CloudWatch Container Insights Prometheus Support with AWS Distro for OpenTelemetry on ECS and EKS</li> <li>Monitoring ECS containerized Applications and Microservices using CW Container Insights</li> <li>Monitoring EKS containerized Applications and Microservices using CW Container Insights</li> <li>Exporting Cloudwatch Metric Streams via Firehose and AWS Lambda to Amazon Managed Service for Prometheus</li> <li>Proactive autoscaling of Kubernetes workloads with KEDA and Amazon CloudWatch</li> <li>Using Amazon CloudWatch Metrics explorer to aggregate and visualize metrics filtered by resource tags</li> </ul>"},{"location":"recipes/dimensions/","title":"Dimensions","text":"<p>In the context of this site we consider the o11y space along six dimensions. Looking at each dimension independently is beneficial from an synthetic point-of-view, that is, when you're trying to build out a concrete o11y solution for a given workload, spanning developer-related aspects such as the programming language used as well as operational topics, for example the runtime environment like containers or Lambda functions.</p> <p></p> <p>What is a signal?</p> <p>When we say signal here we mean any kinds of o11y data and metadata points, including log entries, metrics, and traces. Unless we want to or have to be more specific, we use \"signal\" and it should be clear from the context what restrictions may apply.</p> <p>Let's now have a look at each of the six dimensions one by one:</p>"},{"location":"recipes/dimensions/#destinations","title":"Destinations","text":"<p>In this dimension we consider all kinds of signal destinations including long term storage and graphical interfaces that let you consume signals. As a developer, you want access to an UI or an API that allows you to discover, look up, and correlate signals to troubleshoot your service. In an infrastructure or platform role you want access to an UI or an API that allows you to manage, discover, look up, and correlate signals to understand the state of the infrastructure.</p> <p></p> <p>Ultimately, this is the most interesting dimension from a human point of view. However, in order to be able to reap the benefits we first have to invest a bit of work: we need to instrument our software and external dependencies and ingest the signals into the destinations.</p> <p>So, how do the signals arrive in the destinations? Glad you asked, it's \u2026</p>"},{"location":"recipes/dimensions/#agents","title":"Agents","text":"<p>How the signals are collected and routed to analytics. The signals can come  from two sources: either your application source code (see also the language section) or from things your application depends on,  such as state managed in datastores as well as infrastructure like VPCs (see also the infra &amp; data section).</p> <p>Agents are part of the telemetry that you would use to collect and ingest signals. The other part are the instrumented applications and infra pieces like databases.</p>"},{"location":"recipes/dimensions/#languages","title":"Languages","text":"<p>This dimension is concerned with the programming language you use for writing your service or application. Here, we're dealing with SDKs and libraries, such  as the X-Ray SDKs or what OpenTelemetry provides in the context of instrumentation. You want to make sure that an o11y solution supports your programming language of choice for a given signal type such as logs or metrics.</p>"},{"location":"recipes/dimensions/#infrastructure-databases","title":"Infrastructure &amp; databases","text":"<p>With this dimension we mean any sort of application-external dependencies,  be it infrastructure like the VPC the service is running in or a datastore like RDS or DynamoDB or a queue like SQS. </p> <p>Commonalities</p> <p>One thing all the sources in this dimension have in common is that they are located outside of your application (as well as the compute environment your app runs in) and with that you have to treat them as an opaque box.</p> <p>This dimension includes but is not limited to:</p> <ul> <li>AWS infrastructure, for example VPC flow logs.</li> <li>Secondary APIs such as Kubernetes control plane logs.</li> <li>Signals from datastores, such as or S3, RDS or SQS.</li> </ul>"},{"location":"recipes/dimensions/#compute-unit","title":"Compute unit","text":"<p>The way your package, schedule, and run your code. For example, in Lambda that's a function and in ECS and EKS that unit is a container running in a tasks (ECS) or pods (EKS), respectively. Containerized environments like Kubernetes often allow for two options concerning telemetry deployments: as side cars or as per-node (instance) daemon processes.</p>"},{"location":"recipes/dimensions/#compute-engine","title":"Compute engine","text":"<p>This dimension refers to the base runtime environment, which may (in case of an EC2 instance, for example) or may not (serverless offerings such as Fargate or Lambda) be your responsibility to provision and patch. Depending on the compute engine you use, the telemetry part might already be part of the offering, for example, EKS on Fargate has log routing via Fluent Bit integrated.</p>"},{"location":"recipes/dynamodb/","title":"Amazon DynamoDB","text":"<p>Amazon DynamoDB is a key-value and document database that delivers  single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-active, durable database with built-in security, backup and  restore, and in-memory caching for internet-scale applications. </p> <p>Check out the following recipes:</p> <ul> <li>Monitoring Amazon DynamoDB for operational awareness</li> <li>Searching DynamoDB data with Amazon Elasticsearch Service</li> <li>DynamoDB Contributor Insights</li> </ul>"},{"location":"recipes/ecs/","title":"Amazon Elastic Container Service","text":"<p>Amazon Elastic Container Service (ECS) is a fully managed container orchestration service that helps you easily deploy, manage, and scale  containerized applications, deeply integrating with the rest of AWS.</p> <p>Check out the following recipes, grouped by compute engine:</p>"},{"location":"recipes/ecs/#general","title":"General","text":"<ul> <li>Deployment patterns for the AWS Distro for OpenTelemetry Collector with ECS</li> <li>Simplifying Amazon ECS monitoring set up with AWS Distro for OpenTelemetry</li> </ul>"},{"location":"recipes/ecs/#ecs-on-ec2","title":"ECS on EC2","text":""},{"location":"recipes/ecs/#logs","title":"Logs","text":"<ul> <li>Under the hood: FireLens for Amazon ECS Tasks</li> </ul>"},{"location":"recipes/ecs/#metrics","title":"Metrics","text":"<ul> <li>Using AWS Distro for OpenTelemetry collector for cross-account metrics collection on Amazon ECS</li> <li>Metrics collection from ECS using Amazon Managed Service for Prometheus</li> <li>Sending Envoy metrics from AWS App Mesh to Amazon CloudWatch</li> </ul>"},{"location":"recipes/ecs/#ecs-on-fargate","title":"ECS on Fargate","text":""},{"location":"recipes/ecs/#logs_1","title":"Logs","text":"<ul> <li>Sample logging architectures for FireLens on Amazon ECS and AWS Fargate using Fluent Bit</li> </ul>"},{"location":"recipes/eks/","title":"Amazon Elastic Kubernetes Service","text":"<p>Amazon Elastic Kubernetes Service (EKS) gives you the flexibility to start, run, and scale Kubernetes applications in the AWS Cloud or on-premises.</p> <p>Check out the following recipes, grouped by compute engine:</p>"},{"location":"recipes/eks/#eks-on-ec2","title":"EKS on EC2","text":""},{"location":"recipes/eks/#logs","title":"Logs","text":"<ul> <li>Fluent Bit Integration in CloudWatch Container Insights for EKS</li> <li>Logging with EFK Stack</li> <li>Sample logging architectures for Fluent Bit and FluentD on EKS</li> </ul>"},{"location":"recipes/eks/#metrics","title":"Metrics","text":"<ul> <li>Getting Started with Amazon Managed Service for Prometheus</li> <li>Using ADOT in EKS on EC2 to ingest metrics to AMP and visualize in AMG</li> <li>Configuring Grafana Cloud Agent for Amazon Managed Service for Prometheus</li> <li>Monitoring cluster using Prometheus and Grafana</li> <li>Monitoring with Managed Prometheus and Managed Grafana</li> <li>CloudWatch Container Insights</li> <li>Set up cross-region metrics collection for AMP workspaces</li> <li>Monitoring App Mesh environment on EKS using Amazon Managed Service for Prometheus</li> <li>Monitor Istio on EKS using Amazon Managed Prometheus and Amazon Managed Grafana</li> <li>Proactive autoscaling of Kubernetes workloads with KEDA and Amazon CloudWatch</li> <li>Monitoring Amazon EKS Anywhere using Amazon Managed Service for Prometheus and Amazon Managed Grafana</li> </ul>"},{"location":"recipes/eks/#traces","title":"Traces","text":"<ul> <li>Migrating X-Ray tracing to AWS Distro for OpenTelemetry</li> <li>Tracing with X-Ray</li> </ul>"},{"location":"recipes/eks/#eks-on-fargate","title":"EKS on Fargate","text":""},{"location":"recipes/eks/#logs_1","title":"Logs","text":"<ul> <li>Fluent Bit for Amazon EKS on AWS Fargate is here</li> <li>Sample logging architectures for Fluent Bit and FluentD on EKS</li> </ul>"},{"location":"recipes/eks/#metrics_1","title":"Metrics","text":"<ul> <li>Using ADOT in EKS on Fargate to ingest metrics to AMP and visualize in AMG</li> <li>CloudWatch Container Insights</li> <li>Set up cross-region metrics collection for AMP workspaces</li> </ul>"},{"location":"recipes/eks/#traces_1","title":"Traces","text":"<ul> <li>Using ADOT in EKS on Fargate with AWS X-Ray</li> <li>Tracing with X-Ray</li> </ul>"},{"location":"recipes/infra/","title":"Infrastructure &amp; Databases","text":""},{"location":"recipes/infra/#networking","title":"Networking","text":"<ul> <li>Monitor your Application Load Balancers</li> <li>Monitor your Network Load Balancers</li> <li>VPC Flow Logs</li> <li>VPC Flow logs analysis using Amazon Elasticsearch Service</li> </ul>"},{"location":"recipes/infra/#compute","title":"Compute","text":"<ul> <li>Amazon EKS control plane logging</li> <li>AWS Lambda monitoring and observability</li> </ul>"},{"location":"recipes/infra/#databases-storage-and-queues","title":"Databases, storage and queues","text":"<ul> <li>Amazon Relational Database Service</li> <li>Amazon DynamoDB</li> <li>Amazon Managed Streaming for Apache Kafka</li> <li>Logging and monitoring in Amazon S3</li> <li>Amazon SQS and AWS X-Ray</li> </ul>"},{"location":"recipes/infra/#others","title":"Others","text":"<ul> <li>Prometheus exporters</li> </ul>"},{"location":"recipes/java/","title":"Java","text":"<ul> <li>StatsD and Java Support in AWS Distro for OpenTelemetry</li> </ul>"},{"location":"recipes/lambda/","title":"AWS Lambda","text":"<p>AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers, creating workload-aware cluster  scaling logic, maintaining event integrations, or managing runtimes.</p> <p>Check out the following recipes:</p>"},{"location":"recipes/lambda/#logs","title":"Logs","text":"<ul> <li>Deploy and Monitor a Serverless Application</li> </ul>"},{"location":"recipes/lambda/#metrics","title":"Metrics","text":"<ul> <li>Introducing CloudWatch Lambda Insights</li> <li>Exporting Cloudwatch Metric Streams via Firehose and AWS Lambda to Amazon Managed Service for Prometheus</li> </ul>"},{"location":"recipes/lambda/#traces","title":"Traces","text":"<ul> <li>Auto-instrumenting a Python application with an AWS Distro for OpenTelemetry Lambda layer</li> <li>Tracing AWS Lambda functions in AWS X-Ray with OpenTelemetry</li> </ul>"},{"location":"recipes/msk/","title":"Amazon Managed Streaming for Apache Kafka","text":"<p>Amazon Managed Streaming for Apache Kafka (MSK) is a fully managed service that makes it  easy for you to build and run applications that use Apache Kafka to process  streaming data. Amazon MSK continuously monitors cluster health and automatically replaces unhealthy nodes with no downtime to your application. In addition,  Amazon MSK secures your Apache Kafka cluster by encrypting data at rest.</p> <p>Check out the following recipes:</p> <ul> <li>Amazon Managed Streaming for Apache Kafka: Open Monitoring with Prometheus</li> </ul>"},{"location":"recipes/nodejs/","title":"Node.js","text":"<ul> <li>NodeJS library to generate embedded CloudWatch metrics</li> </ul>"},{"location":"recipes/rds/","title":"Amazon Relational Database Service","text":"<p>Amazon Relational Database Service (RDS) makes it easy to set up,  operate, and scale a relational database in the cloud. It provides cost-efficient  and resizable capacity while automating time-consuming administration tasks such  as hardware provisioning, database setup, patching and backups.</p> <p>Check out the following recipes:</p> <ul> <li>Build proactive database monitoring for RDS with CloudWatch Logs, Lambda, and SNS</li> <li>Monitor RDS for PostgreSQL and Aurora for PostgreSQL database log errors and set up notifications using CloudWatch</li> <li>Logging and monitoring in Amazon RDS</li> <li>Performance Insights metrics published to CloudWatch</li> </ul>"},{"location":"recipes/telemetry/","title":"Telemetry","text":"<p>Telemetry is all about how the signals are collected from various sources, including your own app and infrastructure and routed to destinations where they are consumed:</p> <p></p> <p>Success</p> <p>See the Data types section for a detailed breakdown of the best practices for each type of telemetry.</p> <p>Let's further dive into the concepts introduced in above figure.</p>"},{"location":"recipes/telemetry/#sources","title":"Sources","text":"<p>We consider sources as something where signals come from. There are two types of sources:</p> <ol> <li>Things under your control, that is, the application source code, via instrumentation.</li> <li>Everything else you may use, such as managed services, not under your (direct) control.    These types of sources are typically provided by AWS, exposing signals via an API.</li> </ol>"},{"location":"recipes/telemetry/#agents","title":"Agents","text":"<p>In order to transport signals from the sources to the destinations, you need some sort of intermediary we call agent. These agents receive or pull signals  from the sources and, typically via configuration, determine where signals  shoud go, optionally supporting filtering and aggregation.</p> <p>Agents? Routing? Shipping? Ingesting?</p> <p>There are many terms out there people use to refer to the process of getting the signals from sources to destinations including routing, shipping, aggregation, ingesting etc. and while they may mean slightly  different things, we will use them here interchangeably. Canonically,  we will refer to those intermediary transport components as agents.</p>"},{"location":"recipes/telemetry/#destinations","title":"Destinations","text":"<p>Where signals end up, for consumption. No matter if you want to store signals for later consumption, if you want to dashboard them, set an alert if a certain condition is true, or correlate signals. All of those components that serve you as the end-user are destinations.</p>"},{"location":"recipes/troubleshooting/","title":"Troubleshooting","text":"<p>We include troubleshooting recipes for various situations and dimensions in this section.</p> <ul> <li>Troubleshooting performance bottleneck in DynamoDB</li> </ul>"},{"location":"recipes/workshops/","title":"Workshops","text":"<p>This section contains workshops to which you can return for samples and demonstrations around o11y systems and tooling.</p> <ul> <li>One Observability Workshop</li> <li>EKS Workshop</li> <li>ECS Workshop</li> <li>App Runner Workshop</li> </ul>"},{"location":"recipes/recipes/amg-athena-plugin/","title":"Using Athena in Amazon Managed Grafana","text":"<p>In this recipe we show you how to use Amazon Athena\u2014a serverless,  interactive query service allowing you to analyze data in Amazon S3 using  standard SQL\u2014in Amazon Managed Grafana. This integration is enabled by the Athena data source for Grafana, an open source plugin available for you to use in any DIY Grafana instance as well as  pre-installed in Amazon Managed Grafana.</p> <p>Note</p> <p>This guide will take approximately 20 minutes to complete.</p>"},{"location":"recipes/recipes/amg-athena-plugin/#prerequisites","title":"Prerequisites","text":"<ul> <li>The AWS CLI is installed and configured in your environment.</li> <li>You have access to Amazon Athena from your account.</li> </ul>"},{"location":"recipes/recipes/amg-athena-plugin/#infrastructure","title":"Infrastructure","text":"<p>Let's first set up the necessary infrastructure.</p>"},{"location":"recipes/recipes/amg-athena-plugin/#set-up-amazon-athena","title":"Set up Amazon Athena","text":"<p>We want to see how to use Athena in two different scenarios: one scenario around geographical data along with the Geomap plugin, and one in a security-relevant scenario around VPC flow logs.</p> <p>First, let's make sure Athena is set up and the datasets are loaded.</p> <p>Warning</p> <p>You have to use the Amazon Athena console to execute these queries. Grafana in general has read-only access to the data sources, so can not be used to create or update data.</p>"},{"location":"recipes/recipes/amg-athena-plugin/#load-geographical-data","title":"Load geographical data","text":"<p>In this first use case we use a dataset from the Registry of Open Data on AWS. More specifically, we will use OpenStreetMap (OSM) to demonstrate the usage of the Athena plugin for a geographical data motivated use case. For that to work, we need to first get the OSM data into Athena.</p> <p>So, first off, create a new database in Athena. Go to the Athena console and there use the following three  SQL queries to import the OSM data into the database.</p> <p>Query 1:</p> <pre><code>CREATE EXTERNAL TABLE planet (\n  id BIGINT,\n  type STRING,\n  tags MAP&lt;STRING,STRING&gt;,\n  lat DECIMAL(9,7),\n  lon DECIMAL(10,7),\n  nds ARRAY&lt;STRUCT&lt;ref: BIGINT&gt;&gt;,\n  members ARRAY&lt;STRUCT&lt;type: STRING, ref: BIGINT, role: STRING&gt;&gt;,\n  changeset BIGINT,\n  timestamp TIMESTAMP,\n  uid BIGINT,\n  user STRING,\n  version BIGINT\n)\nSTORED AS ORCFILE\nLOCATION 's3://osm-pds/planet/';\n</code></pre> <p>Query 2:</p> <pre><code>CREATE EXTERNAL TABLE planet_history (\n    id BIGINT,\n    type STRING,\n    tags MAP&lt;STRING,STRING&gt;,\n    lat DECIMAL(9,7),\n    lon DECIMAL(10,7),\n    nds ARRAY&lt;STRUCT&lt;ref: BIGINT&gt;&gt;,\n    members ARRAY&lt;STRUCT&lt;type: STRING, ref: BIGINT, role: STRING&gt;&gt;,\n    changeset BIGINT,\n    timestamp TIMESTAMP,\n    uid BIGINT,\n    user STRING,\n    version BIGINT,\n    visible BOOLEAN\n)\nSTORED AS ORCFILE\nLOCATION 's3://osm-pds/planet-history/';\n</code></pre> <p>Query 3:</p> <pre><code>CREATE EXTERNAL TABLE changesets (\n    id BIGINT,\n    tags MAP&lt;STRING,STRING&gt;,\n    created_at TIMESTAMP,\n    open BOOLEAN,\n    closed_at TIMESTAMP,\n    comments_count BIGINT,\n    min_lat DECIMAL(9,7),\n    max_lat DECIMAL(9,7),\n    min_lon DECIMAL(10,7),\n    max_lon DECIMAL(10,7),\n    num_changes BIGINT,\n    uid BIGINT,\n    user STRING\n)\nSTORED AS ORCFILE\nLOCATION 's3://osm-pds/changesets/';\n</code></pre>"},{"location":"recipes/recipes/amg-athena-plugin/#load-vpc-flow-logs-data","title":"Load VPC flow logs data","text":"<p>The second use case is a security-motivated one: analyzing network traffic using VPC Flow Logs.</p> <p>First, we need to tell EC2 to generate VPC Flow Logs for us. So, if you have  not done this already, you go ahead now and create VPC flow logs  either on the network interfaces level, subnet level, or VPC level.</p> <p>Note</p> <p>To improve query performance and minimize the storage footprint, we store the VPC flow logs in Parquet, a columnar storage format that supports nested data.</p> <p>For our setup it doesn't matter what option you choose (network interfaces,  subnet, or VPC), as long as you publish them to an S3 bucket in Parquet format as shown below:</p> <p></p> <p>Now, again via the Athena console, create the table for the VPC flow logs data in the same database you imported the OSM data, or create a new one, if you prefer to do so.</p> <p>Use the following SQL query and make sure that you're replacing <code>VPC_FLOW_LOGS_LOCATION_IN_S3</code> with your own bucket/folder:</p> <pre><code>CREATE EXTERNAL TABLE vpclogs (\n  `version` int, \n  `account_id` string, \n  `interface_id` string, \n  `srcaddr` string, \n  `dstaddr` string, \n  `srcport` int, \n  `dstport` int, \n  `protocol` bigint, \n  `packets` bigint, \n  `bytes` bigint, \n  `start` bigint, \n  `end` bigint, \n  `action` string, \n  `log_status` string, \n  `vpc_id` string, \n  `subnet_id` string, \n  `instance_id` string, \n  `tcp_flags` int, \n  `type` string, \n  `pkt_srcaddr` string, \n  `pkt_dstaddr` string, \n  `region` string, \n  `az_id` string, \n  `sublocation_type` string, \n  `sublocation_id` string, \n  `pkt_src_aws_service` string, \n  `pkt_dst_aws_service` string, \n  `flow_direction` string, \n  `traffic_path` int\n)\nSTORED AS PARQUET\nLOCATION 'VPC_FLOW_LOGS_LOCATION_IN_S3'\n</code></pre> <p>For example, <code>VPC_FLOW_LOGS_LOCATION_IN_S3</code> could look something like the following if you're using the S3 bucket <code>allmyflowlogs</code>:</p> <pre><code>s3://allmyflowlogs/AWSLogs/12345678901/vpcflowlogs/eu-west-1/2021/\n</code></pre> <p>Now that the datasets are available in Athena, let's move on to Grafana.</p>"},{"location":"recipes/recipes/amg-athena-plugin/#set-up-grafana","title":"Set up Grafana","text":"<p>We need a Grafana instance, so go ahead and set up a new Amazon Managed Grafana workspace, for example by using the Getting Started guide, or use an existing one.</p> <p>Warning</p> <p>To use AWS data source configuration, first go to the Amazon Managed Grafana console to enable service-mananged IAM roles that grant the workspace the  IAM policies necessary to read the Athena resources. Further, note the following:</p> <ol> <li>The Athena workgroup you plan to use needs to be tagged with the key  <code>GrafanaDataSource</code> and value <code>true</code> for the service managed permissions to be permitted to use the workgroup.</li> <li>The service-managed IAM policy only grants access to query result buckets  that start with <code>grafana-athena-query-results-</code>, so for any other bucket you MUST add permissions manually.</li> <li>You have to add the <code>s3:Get*</code> and <code>s3:List*</code> permissions for the underlying data source  being queried manually.</li> </ol> <p>To set up the Athena data source, use the left-hand toolbar and choose the  lower AWS icon and then choose \"Athena\". Select your default region you want  the plugin to discover the Athena data source to use, and then select the  accounts that you want, and finally choose \"Add data source\".</p> <p>Alternatively, you can manually add and configure the Athena data source by  following these steps:</p> <ol> <li>Click on the \"Configurations\" icon on the left-hand toolbar and then on \"Add data source\".</li> <li>Search for \"Athena\".</li> <li>[OPTIONAL] Configure the authentication provider (recommended: workspace IAM    role).</li> <li>Select your targeted Athena data source, database, and workgroup.</li> <li>If your workgroup doesn't have an output location configured already,    specify the S3 bucket and folder to use for query results. Note that the    bucket has to start with <code>grafana-athena-query-results-</code> if you want to    benefit from the service-managed policy.</li> <li>Click \"Save &amp; test\".</li> </ol> <p>You should see something like the following:</p> <p></p>"},{"location":"recipes/recipes/amg-athena-plugin/#usage","title":"Usage","text":"<p>And now let's look at how to use our Athena datasets from Grafana.</p>"},{"location":"recipes/recipes/amg-athena-plugin/#use-geographical-data","title":"Use geographical data","text":"<p>The OpenStreetMap (OSM) data in Athena can answer a number of questions, such as \"where are certain amenities\". Let's see that in action.</p> <p>For example, a SQL query against the OSM dataset to list places that offer food in the Las Vegas region is as follows:</p> <pre><code>SELECT \ntags['amenity'] AS amenity,\ntags['name'] AS name,\ntags['website'] AS website,\nlat, lon\nFROM planet\nWHERE type = 'node'\n  AND tags['amenity'] IN ('bar', 'pub', 'fast_food', 'restaurant')\n  AND lon BETWEEN -115.5 AND -114.5\n  AND lat BETWEEN 36.1 AND 36.3\nLIMIT 500;\n</code></pre> <p>Info</p> <p>The Las Vegas region in above query is defined as everything with a latitude  between <code>36.1</code> and <code>36.3</code> as well as a longitude between <code>-115.5</code> and <code>-114.5</code>. You could turn that into a set of variables (one for each corner) and make the Geomap plugin adaptable to other regions.</p> <p>To visualize the OSM data using above query, you can import an example dashboard,  available via osm-sample-dashboard.json that looks as follows:</p> <p></p> <p>Note</p> <p>In above screen shot we use the Geomap visualization (in the left panel) to plot the data points.</p>"},{"location":"recipes/recipes/amg-athena-plugin/#use-vpc-flow-logs-data","title":"Use VPC flow logs data","text":"<p>To analyze the VPC flow log data, detecting SSH and RDP traffic, use the following SQL queries.</p> <p>Getting a tabular overview on SSH/RDP traffic:</p> <pre><code>SELECT\nsrcaddr, dstaddr, account_id, action, protocol, bytes, log_status\nFROM vpclogs\nWHERE\nsrcport in (22, 3389)\nOR\ndstport IN (22, 3389)\nORDER BY start ASC;\n</code></pre> <p>Getting a time series view on bytes accepted and rejected:</p> <pre><code>SELECT\nfrom_unixtime(start), sum(bytes), action\nFROM vpclogs\nWHERE\nsrcport in (22,3389)\nOR\ndstport IN (22, 3389)\nGROUP BY start, action\nORDER BY start ASC;\n</code></pre> <p>Tip</p> <p>If you want to limit the amount of data queried in Athena, consider using the <code>$__timeFilter</code> macro.</p> <p>To visualize the VPC flow log data, you can import an example dashboard,  available via vpcfl-sample-dashboard.json that looks as follows:</p> <p></p> <p>From here, you can use the following guides to create your own dashboard in Amazon Managed Grafana:</p> <ul> <li>User Guide: Dashboards</li> <li>Best practices for creating dashboards</li> </ul> <p>That's it, congratulations you've learned how to use Athena from Grafana!</p>"},{"location":"recipes/recipes/amg-athena-plugin/#cleanup","title":"Cleanup","text":"<p>Remove the OSM data from the Athena database you've been using and then the Amazon Managed Grafana workspace by removing it from the console.</p>"},{"location":"recipes/recipes/amg-automation-tf/","title":"Using Terraform for Amazon Managed Grafana automation","text":"<p>In this recipe we show you how use Terraform to automate Amazon Managed Grafana,  for example to add datasources or dashboards consistently across a number of workspaces.</p> <p>Note</p> <p>This guide will take approximately 30 minutes to complete.</p>"},{"location":"recipes/recipes/amg-automation-tf/#prerequisites","title":"Prerequisites","text":"<ul> <li>The AWS command line is installed and configured in your local environment.</li> <li>You have the Terraform command line installed in your local environment.</li> <li>You have an Amazon Managed Service for Prometheus workspace ready to use.</li> <li>You have an Amazon Managed Grafana workspace ready to use.</li> </ul>"},{"location":"recipes/recipes/amg-automation-tf/#set-up-amazon-managed-grafana","title":"Set up Amazon Managed Grafana","text":"<p>In order for Terraform to authenticate against Grafana, we are  using an API Key, which acts as a kind of password. </p> <p>Info</p> <p>The API key is an RFC 6750 HTTP Bearer header with a 51 character long alpha-numeric value authenticating the caller with every request against the Grafana API.</p> <p>So, before we can set up the Terraform manifest, we first need to create an API key. You do this via the Grafana UI as follows.</p> <p>First, select from the left-hand side menu in the <code>Configuration</code> section the <code>API keys</code> menu item:</p> <p></p> <p>Now create a new API key, give it a name that makes sense for your task at hand, assign it <code>Admin</code> role and set the duration time to, for example, one day:</p> <p></p> <p>Note</p> <p>The API key is valid for a limited time, in AMG you can use values up to 30 days.</p> <p>Once you hit the <code>Add</code> button you should see a pop-up dialog that contains the API key:</p> <p></p> <p>Warning</p> <p>This is the only time you will see the API key, so store it from here in a safe place, we will need it in the Terraform manifest later.</p> <p>With this we've set up everything we need in Amazon Managed Grafana in order to use Terraform for automation, so let's move on to this step.</p>"},{"location":"recipes/recipes/amg-automation-tf/#automation-with-terraform","title":"Automation with Terraform","text":""},{"location":"recipes/recipes/amg-automation-tf/#preparing-terraform","title":"Preparing Terraform","text":"<p>For Terraform to be able to interact with Grafana, we're using the official Grafana provider in version 1.13.3 or above.</p> <p>In the following, we want to automate the creation of a data source, in our case we want to add a Prometheus data source, to be exact, an AMP workspace.</p> <p>First, create a file called <code>main.tf</code> with the following content:</p> <p><pre><code>terraform {\n  required_providers {\n    grafana = {\n      source  = \"grafana/grafana\"\n      version = \"&gt;= 1.13.3\"\n    }\n  }\n}\n\nprovider \"grafana\" {\n  url  = \"INSERT YOUR GRAFANA WORKSPACE URL HERE\"\n  auth = \"INSERT YOUR API KEY HERE\"\n}\n\nresource \"grafana_data_source\" \"prometheus\" {\n  type          = \"prometheus\"\n  name          = \"amp\"\n  is_default    = true\n  url           = \"INSERT YOUR AMP WORKSPACE URL HERE \"\n  json_data {\n    http_method     = \"POST\"\n    sigv4_auth      = true\n    sigv4_auth_type = \"workspace-iam-role\"\n    sigv4_region    = \"eu-west-1\"\n  }\n}\n</code></pre> In above file you need to insert three values that depend on your environment.</p> <p>In the Grafana provider section:</p> <ul> <li><code>url</code> \u2026 the Grafana workspace URL which looks something like the following:       <code>https://xxxxxxxx.grafana-workspace.eu-west-1.amazonaws.com</code>.</li> <li><code>auth</code> \u2026 the API key you have created in the previous step.</li> </ul> <p>In the Prometheus resource section, insert the <code>url</code> which is the AMP  workspace URL in the form of  <code>https://aps-workspaces.eu-west-1.amazonaws.com/workspaces/ws-xxxxxxxxx</code>.</p> <p>Note</p> <p>If you're using Amazon Managed Grafana in a different region than the one shown in the file, you will have to, in addition to above, also set the <code>sigv4_region</code> to your region.</p> <p>To wrap up the preparation phase, let's now initialize Terraform:</p> <pre><code>$ terraform init\nInitializing the backend...\n\nInitializing provider plugins...\n- Finding grafana/grafana versions matching \"&gt;= 1.13.3\"...\n- Installing grafana/grafana v1.13.3...\n- Installed grafana/grafana v1.13.3 (signed by a HashiCorp partner, key ID 570AA42029AE241A)\n\nPartner and community providers are signed by their developers.\nIf you'd like to know more about provider signing, you can read about it here:\nhttps://www.terraform.io/docs/cli/plugins/signing.html\n\nTerraform has created a lock file .terraform.lock.hcl to record the provider\nselections it made above. Include this file in your version control repository\nso that Terraform can guarantee to make the same selections by default when\nyou run \"terraform init\" in the future.\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n</code></pre> <p>With that, we're all set and can use Terraform to automate the data source creation as explained in the following.</p>"},{"location":"recipes/recipes/amg-automation-tf/#using-terraform","title":"Using Terraform","text":"<p>Usually, you would first have a look what Terraform's plan is, like so:</p> <pre><code>$ terraform plan\n\nTerraform used the selected providers to generate the following execution plan. \nResource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # grafana_data_source.prometheus will be created\n  + resource \"grafana_data_source\" \"prometheus\" {\n      + access_mode        = \"proxy\"\n      + basic_auth_enabled = false\n      + id                 = (known after apply)\n      + is_default         = true\n      + name               = \"amp\"\n      + type               = \"prometheus\"\n      + url                = \"https://aps-workspaces.eu-west-1.amazonaws.com/workspaces/ws-xxxxxx/\"\n\n      + json_data {\n          + http_method     = \"POST\"\n          + sigv4_auth      = true\n          + sigv4_auth_type = \"workspace-iam-role\"\n          + sigv4_region    = \"eu-west-1\"\n        }\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nNote: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now.\n</code></pre> <p>If you're happy with what you see there, you can apply the plan:</p> <pre><code>$ terraform apply\n\nTerraform used the selected providers to generate the following execution plan. \nResource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # grafana_data_source.prometheus will be created\n  + resource \"grafana_data_source\" \"prometheus\" {\n      + access_mode        = \"proxy\"\n      + basic_auth_enabled = false\n      + id                 = (known after apply)\n      + is_default         = true\n      + name               = \"amp\"\n      + type               = \"prometheus\"\n      + url                = \"https://aps-workspaces.eu-west-1.amazonaws.com/workspaces/ws-xxxxxxxxx/\"\n\n      + json_data {\n          + http_method     = \"POST\"\n          + sigv4_auth      = true\n          + sigv4_auth_type = \"workspace-iam-role\"\n          + sigv4_region    = \"eu-west-1\"\n        }\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n\n  Enter a value: yes\n\ngrafana_data_source.prometheus: Creating...\ngrafana_data_source.prometheus: Creation complete after 1s [id=10]\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n</code></pre> <p>When you now go to the data source list in Grafana you should see something like the following:</p> <p></p> <p>To verify if your newly created data source works, you can hit the blue <code>Save &amp; test</code> button at the bottom and you should see a <code>Data source is working</code> confirmation message as a result here.</p> <p>You can use Terraform also to automate other things, for example, the Grafana  provider supports managing folders and dashboards.</p> <p>Let's say you want to create a folder to organize your dashboards, for example:</p> <pre><code>resource \"grafana_folder\" \"examplefolder\" {\n  title = \"devops\"\n}\n</code></pre> <p>Further, say you have a dashboard called <code>example-dashboard.json</code>, and you want to create it in the folder from above, then you would use the following snippet:</p> <pre><code>resource \"grafana_dashboard\" \"exampledashboard\" {\n  folder = grafana_folder.examplefolder.id\n  config_json = file(\"example-dashboard.json\")\n}\n</code></pre> <p>Terraform is a powerful tool for automation and you can use it as shown here to manage your Grafana resources. </p> <p>Note</p> <p>Keep in mind, though, that the state in Terraform is, by default, managed locally. This means, if you plan to collaboratively work with Terraform, you need to pick one of the options available that allow you to share the state across a team.</p>"},{"location":"recipes/recipes/amg-automation-tf/#cleanup","title":"Cleanup","text":"<p>Remove the Amazon Managed Grafana workspace by removing it from the console.</p>"},{"location":"recipes/recipes/amg-google-auth-saml/","title":"Configure Google Workspaces authentication with Amazon Managed Grafana using SAML","text":"<p>In this guide, we will walk through how you can setup Google Workspaces as an identity provider (IdP) for Amazon Managed Grafana using SAML v2.0 protocol.</p> <p>In order to follow this guide you need to create a paid Google Workspaces  account in addition to having an Amazon Managed Grafana workspace created.</p>"},{"location":"recipes/recipes/amg-google-auth-saml/#create-amazon-managed-grafana-workspace","title":"Create Amazon Managed Grafana workspace","text":"<p>Log into the Amazon Managed Grafana console and click Create workspace. In the following screen, provide a workspace name as shown below. Then click Next:</p> <p></p> <p>In the Configure settings page, select Security Assertion Markup Language (SAML)  option so you can configure a SAML based Identity Provider for users to log in:</p> <p></p> <p>Select the data sources you want to choose and click Next: </p> <p>Click on Create workspace button in the Review and create screen: </p> <p>This will create a new Amazon Managed Grafana workspace as shown below:</p> <p></p>"},{"location":"recipes/recipes/amg-google-auth-saml/#configure-google-workspaces","title":"Configure Google Workspaces","text":"<p>Login to Google Workspaces with Super Admin permissions and go to Web and mobile apps under Apps section. There, click on Add App  and select Add custom SAML app. Now give the app a name as shown below.  Click CONTINUE.:</p> <p></p> <p>On the next screen, click on DOWNLOAD METADATA button to download the SAML metadata file. Click CONTINUE.</p> <p></p> <p>On the next screen, you will see the ACS URL, Entity ID and Start URL fields. You can get the values for these fields from the Amazon Managed Grafana console. </p> <p>Select EMAIL from the drop down in the Name ID format field and select Basic Information &gt; Primary email in the Name ID field.</p> <p>Click CONTINUE. </p> <p></p> <p>In the Attribute mapping screen, make the mapping between Google Directory attributes and App attributes as shown in the screenshot below</p> <p></p> <p>For users logging in through Google authentication to have Admin privileges in Amazon Managed Grafana, set the Department field\u2019s value as monitoring. You can choose any field and any value for this. Whatever you choose to use on the Google Workspaces side, make sure you make the mapping on Amazon Managed Grafana SAML settings to reflect that.</p>"},{"location":"recipes/recipes/amg-google-auth-saml/#upload-saml-metadata-into-amazon-managed-grafana","title":"Upload SAML metadata into Amazon Managed Grafana","text":"<p>Now in the Amazon Managed Grafana console, click Upload or copy/paste option and select Choose file button to upload the SAML metadata file downloaded from Google Workspaces, earlier. </p> <p>In the Assertion mapping section, type in Department in the Assertion attribute role field and monitoring in the Admin role values field.  This will allow users logging in with Department as monitoring to  have Admin privileges in Grafana so they can perform administrator duties such as creating dashboards and datasources.</p> <p>Set values under Additional settings - optional section as shown in the  screenshot below. Click on Save SAML configuration:</p> <p></p> <p>Now Amazon Managed Grafana is set up to authenticate users using Google Workspaces. </p> <p>When users login, they will be redirected to the Google login page like so:</p> <p></p> <p>After entering their credentials, they will be logged into Grafana as shown in the screenshot below. </p> <p>As you can see, the user was able to successfully login to Grafana using Google Workspaces authentication.</p>"},{"location":"recipes/recipes/amg-redshift-plugin/","title":"Using Redshift in Amazon Managed Grafana","text":"<p>In this recipe we show you how to use Amazon Redshift\u2014a petabyte-scale data  warehouse service using standard SQL\u2014in Amazon Managed Grafana. This integration is enabled by the Redshift data source for Grafana, an open source plugin available for you to use in any DIY Grafana instance as well as  pre-installed in Amazon Managed Grafana.</p> <p>Note</p> <p>This guide will take approximately 10 minutes to complete.</p>"},{"location":"recipes/recipes/amg-redshift-plugin/#prerequisites","title":"Prerequisites","text":"<ol> <li>You have admin access to Amazon Redshift from your account.</li> <li>Tag your Amazon Redshift cluster with <code>GrafanaDataSource: true</code>. </li> <li>In order to benefit from the service-managed policies, create the database     credentials in one of the following ways:<ol> <li>If you want to use the default mechanism, that is, the temporary credentials  option, to authenticate against the Redshift database, you must create a database  user named <code>redshift_data_api_user</code>.</li> <li>If you want to use the credentials from Secrets Manager, you must tag the  secret with <code>RedshiftQueryOwner: true</code>.</li> </ol> </li> </ol> <p>Tip</p> <p>For more information on how to work with the service-managed or custom policies, see the examples in the Amazon Managed Grafana docs.</p>"},{"location":"recipes/recipes/amg-redshift-plugin/#infrastructure","title":"Infrastructure","text":"<p>We need a Grafana instance, so go ahead and set up a new Amazon Managed Grafana workspace, for example by using the Getting Started guide, or use an existing one.</p> <p>Note</p> <p>To use AWS data source configuration, first go to the Amazon Managed Grafana console to enable service-mananged IAM roles that grant the workspace the  IAM policies necessary to read the Athena resources.</p> <p>To set up the Athena data source, use the left-hand toolbar and choose the  lower AWS icon and then choose \"Redshift\". Select your default region you want  the plugin to discover the Redshift data source to use, and then select the  accounts that you want, and finally choose \"Add data source\".</p> <p>Alternatively, you can manually add and configure the Redshift data source by  following these steps:</p> <ol> <li>Click on the \"Configurations\" icon on the left-hand toolbar and then on \"Add data source\".</li> <li>Search for \"Redshift\".</li> <li>[OPTIONAL] Configure the authentication provider (recommended: workspace IAM    role).</li> <li>Provide the \"Cluster Identifier\", \"Database\", and \"Database User\" values.</li> <li>Click \"Save &amp; test\".</li> </ol> <p>You should see something like the following:</p> <p></p>"},{"location":"recipes/recipes/amg-redshift-plugin/#usage","title":"Usage","text":"<p>We will be using the Redshift Advance Monitoring setup. Since all is available out of the box, there's nothing else to configure at this point.</p> <p>You can import the Redshift monitoring dashboard, included in the Redshift plugin. Once imported you should see something like this:</p> <p></p> <p>From here, you can use the following guides to create your own dashboard in Amazon Managed Grafana:</p> <ul> <li>User Guide: Dashboards</li> <li>Best practices for creating dashboards</li> </ul> <p>That's it, congratulations you've learned how to use Redshift from Grafana!</p>"},{"location":"recipes/recipes/amg-redshift-plugin/#cleanup","title":"Cleanup","text":"<p>Remove the Redshift database you've been using and then the Amazon Managed Grafana workspace by removing it from the console.</p>"},{"location":"recipes/recipes/amp-alertmanager-terraform/","title":"Terraform as Infrastructure as a Code to deploy Amazon Managed Service for Prometheus and configure Alert manager","text":"<p>In this recipe, we will demonstrate how you can use Terraform to provision Amazon Managed Service for Prometheus and configure rules management and alert manager to send notification to a SNS topic if a certain condition is met.</p> <p>Note</p> <p>This guide will take approximately 30 minutes to complete.</p>"},{"location":"recipes/recipes/amp-alertmanager-terraform/#prerequisites","title":"Prerequisites","text":"<p>You will need the following to complete the setup:</p> <ul> <li>Amazon EKS cluster</li> <li>AWS CLI version 2</li> <li>Terraform CLI</li> <li>AWS Distro for OpenTelemetry(ADOT)</li> <li>eksctl</li> <li>kubectl</li> <li>jq</li> <li>helm</li> <li>SNS topic</li> <li>awscurl</li> </ul> <p>In the recipe, we will use a sample application in order to demonstrate the metric scraping using ADOT and remote write the metrics to the Amazon Managed Service for Prometheus workspace. Fork and clone the sample app from the repository at aws-otel-community.</p> <p>This Prometheus sample app generates all 4 Prometheus metric types (counter, gauge, histogram, summary) and exposes them at the /metrics endpoint</p> <p>A health check endpoint also exists at /</p> <p>The following is a list of optional command line flags for configuration:</p> <p>listen_address: (default = 0.0.0.0:8080) defines the address and port that the sample app is exposed to. This is primarily to conform with the test framework requirements.</p> <p>metric_count: (default=1) the amount of each type of metric to generate. The same amount of metrics is always generated per metric type.</p> <p>label_count: (default=1) the amount of labels per metric to generate.</p> <p>datapoint_count: (default=1) the number of data-points per metric to generate.</p>"},{"location":"recipes/recipes/amp-alertmanager-terraform/#enabling-metric-collection-using-aws-distro-for-opentelemetry","title":"Enabling Metric collection using AWS Distro for Opentelemetry","text":"<ol> <li>Fork and clone the sample app from the repository at aws-otel-community. Then run the following commands.</li> </ol> <p><pre><code>cd ./sample-apps/prometheus\ndocker build . -t prometheus-sample-app:latest\n</code></pre> 2. Push this image to a registry such as Amazon ECR. You can use the following command to create a new ECR repository in your account. Make sure to set  as well. <p><pre><code>aws ecr create-repository \\\n    --repository-name prometheus-sample-app \\\n    --image-scanning-configuration scanOnPush=true \\\n    --region &lt;YOUR_REGION&gt;\n</code></pre> 3. Deploy the sample app in the cluster by copying this Kubernetes configuration and applying it. Change the image to the image that you just pushed by replacing <code>PUBLIC_SAMPLE_APP_IMAGE</code> in the prometheus-sample-app.yaml file.</p> <p><pre><code>curl https://raw.githubusercontent.com/aws-observability/aws-otel-collector/main/examples/eks/aws-prometheus/prometheus-sample-app.yaml -o prometheus-sample-app.yaml\nkubectl apply -f prometheus-sample-app.yaml\n</code></pre> 4. Start a default instance of the ADOT Collector. To do so, first enter the following command to pull the Kubernetes configuration for ADOT Collector.</p> <p><pre><code>curl https://raw.githubusercontent.com/aws-observability/aws-otel-collector/main/examples/eks/aws-prometheus/prometheus-daemonset.yaml -o prometheus-daemonset.yaml\n</code></pre> Then edit the template file, substituting the remote_write endpoint for your Amazon Managed Service for Prometheus workspace for <code>YOUR_ENDPOINT</code> and your Region for <code>YOUR_REGION</code>.  Use the remote_write endpoint that is displayed in the Amazon Managed Service for Prometheus console when you look at your workspace details. You'll also need to change <code>YOUR_ACCOUNT_ID</code> in the service account section of the Kubernetes configuration to your AWS account ID.</p> <p>In this recipe, the ADOT Collector configuration uses an annotation <code>(scrape=true)</code> to tell which target endpoints to scrape. This allows the ADOT Collector to distinguish the sample app endpoint from kube-system endpoints in your cluster. You can remove this from the re-label configurations if you want to scrape a different sample app. 5. Enter the following command to deploy the ADOT collector. <pre><code>kubectl apply -f eks-prometheus-daemonset.yaml\n</code></pre></p>"},{"location":"recipes/recipes/amp-alertmanager-terraform/#configure-workspace-with-terraform","title":"Configure workspace with Terraform","text":"<p>Now, we will  provision a Amazon Managed Service for Prometheus workspace and will define an alerting rule that causes the Alert Manager to send a notification if a certain condition (defined in <code>expr</code>) holds true for a specified time period (<code>for</code>). Code in the Terraform language is stored in plain text files with the .tf file extension. There is also a JSON-based variant of the language that is named with the .tf.json file extension.</p> <p>We will now use the main.tf to deploy the resources using terraform. Before running the terraform command, we will export the <code>region</code> and <code>sns_topic</code> variable.</p> <pre><code>export TF_VAR_region=&lt;your region&gt;\nexport TF_VAR_sns_topic=&lt;ARN of the SNS topic used by the SNS receiver&gt;\n</code></pre> <p>Now, we will execute the below commands to provision the workspace: </p> <pre><code>terraform init\nterraform plan\nterraform apply\n</code></pre> <p>Once the above steps are complete, verify the setup end-to-end by using awscurl and query the endpoint. Ensure the <code>WORKSPACE_ID</code> variable is replaced with the appropriate Amazon Managed Service for Prometheus workspace id.</p> <p>On running the below command, look for the metric \u201cmetric:recording_rule\u201d, and, if you successfully find the metric, then you\u2019ve successfully created a recording rule:</p> <p><pre><code>awscurl https://aps-workspaces.us-east-1.amazonaws.com/workspaces/$WORKSPACE_ID/api/v1/rules  --service=\"aps\"\n</code></pre> Sample Output: <pre><code>\"status\":\"success\",\"data\":{\"groups\":[{\"name\":\"alert-test\",\"file\":\"rules\",\"rules\":[{\"state\":\"firing\",\"name\":\"metric:alerting_rule\",\"query\":\"rate(adot_test_counter0[5m]) \\u003e 5\",\"duration\":0,\"labels\":{},\"annotations\":{},\"alerts\":[{\"labels\":{\"alertname\":\"metric:alerting_rule\"},\"annotations\":{},\"state\":\"firing\",\"activeAt\":\"2021-09-16T13:20:35.9664022Z\",\"value\":\"6.96890019778219e+01\"}],\"health\":\"ok\",\"lastError\":\"\",\"type\":\"alerting\",\"lastEvaluation\":\"2021-09-16T18:41:35.967122005Z\",\"evaluationTime\":0.018121408}],\"interval\":60,\"lastEvaluation\":\"2021-09-16T18:41:35.967104769Z\",\"evaluationTime\":0.018142997},{\"name\":\"test\",\"file\":\"rules\",\"rules\":[{\"name\":\"metric:recording_rule\",\"query\":\"rate(adot_test_counter0[5m])\",\"labels\":{},\"health\":\"ok\",\"lastError\":\"\",\"type\":\"recording\",\"lastEvaluation\":\"2021-09-16T18:40:44.650001548Z\",\"evaluationTime\":0.018381387}],\"interval\":60,\"lastEvaluation\":\"2021-09-16T18:40:44.649986468Z\",\"evaluationTime\":0.018400463}]},\"errorType\":\"\",\"error\":\"\"}\n</code></pre></p> <p>We can further query the alertmanager endpoint to confirm the same <pre><code>awscurl https://aps-workspaces.us-east-1.amazonaws.com/workspaces/$WORKSPACE_ID/alertmanager/api/v2/alerts --service=\"aps\" -H \"Content-Type: application/json\"\n</code></pre> Sample Output: <pre><code>[{\"annotations\":{},\"endsAt\":\"2021-09-16T18:48:35.966Z\",\"fingerprint\":\"114212a24ca97549\",\"receivers\":[{\"name\":\"default\"}],\"startsAt\":\"2021-09-16T13:20:35.966Z\",\"status\":{\"inhibitedBy\":[],\"silencedBy\":[],\"state\":\"active\"},\"updatedAt\":\"2021-09-16T18:44:35.984Z\",\"generatorURL\":\"/graph?g0.expr=sum%28rate%28envoy_http_downstream_rq_time_bucket%5B1m%5D%29%29+%3E+5\\u0026g0.tab=1\",\"labels\":{\"alertname\":\"metric:alerting_rule\"}}]\n</code></pre> This confirms the alert was triggered and sent to SNS via the SNS receiver</p>"},{"location":"recipes/recipes/amp-alertmanager-terraform/#clean-up","title":"Clean up","text":"<p>Run the following command to terminate the Amazon Managed Service for Prometheus workspace. Make sure you delete the EKS Cluster that was created as well:</p> <pre><code>terraform destroy\n</code></pre>"},{"location":"recipes/recipes/amp-mixin-dashboards/","title":"Adding kubernetes-mixin dashboards to Managed Grafana","text":"<p>Even as a managed service, EKS still exposes many of the metrics from the Kubernetes control plane. The Prometheus community has put together a series of dashboards to review and investigate these metrics. This document will show you how to install them in an environment hosted by Amazon Managed Service for Prometheus.</p> <p>The Prometheus mixin project expects prometheus to be installed via the Prometheus Operator, but the Terraform blueprints install the Prometheus agent via the default helm charts. In order for the scraping jobs and the dashboards to line up, we need to update the Prometheus rules and the mixin dashboard configuration, then upload the dashboard to our Grafana instance.</p>"},{"location":"recipes/recipes/amp-mixin-dashboards/#prerequisites","title":"Prerequisites","text":"<ul> <li>An EKS cluster - Starting from: https://github.com/aws-ia/terraform-aws-eks-blueprints/tree/main/examples/complete-kubernetes-addons</li> <li>A Cloud9 environment</li> <li>kubectl in Cloud9 configured to manage the EKS cluster</li> <li>IAM credentials for EKS</li> <li>An instance of AMP</li> <li>An instance of Amazon Managed Grafana</li> </ul>"},{"location":"recipes/recipes/amp-mixin-dashboards/#installing-the-mixin-dashboards","title":"Installing the mixin dashboards","text":"<p>Starting from a fresh Cloud9 instance and using the AWS blueprint for terraform complete addon example as the target EKS cluster as linked in the Prerequisites:</p> <p>Expand the file system of the Cloud9 instance to at least 20 gb. In the EC2 console, extend the EBS volume to 20 GB thenfrom the Cloud9 shell, run the commands below:</p> <pre><code>sudo growpart /dev/nvme0n1 1\nsudo xfs_growfs -d /\n</code></pre> <p>Upgrade awscli to version 2:</p> <pre><code>sudo yum remove -y awscli\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\nln -s /usr/local/bin/aws /usr/bin/aws\n</code></pre> <p>Install prerequisites:</p> <pre><code>sudo yum install -y jsonnet\ngo install -a github.com/jsonnet-bundler/jsonnet-bundler/cmd/jb@latest\nexport PATH=\"$PATH:~/go/bin\"\n</code></pre> <p>Download and install the jsonnet libraries for the kubernetes-mixin project:</p> <pre><code>git clone https://github.com/kubernetes-monitoring/kubernetes-mixincd kubernetes-mixin/\njb install\n</code></pre> <p>Edit config.libsonnet and replace the \u201cselectors\u201c section with the following in order to match the prometheus job names:</p> <pre><code> // Selectors are inserted between {} in Prometheus queries.\n cadvisorSelector: 'job=\"kubernetes-nodes-cadvisor\"',\n kubeletSelector: 'job=\"kubernetes-nodes\"',\n kubeStateMetricsSelector: 'job=\"kubernetes-service-endpoints\"',\n nodeExporterSelector: 'job=\"kubernetes-service-endpoints\"',\n kubeSchedulerSelector: 'job=\"kube-scheduler\"',\n kubeControllerManagerSelector: 'job=\"kube-controller-manager\"',\n kubeApiserverSelector: 'job=\"kubernetes-apiservers\"',\n kubeProxySelector: 'job=\"kubernetes-nodes\"',\n podLabel: 'pod',\n hostNetworkInterfaceSelector: 'device!~\"veth.+\"',\n hostMountpointSelector: 'mountpoint=\"/\"',\n windowsExporterSelector: 'job=\"kubernetes-windows-exporter\"',\n containerfsSelector: 'container!=\"\"',\n</code></pre> <p>Build the prometheus rules, alerts, and grafana dashboards:</p> <pre><code>make prometheus_alerts.yaml\nmake prometheus_rules.yaml\nmake dashboards_out\n</code></pre> <p>Upload the prometheus rules to managed prometheus. Replace &lt;&gt; with the ID of your managed prometheus instance and &lt;&gt; with the appropriate value <pre><code>base64 prometheus_rules.yaml &gt; prometheus_rules.b64\naws amp create-rule-groups-namespace --data file://prometheus_rules.b64 --name kubernetes-mixin  --workspace-id &lt;&lt;WORKSPACE-ID&gt; --region &lt;&lt;REGION&gt;&gt;\n</code></pre> <p>Download the contents of the \u2018dashboard_out\u2019 folder from the Cloud9 environment and upload them using the Grafana web UI.</p>"},{"location":"recipes/recipes/as-ec2-using-amp-and-alertmanager/","title":"Auto-scaling Amazon EC2 using Amazon Managed Service for Prometheus and alert manager","text":"<p>Customers want to migrate their existing Prometheus workloads to the cloud and utilize all that the cloud offers. AWS has services like Amazon EC2 Auto Scaling, which lets you scale out Amazon Elastic Compute Cloud (Amazon EC2) instances based on metrics like CPU or memory utilization. Applications that use Prometheus metrics can easily integrate into EC2 Auto Scaling without needing to replace their monitoring stack. In this post, I will walk you through configuring Amazon EC2 Auto Scaling to work with Amazon Managed Service for Prometheus Alert Manager. This approach lets you move a Prometheus-based workload to the cloud while taking advantage of services like autoscaling.</p> <p>Amazon Managed Service for Prometheus provides support for alerting rules that use PromQL. The Prometheus alerting rules documentation provides the syntax and examples of valid alerting rules. Likewise, the Prometheus alert manager documentation references both the syntax and examples of valid alert manager configurations.</p>"},{"location":"recipes/recipes/as-ec2-using-amp-and-alertmanager/#solution-overview","title":"Solution overview","text":"<p>First, let\u2019s briefly review Amazon EC2 Auto Scaling\u2018s concept of an Auto Scaling group which is a logical collection of Amazon EC2 instances. An Auto Scaling group can launch EC2 instances based on a predefined launch template. The launch template contains information used to launch the Amazon EC2 instance, including the AMI ID, the instance type, network settings, and AWS Identity and Access Management (IAM) instance profile.</p> <p>Amazon EC2 Auto Scaling groups have a minimum size, maximum size, and desired capacity concepts. When Amazon EC2 Auto Scaling detects that the current running capacity of the Auto Scaling group is above or below the desired capacity, it will automatically scale out or scale in as needed. This scaling approach lets you utilize elasticity within your workload while still keeping bounds on both capacity and costs.</p> <p>To demonstrate this solution, I have created an Amazon EC2 Auto Scaling group that contains two Amazon EC2 instances. These instances remote write instance metrics to an Amazon Managed Service for Prometheus workspace. I have set the Auto Scaling group\u2019s minimum size to two (to maintain high availability), and I\u2019ve set the group\u2019s maximum size to 10 (to help control costs). As more traffic hits the solution, additional Amazon EC2 instances are automatically added to support the load, up to the Amazon EC2 Auto Scaling group\u2019s maximum size. As the load decreases, those Amazon EC2 instances are terminated until the Amazon EC2 Auto Scaling group reaches the group\u2019s minimum size. This approach lets you have a performant application by utilizing the elasticity of the cloud.</p> <p>Note that as you scrape more and more resources, you could quickly overwhelm the capabilities of a single Prometheus server. You can avoid this situation by scaling Prometheus servers linearly with the workload. This approach ensures that you can collect metric data at the granularity that you want.</p> <p>To support the Auto Scaling of a Prometheus workload, I have created an Amazon Managed Service for Prometheus workspace with the following rules:</p> <p><code>YAML</code> <pre><code>groups:\n- name: example\n  rules:\n  - alert: HostHighCpuLoad\n    expr: 100 - (avg(rate(node_cpu_seconds_total{mode=\"idle\"}[2m])) * 100) &gt; 60\n    for: 5m\n    labels:\n      severity: warning\n      event_type: scale_up\n    annotations:\n      summary: Host high CPU load (instance {{ $labels.instance }})\n      description: \"CPU load is &gt; 60%\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n  - alert: HostLowCpuLoad\n    expr: 100 - (avg(rate(node_cpu_seconds_total{mode=\"idle\"}[2m])) * 100) &lt; 30\n    for: 5m\n    labels:\n      severity: warning\n      event_type: scale_down\n    annotations:\n      summary: Host low CPU load (instance {{ $labels.instance }})\n      description: \"CPU load is &lt; 30%\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n</code></pre></p> <p>This rules set creates a <code>HostHighCpuLoad</code> and a <code>HostLowCpuLoad</code> rules. These alerts trigger when the CPU is greater than 60% or less than 30% utilization over a five-minute period.</p> <p>After raising an alert, the alert manager will forward the message into an Amazon SNS topic, passing an <code>alert_type</code> (the alert name) and <code>event_type</code> (scale_down or scale_up).</p> <p><code>YAML</code> <pre><code>alertmanager_config: |\n  route: \n    receiver: default_receiver\n    repeat_interval: 5m\n\n  receivers:\n    - name: default_receiver\n      sns_configs:\n        - topic_arn: &lt;ARN OF SNS TOPIC GOES HERE&gt;\n          send_resolved: false\n          sigv4:\n            region: us-east-1\n          message: |\n            alert_type: {{ .CommonLabels.alertname }}\n            event_type: {{ .CommonLabels.event_type }}\n</code></pre></p> <p>An AWS Lambda function is subscribed to the Amazon SNS topic. I have written logic in the Lambda function to inspect the Amazon SNS message and determine if a <code>scale_up</code> or <code>scale_down</code> event should happen. Then, the Lambda function increments or decrements the desired capacity of the Amazon EC2 Auto Scaling group. The Amazon EC2 Auto Scaling group detects a requested change in capacity, and then invokes or deallocates Amazon EC2 instances.</p> <p>The Lambda code to support Auto Scaling is as follows:</p> <p><code>Python</code> <pre><code>import json\nimport boto3\nimport os\n\ndef lambda_handler(event, context):\n    print(event)\n    msg = event['Records'][0]['Sns']['Message']\n\n    scale_type = ''\n    if msg.find('scale_up') &gt; -1:\n        scale_type = 'scale_up'\n    else:\n        scale_type = 'scale_down'\n\n    get_desired_instance_count(scale_type)\n\ndef get_desired_instance_count(scale_type):\n\n    client = boto3.client('autoscaling')\n    asg_name = os.environ['ASG_NAME']\n    response = client.describe_auto_scaling_groups(AutoScalingGroupNames=[ asg_name])\n\n    minSize = response['AutoScalingGroups'][0]['MinSize']\n    maxSize = response['AutoScalingGroups'][0]['MaxSize']\n    desiredCapacity = response['AutoScalingGroups'][0]['DesiredCapacity']\n\n    if scale_type == \"scale_up\":\n        desiredCapacity = min(desiredCapacity+1, maxSize)\n    if scale_type == \"scale_down\":\n        desiredCapacity = max(desiredCapacity - 1, minSize)\n\n    print('Scale type: {}; new capacity: {}'.format(scale_type, desiredCapacity))\n    response = client.set_desired_capacity(AutoScalingGroupName=asg_name, DesiredCapacity=desiredCapacity, HonorCooldown=False)\n</code></pre></p> <p>The full architecture can be reviewed in the following figure.</p> <p></p>"},{"location":"recipes/recipes/as-ec2-using-amp-and-alertmanager/#testing-out-the-solution","title":"Testing out the solution","text":"<p>You can launch an AWS CloudFormation template to automatically provision this solution.</p> <p>Stack prerequisites:</p> <ul> <li>An Amazon Virtual Private Cloud (Amazon VPC)</li> <li>An AWS Security Group that allows outbound traffic</li> </ul> <p>Select the Download Launch Stack Template link to download and set up the template in your account. As part of the configuration process, you must specify the subnets and the security groups that you want associated with the Amazon EC2 instances. See the following figure for details.</p> <p>## Download Launch Stack Template </p> <p></p> <p>This is the CloudFormation stack details screen, where the stack name has been set as prometheus-autoscale. The stack parameters include a URL of the Linux installer for Prometheus, the URL for the Linux Node Exporter for Prometheus, the subnets and security groups used in the solution, the AMI and instance type to use, and the maximum capacity of the Amazon EC2 Auto Scaling group.</p> <p>The stack will take approximately eight minutes to deploy. Once complete, you will find two Amazon EC2 instances that have been deployed and are running in the Amazon EC2 Auto Scaling group that has been created for you. To validate that this solution auto-scales via Amazon Managed Service for Prometheus Alert Manager, you apply load to the Amazon EC2 instances using the AWS Systems Manager Run Command and the AWSFIS-Run-CPU-Stress automation document.</p> <p>As stress is applied to the CPUs in the Amazon EC2 Auto Scaling group, alert manager publishes these alerts, which the Lambda function responds to by scaling up the Auto Scaling group.  As CPU consumption decreases, the low CPU alert in the Amazon Managed Service for Prometheus workspace fires, alert manager publishes the alert to the Amazon SNS topic, and the Lambda function responds by responds by scaling down the Auto Scaling group, as demonstrated in the following figure.</p> <p></p> <p>The Grafana dashboard has a line showing that CPU has spiked to 100%. Although the CPU is high, another line shows that the number of instances has stepped up from 2 to 10. Once CPU has decreased, the number of instances slowly decreases back down to 2.</p>"},{"location":"recipes/recipes/as-ec2-using-amp-and-alertmanager/#costs","title":"Costs","text":"<p>Amazon Managed Service for Prometheus is priced based on the metrics ingested, metrics stored, and metrics queried. Visit the Amazon Managed Service for Prometheus pricing page for the latest pricing and pricing examples.</p> <p>Amazon SNS is priced based on the number of monthly API requests made. Message delivery between Amazon SNS and Lambda is free, but it does charge for the amount of data transferred between Amazon SNS and Lambda. See the latest Amazon SNS pricing details.</p> <p>Lambda is priced based on the duration of your function execution and the number of requests made to the function. See the latest AWS Lambda pricing details.</p> <p>There are no additional charges for using Amazon EC2 Auto Scaling.</p>"},{"location":"recipes/recipes/as-ec2-using-amp-and-alertmanager/#conclusion","title":"Conclusion","text":"<p>By using Amazon Managed Service for Prometheus, alert manager, Amazon SNS, and Lambda, you can control the scaling activities of an Amazon EC2 Auto Scaling group. The solution in this post demonstrates how you can move existing Prometheus workloads to AWS, while also utilizing Amazon EC2 Auto Scaling. As load increases to the application, it seamlessly scales to meet demand.</p> <p>In this example, the Amazon EC2 Auto Scaling group scaled based on CPU, but you can follow a similar approach for any Prometheus metric from your workload. This approach provides fine-grained control over scaling actions, thereby making sure that you can scale your workload on the metric that provides the most business value.</p> <p>In previous blog posts, we\u2019ve also demonstrated how you can use Amazon Managed Service for Prometheus Alert Manager to receive alerts with PagerDuty and how to integrate Amazon Managed Service for Prometheus with Slack. These solutions show how you can receive alerts from your workspace in the way that is most useful to you.</p> <p>For next steps, see how to create your own rules configuration file for Amazon Managed Service for Prometheus, and set up your own alert receiver. Moreover, check out Awesome Prometheus alerts for some good examples of alerting rules that can be used within alert manager.</p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/","title":"Using AWS Distro for OpenTelemetry in EKS on EC2 with Amazon Managed Service for Prometheus","text":"<p>In this recipe we show you how to instrument a sample Go application and use AWS Distro for OpenTelemetry (ADOT) to ingest metrics into Amazon Managed Service for Prometheus (AMP) . Then we're using Amazon Managed Grafana (AMG) to visualize the metrics.</p> <p>We will be setting up an Amazon Elastic Kubernetes Service (EKS) on EC2 cluster and Amazon Elastic Container Registry (ECR) repository to demonstrate a complete scenario.</p> <p>Note</p> <p>This guide will take approximately 1 hour to complete.</p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#infrastructure","title":"Infrastructure","text":"<p>In the following section we will be setting up the infrastructure for this recipe.</p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#architecture","title":"Architecture","text":"<p>The ADOT pipeline enables us to use the ADOT Collector to scrape a Prometheus-instrumented application, and ingest the scraped metrics to Amazon Managed Service for Prometheus.</p> <p></p> <p>The ADOT Collector includes two components specific to Prometheus:</p> <ul> <li>the Prometheus Receiver, and</li> <li>the AWS Prometheus Remote Write Exporter.</li> </ul> <p>Info</p> <p>For more information on Prometheus Remote Write Exporter check out: Getting Started with Prometheus Remote Write Exporter for AMP</p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#prerequisites","title":"Prerequisites","text":"<ul> <li>The AWS CLI is installed and configured in your environment.</li> <li>You need to install the eksctl command in your environment.</li> <li>You need to install kubectl in your environment.</li> <li>You have docker installed into your environment.</li> </ul>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#create-eks-on-ec2-cluster","title":"Create EKS on EC2 cluster","text":"<p>Our demo application in this recipe will be running on top of EKS. You can either use an existing EKS cluster or create one using cluster-config.yaml.</p> <p>This template will create a new cluster with two EC2 <code>t2.large</code> nodes.</p> <p>Edit the template file and set <code>&lt;YOUR_REGION&gt;</code> to one of the supported regions for AMP.</p> <p>Make sure to overwrite <code>&lt;YOUR_REGION&gt;</code> in your session, for example in bash: <pre><code>export AWS_DEFAULT_REGION=&lt;YOUR_REGION&gt;\n</code></pre></p> <p>Create your cluster using the following command. <pre><code>eksctl create cluster -f cluster-config.yaml\n</code></pre></p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#set-up-an-ecr-repository","title":"Set up an ECR repository","text":"<p>In order to deploy our application to EKS we need a container registry. You can use the following command to create a new ECR registry in your account. Make sure to set <code>&lt;YOUR_REGION&gt;</code> as well.</p> <pre><code>aws ecr create-repository \\\n    --repository-name prometheus-sample-app \\\n    --image-scanning-configuration scanOnPush=true \\\n    --region &lt;YOUR_REGION&gt;\n</code></pre>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#set-up-amp","title":"Set up AMP","text":"<p>create a workspace using the AWS CLI <pre><code>aws amp create-workspace --alias prometheus-sample-app\n</code></pre></p> <p>Verify the workspace is created using: <pre><code>aws amp list-workspaces\n</code></pre></p> <p>Info</p> <p>For more details check out the AMP Getting started guide.</p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#set-up-adot-collector","title":"Set up ADOT Collector","text":"<p>Download adot-collector-ec2.yaml and edit this YAML doc with the parameters described in the next steps.</p> <p>In this example, the ADOT Collector configuration uses an annotation <code>(scrape=true)</code> to tell which target endpoints to scrape. This allows the ADOT Collector to distinguish the sample app endpoint from <code>kube-system</code> endpoints in your cluster. You can remove this from the re-label configurations if you want to scrape a different sample app.</p> <p>Use the following steps to edit the downloaded file for your environment:</p> <p>1. Replace <code>&lt;YOUR_REGION&gt;</code> with your current region.</p> <p>2. Replace <code>&lt;YOUR_ENDPOINT&gt;</code> with the remote write URL of your workspace.</p> <p>Get your AMP remote write URL endpoint by executing the following queries.</p> <p>First, get the workspace ID like so:</p> <pre><code>YOUR_WORKSPACE_ID=$(aws amp list-workspaces \\\n                    --alias prometheus-sample-app \\\n                    --query 'workspaces[0].workspaceId' --output text)\n</code></pre> <p>Now get the remote write URL endpoint URL for your workspace using:</p> <pre><code>YOUR_ENDPOINT=$(aws amp describe-workspace \\\n                --workspace-id $YOUR_WORKSPACE_ID  \\\n                --query 'workspace.prometheusEndpoint' --output text)api/v1/remote_write\n</code></pre> <p>Warning</p> <p>Make sure that <code>YOUR_ENDPOINT</code> is in fact the remote write URL, that is, the URL should end in <code>/api/v1/remote_write</code>.</p> <p>After creating deployment file we can now apply this to our cluster by using the following command:</p> <pre><code>kubectl apply -f adot-collector-ec2.yaml\n</code></pre> <p>Info</p> <p>For more information check out the AWS Distro for OpenTelemetry (ADOT) Collector Setup.</p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#set-up-amg","title":"Set up AMG","text":"<p>Setup a new AMG workspace using the Amazon Managed Grafana \u2013 Getting Started guide.</p> <p>Make sure to add \"Amazon Managed Service for Prometheus\" as a datasource during creation.</p> <p></p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#application","title":"Application","text":"<p>In this recipe we will be using a sample application from the AWS Observability repository.</p> <p>This Prometheus sample app generates all four Prometheus metric types (counter, gauge, histogram, summary) and exposes them at the <code>/metrics</code> endpoint.</p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#build-container-image","title":"Build container image","text":"<p>To build the container image, first clone the Git repository and change into the directory as follows:</p> <pre><code>git clone https://github.com/aws-observability/aws-otel-community.git &amp;&amp; \\\ncd ./aws-otel-community/sample-apps/prometheus\n</code></pre> <p>First, set the region (if not already done above) and account ID to what is applicable in your case. Replace <code>&lt;YOUR_REGION&gt;</code> with your current region. For example, in the Bash shell this would look as follows:</p> <pre><code>export AWS_DEFAULT_REGION=&lt;YOUR_REGION&gt;\nexport ACCOUNTID=`aws sts get-caller-identity --query Account --output text`\n</code></pre> <p>Next, build the container image:</p> <pre><code>docker build . -t \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\"\n</code></pre> <p>Note</p> <p>If <code>go mod</code> fails in your environment due to a proxy.golang.or i/o timeout, you are able to bypass the go mod proxy by editing the Dockerfile.</p> <p>Change the following line in the Docker file: <pre><code>RUN GO111MODULE=on go mod download\n</code></pre> to: <pre><code>RUN GOPROXY=direct GO111MODULE=on go mod download\n</code></pre></p> <p>Now you can push the container image to the ECR repo you created earlier on.</p> <p>For that, first log in to the default ECR registry:</p> <pre><code>aws ecr get-login-password --region $AWS_DEFAULT_REGION | \\\n    docker login --username AWS --password-stdin \\\n    \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\"\n</code></pre> <p>And finally, push the container image to the ECR repository you created, above:</p> <pre><code>docker push \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\"\n</code></pre>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#deploy-sample-app","title":"Deploy sample app","text":"<p>Edit prometheus-sample-app.yaml to contain your ECR image path. That is, replace <code>ACCOUNTID</code> and <code>AWS_DEFAULT_REGION</code> in the file with your own values:</p> <pre><code>    # change the following to your container image:\n    image: \"ACCOUNTID.dkr.ecr.AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\"\n</code></pre> <p>Now you can deploy the sample app to your cluster using:</p> <pre><code>kubectl apply -f prometheus-sample-app.yaml\n</code></pre>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#end-to-end","title":"End-to-end","text":"<p>Now that you have the infrastructure and the application in place, we will test out the setup, sending metrics from the Go app running in EKS to AMP and visualize it in AMG.</p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#verify-your-pipeline-is-working","title":"Verify your pipeline is working","text":"<p>To verify if the ADOT collector is scraping the pod of the sample app and ingests the metrics into AMP, we look at the collector logs.</p> <p>Enter the following command to follow the ADOT collector logs:</p> <pre><code>kubectl -n adot-col logs adot-collector -f\n</code></pre> <p>One example output in the logs of the scraped metrics from the sample app should look like the following:</p> <pre><code>...\nResource labels:\n     -&gt; service.name: STRING(kubernetes-service-endpoints)\n     -&gt; host.name: STRING(192.168.16.238)\n     -&gt; port: STRING(8080)\n     -&gt; scheme: STRING(http)\nInstrumentationLibraryMetrics #0\nMetric #0\nDescriptor:\n     -&gt; Name: test_gauge0\n     -&gt; Description: This is my gauge\n     -&gt; Unit:\n     -&gt; DataType: DoubleGauge\nDoubleDataPoints #0\nStartTime: 0\nTimestamp: 1606511460471000000\nValue: 0.000000\n...\n</code></pre> <p>Tip</p> <p>To verify if AMP received the metrics, you can use awscurl. This tool enables you to send HTTP requests from the command line with AWS Sigv4 authentication, so you must have AWS credentials set up locally with the correct permissions to query from AMP. In the following command replace <code>$AMP_ENDPOINT</code> with the endpoint for your AMP workspace:</p> <pre><code>$ awscurl --service=\"aps\" \\\n        --region=\"$AWS_DEFAULT_REGION\" \"https://$AMP_ENDPOINT/api/v1/query?query=adot_test_gauge0\"\n{\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[{\"metric\":{\"__name__\":\"adot_test_gauge0\"},\"value\":[1606512592.493,\"16.87214000011479\"]}]}}\n</code></pre>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#create-a-grafana-dashboard","title":"Create a Grafana dashboard","text":"<p>You can import an example dashboard, available via prometheus-sample-app-dashboard.json, for the sample app that looks as follows:</p> <p></p> <p>Further, use the following guides to create your own dashboard in Amazon Managed Grafana:</p> <ul> <li>User Guide: Dashboards</li> <li>Best practices for creating dashboards</li> </ul> <p>That's it, congratulations you've learned how to use ADOT in EKS on EC2 to ingest metrics.</p>"},{"location":"recipes/recipes/ec2-eks-metrics-go-adot-ampamg/#cleanup","title":"Cleanup","text":"<ol> <li>Remove the resources and cluster <pre><code>kubectl delete all --all\neksctl delete cluster --name amp-eks-ec2\n</code></pre></li> <li>Remove the AMP workspace <pre><code>aws amp delete-workspace --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text`\n</code></pre></li> <li>Remove the amp-iamproxy-ingest-role IAM role <pre><code>aws delete-role --role-name amp-iamproxy-ingest-role\n</code></pre></li> <li>Remove the AMG workspace by removing it from the console.</li> </ol>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/","title":"Using AWS Distro for OpenTelemetry in EKS on Fargate with Amazon Managed Service for Prometheus","text":"<p>In this recipe we show you how to instrument a sample Go application and use AWS Distro for OpenTelemetry (ADOT) to ingest metrics into Amazon Managed Service for Prometheus . Then we're using Amazon Managed Grafana to visualize the metrics.</p> <p>We will be setting up an Amazon Elastic Kubernetes Service (EKS) on AWS Fargate cluster and use an Amazon Elastic Container Registry (ECR) repository to demonstrate a complete scenario.</p> <p>Note</p> <p>This guide will take approximately 1 hour to complete.</p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#infrastructure","title":"Infrastructure","text":"<p>In the following section we will be setting up the infrastructure for this recipe.</p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#architecture","title":"Architecture","text":"<p>The ADOT pipeline enables us to use the ADOT Collector to scrape a Prometheus-instrumented application, and ingest the scraped metrics to Amazon Managed Service for Prometheus.</p> <p></p> <p>The ADOT Collector includes two components specific to Prometheus:</p> <ul> <li>the Prometheus Receiver, and</li> <li>the AWS Prometheus Remote Write Exporter.</li> </ul> <p>Info</p> <p>For more information on Prometheus Remote Write Exporter check out: Getting Started with Prometheus Remote Write Exporter for AMP.</p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#prerequisites","title":"Prerequisites","text":"<ul> <li>The AWS CLI is installed and configured in your environment.</li> <li>You need to install the eksctl command in your environment.</li> <li>You need to install kubectl in your environment.</li> <li>You have Docker installed into your environment.</li> </ul>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#create-eks-on-fargate-cluster","title":"Create EKS on Fargate cluster","text":"<p>Our demo application is a Kubernetes app that we will run in an EKS on Fargate cluster. So, first create an EKS cluster using the provided cluster-config.yaml template file by changing <code>&lt;YOUR_REGION&gt;</code> to one of the supported regions for AMP.</p> <p>Make sure to set <code>&lt;YOUR_REGION&gt;</code> in your shell session, for example, in Bash:</p> <pre><code>export AWS_DEFAULT_REGION=&lt;YOUR_REGION&gt;\n</code></pre> <p>Create your cluster using the following command:</p> <pre><code>eksctl create cluster -f cluster-config.yaml\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#create-ecr-repository","title":"Create ECR repository","text":"<p>In order to deploy our application to EKS we need a container repository. You can use the following command to create a new ECR repository in your account. Make sure to set <code>&lt;YOUR_REGION&gt;</code> as well.</p> <pre><code>aws ecr create-repository \\\n    --repository-name prometheus-sample-app \\\n    --image-scanning-configuration scanOnPush=true \\\n    --region &lt;YOUR_REGION&gt;\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#set-up-amp","title":"Set up AMP","text":"<p>First, create an Amazon Managed Service for Prometheus workspace using the AWS CLI with:</p> <pre><code>aws amp create-workspace --alias prometheus-sample-app\n</code></pre> <p>Verify the workspace is created using:</p> <pre><code>aws amp list-workspaces\n</code></pre> <p>Info</p> <p>For more details check out the AMP Getting started guide.</p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#set-up-adot-collector","title":"Set up ADOT Collector","text":"<p>Download adot-collector-fargate.yaml and edit this YAML doc with the parameters described in the next steps.</p> <p>In this example, the ADOT Collector configuration uses an annotation <code>(scrape=true)</code> to tell which target endpoints to scrape. This allows the ADOT Collector to distinguish the sample app endpoint from <code>kube-system</code> endpoints in your cluster. You can remove this from the re-label configurations if you want to scrape a different sample app.</p> <p>Use the following steps to edit the downloaded file for your environment:</p> <p>1. Replace <code>&lt;YOUR_REGION&gt;</code> with your current region.</p> <p>2. Replace <code>&lt;YOUR_ENDPOINT&gt;</code> with the remote write URL of your workspace.</p> <p>Get your AMP remote write URL endpoint by executing the following queries.</p> <p>First, get the workspace ID like so:</p> <pre><code>YOUR_WORKSPACE_ID=$(aws amp list-workspaces \\\n                    --alias prometheus-sample-app \\\n                    --query 'workspaces[0].workspaceId' --output text)\n</code></pre> <p>Now get the remote write URL endpoint URL for your workspace using:</p> <pre><code>YOUR_ENDPOINT=$(aws amp describe-workspace \\\n                --workspace-id $YOUR_WORKSPACE_ID  \\\n                --query 'workspace.prometheusEndpoint' --output text)api/v1/remote_write\n</code></pre> <p>Warning</p> <p>Make sure that <code>YOUR_ENDPOINT</code> is in fact the remote write URL, that is, the URL should end in <code>/api/v1/remote_write</code>.</p> <p>After creating deployment file we can now apply this to our cluster by using the following command:</p> <pre><code>kubectl apply -f adot-collector-fargate.yaml\n</code></pre> <p>Info</p> <p>For more information check out the AWS Distro for OpenTelemetry (ADOT) Collector Setup.</p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#set-up-amg","title":"Set up AMG","text":"<p>Set up a new AMG workspace using the Amazon Managed Grafana \u2013 Getting Started guide.</p> <p>Make sure to add \"Amazon Managed Service for Prometheus\" as a datasource during creation.</p> <p></p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#application","title":"Application","text":"<p>In this recipe we will be using a sample application from the AWS Observability repository.</p> <p>This Prometheus sample app generates all four Prometheus metric types (counter, gauge, histogram, summary) and exposes them at the <code>/metrics</code> endpoint.</p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#build-container-image","title":"Build container image","text":"<p>To build the container image, first clone the Git repository and change into the directory as follows:</p> <pre><code>git clone https://github.com/aws-observability/aws-otel-community.git &amp;&amp; \\\ncd ./aws-otel-community/sample-apps/prometheus\n</code></pre> <p>First, set the region (if not already done above) and account ID to what is applicable in your case. Replace <code>&lt;YOUR_REGION&gt;</code> with your current region. For example, in the Bash shell this would look as follows:</p> <pre><code>export AWS_DEFAULT_REGION=&lt;YOUR_REGION&gt;\nexport ACCOUNTID=`aws sts get-caller-identity --query Account --output text`\n</code></pre> <p>Next, build the container image:</p> <pre><code>docker build . -t \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\"\n</code></pre> <p>Note</p> <p>If <code>go mod</code> fails in your environment due to a proxy.golang.or i/o timeout, you are able to bypass the go mod proxy by editing the Dockerfile.</p> <p>Change the following line in the Docker file: <pre><code>RUN GO111MODULE=on go mod download\n</code></pre> to: <pre><code>RUN GOPROXY=direct GO111MODULE=on go mod download\n</code></pre></p> <p>Now you can push the container image to the ECR repo you created earlier on.</p> <p>For that, first log in to the default ECR registry:</p> <pre><code>aws ecr get-login-password --region $AWS_DEFAULT_REGION | \\\n    docker login --username AWS --password-stdin \\\n    \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\"\n</code></pre> <p>And finally, push the container image to the ECR repository you created, above:</p> <pre><code>docker push \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\"\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#deploy-sample-app","title":"Deploy sample app","text":"<p>Edit prometheus-sample-app.yaml to contain your ECR image path. That is, replace <code>ACCOUNTID</code> and <code>AWS_DEFAULT_REGION</code> in the file with your own values:</p> <pre><code>    # change the following to your container image:\n    image: \"ACCOUNTID.dkr.ecr.AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\"\n</code></pre> <p>Now you can deploy the sample app to your cluster using:</p> <pre><code>kubectl apply -f prometheus-sample-app.yaml\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#end-to-end","title":"End-to-end","text":"<p>Now that you have the infrastructure and the application in place, we will test out the setup, sending metrics from the Go app running in EKS to AMP and visualize it in AMG.</p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#verify-your-pipeline-is-working","title":"Verify your pipeline is working","text":"<p>To verify if the ADOT collector is scraping the pod of the sample app and ingests the metrics into AMP, we look at the collector logs.</p> <p>Enter the following command to follow the ADOT collector logs:</p> <pre><code>kubectl -n adot-col logs adot-collector -f\n</code></pre> <p>One example output in the logs of the scraped metrics from the sample app should look like the following:</p> <pre><code>...\nResource labels:\n     -&gt; service.name: STRING(kubernetes-service-endpoints)\n     -&gt; host.name: STRING(192.168.16.238)\n     -&gt; port: STRING(8080)\n     -&gt; scheme: STRING(http)\nInstrumentationLibraryMetrics #0\nMetric #0\nDescriptor:\n     -&gt; Name: test_gauge0\n     -&gt; Description: This is my gauge\n     -&gt; Unit:\n     -&gt; DataType: DoubleGauge\nDoubleDataPoints #0\nStartTime: 0\nTimestamp: 1606511460471000000\nValue: 0.000000\n...\n</code></pre> <p>Tip</p> <p>To verify if AMP received the metrics, you can use awscurl. This tool enables you to send HTTP requests from the command line with AWS Sigv4 authentication, so you must have AWS credentials set up locally with the correct permissions to query from AMP. In the following command replace <code>$AMP_ENDPOINT</code> with the endpoint for your AMP workspace:</p> <pre><code>$ awscurl --service=\"aps\" \\\n        --region=\"$AWS_DEFAULT_REGION\" \"https://$AMP_ENDPOINT/api/v1/query?query=adot_test_gauge0\"\n{\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[{\"metric\":{\"__name__\":\"adot_test_gauge0\"},\"value\":[1606512592.493,\"16.87214000011479\"]}]}}\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#create-a-grafana-dashboard","title":"Create a Grafana dashboard","text":"<p>You can import an example dashboard, available via prometheus-sample-app-dashboard.json, for the sample app that looks as follows:</p> <p></p> <p>Further, use the following guides to create your own dashboard in Amazon Managed Grafana:</p> <ul> <li>User Guide: Dashboards</li> <li>Best practices for creating dashboards</li> </ul> <p>That's it, congratulations you've learned how to use ADOT in EKS on Fargate to ingest metrics.</p>"},{"location":"recipes/recipes/fargate-eks-metrics-go-adot-ampamg/#cleanup","title":"Cleanup","text":"<p>First remove the Kubernetes resources and destroy the EKS cluster:</p> <pre><code>kubectl delete all --all &amp;&amp; \\\neksctl delete cluster --name amp-eks-fargate\n</code></pre> <p>Remove the Amazon Managed Service for Prometheus workspace:</p> <pre><code>aws amp delete-workspace --workspace-id \\\n    `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text`\n</code></pre> <p>Remove the  IAM role:</p> <pre><code>aws delete-role --role-name adot-collector-role\n</code></pre> <p>Finally, remove the Amazon Managed Grafana  workspace by removing it via the AWS console.</p>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/","title":"Using AWS Distro for OpenTelemetry in EKS on Fargate with AWS X-Ray","text":"<p>In this recipe we show you how to instrument a sample Go application and use AWS Distro for OpenTelemetry (ADOT) to  ingest traces into AWS X-Ray and visualize the traces in Amazon Managed Grafana.</p> <p>We will be setting up an Amazon Elastic Kubernetes Service (EKS) on AWS Fargate cluster and use an Amazon Elastic Container Registry (ECR) repository to demonstrate a complete scenario.</p> <p>Note</p> <p>This guide will take approximately 1 hour to complete.</p>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#infrastructure","title":"Infrastructure","text":"<p>In the following section we will be setting up the infrastructure for this recipe. </p>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#architecture","title":"Architecture","text":"<p>The ADOT pipeline enables us to use the  ADOT Collector to  collect traces from an instrumented app and ingest them into X-Ray:</p> <p></p>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#prerequisites","title":"Prerequisites","text":"<ul> <li>The AWS CLI is installed and configured in your environment.</li> <li>You need to install the eksctl command in your environment.</li> <li>You need to install kubectl in your environment. </li> <li>You have Docker installed into your environment.</li> <li>You have the aws-observability/aws-o11y-recipes   repo cloned into your local environment.</li> </ul>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#create-eks-on-fargate-cluster","title":"Create EKS on Fargate cluster","text":"<p>Our demo application is a Kubernetes app that we will run in an EKS on Fargate cluster. So, first create an EKS cluster using the provided cluster_config.yaml.</p> <p>Create your cluster using the following command:</p> <pre><code>eksctl create cluster -f cluster-config.yaml\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#create-ecr-repository","title":"Create ECR repository","text":"<p>In order to deploy our application to EKS we need a container repository. We will use the private ECR registry, but you can also use ECR Public, if you want to share the container image.</p> <p>First, set the environment variables, such as shown here (substitute for your region):</p> <pre><code>export REGION=\"eu-west-1\"\nexport ACCOUNTID=`aws sts get-caller-identity --query Account --output text`\n</code></pre> <p>You can use the following command to create a new ECR repository in your account: </p> <pre><code>aws ecr create-repository \\\n    --repository-name ho11y \\\n    --image-scanning-configuration scanOnPush=true \\\n    --region $REGION\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#set-up-adot-collector","title":"Set up ADOT Collector","text":"<p>Download adot-collector-fargate.yaml  and edit this YAML doc with the parameters described in the next steps.</p> <pre><code>kubectl apply -f adot-collector-fargate.yaml\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#set-up-managed-grafana","title":"Set up Managed Grafana","text":"<p>Set up a new workspace using the  Amazon Managed Grafana \u2013 Getting Started guide and add X-Ray as a data source.</p>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#signal-generator","title":"Signal generator","text":"<p>We will be using <code>ho11y</code>, a synthetic signal generator available via the sandbox of the recipes repository. So, if you haven't cloned the repo into your local environment, do now:</p> <pre><code>git clone https://github.com/aws-observability/aws-o11y-recipes.git\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#build-container-image","title":"Build container image","text":"<p>Make sure that your <code>ACCOUNTID</code> and <code>REGION</code> environment variables are set,  for example:</p> <p><pre><code>export REGION=\"eu-west-1\"\nexport ACCOUNTID=`aws sts get-caller-identity --query Account --output text`\n</code></pre> To build the <code>ho11y</code> container image, first change into the <code>./sandbox/ho11y/</code> directory and build the container image :</p> <p>Note</p> <p>The following build step assumes that the Docker daemon or an equivalent OCI image  build tool is running.</p> <pre><code>docker build . -t \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/ho11y:latest\"\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#push-container-image","title":"Push container image","text":"<p>Next, you can push the container image to the ECR repo you created earlier on. For that, first log in to the default ECR registry:</p> <pre><code>aws ecr get-login-password --region $REGION | \\\n    docker login --username AWS --password-stdin \\\n    \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com\"\n</code></pre> <p>And finally, push the container image to the ECR repository you created, above:</p> <pre><code>docker push \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/ho11y:latest\"\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#deploy-signal-generator","title":"Deploy signal generator","text":"<p>Edit x-ray-sample-app.yaml to contain your ECR image path. That is, replace <code>ACCOUNTID</code> and <code>REGION</code> in the file with your own values (overall, in three locations):</p> <pre><code>    # change the following to your container image:\n    image: \"ACCOUNTID.dkr.ecr.REGION.amazonaws.com/ho11y:latest\"\n</code></pre> <p>Now you can deploy the sample app to your cluster using:</p> <pre><code>kubectl -n example-app apply -f x-ray-sample-app.yaml\n</code></pre>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#end-to-end","title":"End-to-end","text":"<p>Now that you have the infrastructure and the application in place, we will test out the setup, sending traces from <code>ho11y</code> running in EKS to X-Ray and visualize it in AMG.</p>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#verify-pipeline","title":"Verify pipeline","text":"<p>To verify if the ADOT collector is ingesting traces from <code>ho11y</code>, we make one of the services available locally and invoke it.</p> <p>First, let's forward traffic as so:</p> <pre><code>kubectl -n example-app port-forward svc/frontend 8765:80\n</code></pre> <p>With above command, the <code>frontend</code> microservice (a <code>ho11y</code> instance configured to talk to two other <code>ho11y</code> instances) is available in your local environment and you can invoke it as follows (triggering the creation of traces):</p> <pre><code>$ curl localhost:8765/\n{\"traceId\":\"1-6193a9be-53693f29a0119ee4d661ba0d\"}\n</code></pre> <p>Tip</p> <p>If you want to automate the invocation, you can wrap the <code>curl</code> call into a <code>while true</code> loop.</p> <p>To verify our setup, visit the X-Ray view in CloudWatch where you should see something like shown below:</p> <p></p> <p>Now that we have the signal generator set up and active and the OpenTelemetry pipeline set up, let's see how to consume the traces in Grafana.</p>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#grafana-dashboard","title":"Grafana dashboard","text":"<p>You can import an example dashboard, available via x-ray-sample-dashboard.json that looks as follows:</p> <p></p> <p>Further, when you click on any of the traces in the lower <code>downstreams</code> panel, you can dive into it and view it in the \"Explore\" tab like so:</p> <p></p> <p>From here, you can use the following guides to create your own dashboard in Amazon Managed Grafana:</p> <ul> <li>User Guide: Dashboards</li> <li>Best practices for creating dashboards</li> </ul> <p>That's it, congratulations you've learned how to use ADOT in EKS on Fargate to  ingest traces.</p>"},{"location":"recipes/recipes/fargate-eks-xray-go-adot-amg/#cleanup","title":"Cleanup","text":"<p>First remove the Kubernetes resources and destroy the EKS cluster:</p> <p><pre><code>kubectl delete all --all &amp;&amp; \\\neksctl delete cluster --name xray-eks-fargate\n</code></pre> Finally, remove the Amazon Managed Grafana workspace by removing it via the AWS console. </p>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/","title":"Exporting CloudWatch Metric Streams via Firehose and AWS Lambda to Amazon Managed Service for Prometheus","text":"<p>In this recipe we show you how to instrument a CloudWatch Metric Stream and use Kinesis Data Firehose and AWS Lambda to ingest metrics into Amazon Managed Service for Prometheus (AMP).</p> <p>We will be setting up a stack using AWS Cloud Development Kit (CDK) to create a Firehose Delivery Stream, Lambda, and a S3 Bucket to demonstrate a complete scenario.</p> <p>Note</p> <p>This guide will take approximately 30 minutes to complete.</p>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/#infrastructure","title":"Infrastructure","text":"<p>In the following section we will be setting up the infrastructure for this recipe. </p> <p>CloudWatch Metric Streams allow forwarding of the streaming metric data to a  HTTP endpoint or S3 bucket.</p>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/#prerequisites","title":"Prerequisites","text":"<ul> <li>The AWS CLI is installed and configured in your environment.</li> <li>The AWS CDK Typescript is installed in your environment.</li> <li>Node.js and Go.</li> <li>The repo has been cloned to your local machine. The code for this project is under <code>/sandbox/CWMetricStreamExporter</code>.</li> </ul>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/#create-an-amp-workspace","title":"Create an AMP workspace","text":"<p>Our demo application in this recipe will be running on top of AMP.  Create your AMP Workspace via the following command:</p> <pre><code>aws amp create-workspace --alias prometheus-demo-recipe\n</code></pre> <p>Ensure your workspace has been created with the following command: <pre><code>aws amp list-workspaces\n</code></pre></p> <p>Info</p> <p>For more details check out the AMP Getting started guide.</p>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/#install-dependencies","title":"Install dependencies","text":"<p>From the root of the aws-o11y-recipes repository, change your directory to CWMetricStreamExporter via the command:</p> <pre><code>cd sandbox/CWMetricStreamExporter\n</code></pre> <p>This will now be considered the root of the repo, going forward.</p> <p>Change directory to <code>/cdk</code> via the following command:</p> <pre><code>cd cdk\n</code></pre> <p>Install the CDK dependencies via the following command:</p> <pre><code>npm install\n</code></pre> <p>Change directory back to the root of the repo, and then change directory  to <code>/lambda</code> using following command:</p> <pre><code>cd lambda\n</code></pre> <p>Once in the <code>/lambda</code> folder, install the Go dependencies using:</p> <pre><code>go get\n</code></pre> <p>All the dependencies are now installed.</p>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/#modify-config-file","title":"Modify config file","text":"<p>In the root of the repo, open <code>config.yaml</code> and modify the AMP workspace URL  by replacing the <code>{workspace}</code> with the newly created workspace id, and the  region your AMP workspace is in.</p> <p>For example, modify the following with:</p> <pre><code>AMP:\n    remote_write_url: \"https://aps-workspaces.us-east-2.amazonaws.com/workspaces/{workspaceId}/api/v1/remote_write\"\n    region: us-east-2\n</code></pre> <p>Change the names of the Firehose Delivery Stream and S3 Bucket to your liking.</p>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/#deploy-stack","title":"Deploy stack","text":"<p>Once the <code>config.yaml</code> has been modified with the AMP workspace ID, it is time  to deploy the stack to CloudFormation. To build the CDK and the Lambda code,  in the root of the repo run the following commend:</p> <pre><code>npm run build\n</code></pre> <p>This build step ensures that the Go Lambda binary is built, and deploys the CDK to CloudFormation.</p> <p>Accept the following IAM changes to deploy the stack:</p> <p></p> <p>Verify that the stack has been created by running the following command:</p> <pre><code>aws cloudformation list-stacks\n</code></pre> <p>A stack by the name <code>CDK Stack</code> should have been created.</p>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/#create-cloudwatch-stream","title":"Create CloudWatch stream","text":"<p>Navigate to the CloudWatch consoloe, for example  <code>https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#metric-streams:streamsList</code>  and click \"Create metric stream\".</p> <p>Select the metics needed, either all metrics of only from selected namespaces.</p> <p>Configure the Metric Stream by using an existing Firehose which was created by the CDK. Change the output format to JSON instead of OpenTelemetry 0.7. Modify the Metric Stream name to your liking, and click \"Create metric stream\":</p> <p></p> <p>To verify the Lambda function invocation, navigate to the Lambda console and click the function <code>KinesisMessageHandler</code>. Click the <code>Monitor</code> tab and <code>Logs</code> subtab, and under <code>Recent Invocations</code> there should be entries of the Lambda function being triggered.</p> <p>Note</p> <p>It may take upto 5 minutes for invocations to show in the Monitor tab.</p> <p>That is it! Congratulations, your metrics are now being streamed from CloudWatch to Amazon Managed Service for Prometheus.</p>"},{"location":"recipes/recipes/lambda-cw-metrics-go-amp/#cleanup","title":"Cleanup","text":"<p>First, delete the CloudFormation stack:</p> <pre><code>cd cdk\ncdk destroy\n</code></pre> <p>Remove the AMP workspace:</p> <pre><code>aws amp delete-workspace --workspace-id \\\n    `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text`\n</code></pre> <p>Last but not least, remove the CloudWatch Metric Stream by removing it from the console.</p>"},{"location":"recipes/recipes/metrics-explorer-filter-by-tags/","title":"Using Amazon CloudWatch Metrics explorer to aggregate and visualize metrics filtered by resource tags","text":"<p>In this recipe we show you how to use Metrics explorer to filter, aggregate, and visualize metrics by resource tags and resource properties - Use metrics explorer to monitor resources by their tags and properties.</p> <p>There are number of ways to create visualizations with Metrics explorer; in this walkthrough we simply leverage the AWS Console.</p> <p>Note</p> <p>This guide will take approximately 5 minutes to complete.</p>"},{"location":"recipes/recipes/metrics-explorer-filter-by-tags/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to AWS account</li> <li>Access to Amazon CloudWatch Metrics explorer via AWS Console</li> <li>Resource tags set for the relevant resources </li> </ul>"},{"location":"recipes/recipes/metrics-explorer-filter-by-tags/#metrics-explorer-tag-based-queries-and-visualizations","title":"Metrics Explorer tag based queries and visualizations","text":"<ul> <li> <p>Open the CloudWatch console </p> </li> <li> <p>Under Metrics, click on the Explorer menu </p> </li> </ul> <p></p> <ul> <li>You can either choose from one of the Generic templates or a Service based templates list; in this example we use the EC2 Instances by type template </li> </ul> <p></p> <ul> <li>Choose metrics you would like to explore; remove obsolete once, and add other metrics you would like to see</li> </ul> <p></p> <ul> <li>Under From, choose a resource tag or a resource property you are looking for; in the below example we show number of CPU and Network related metrics for different EC2 instances with Name: TeamX Tag</li> </ul> <p></p> <ul> <li>Please note, you can combine time series using an aggregation function under Aggregated by; in the below example TeamX metrics are aggregated by Availability Zone</li> </ul> <p></p> <p>Alternatively, you could aggregate TeamX and TeamY by the Team Tag, or choose any other configuration that suits your needs</p> <p></p>"},{"location":"recipes/recipes/metrics-explorer-filter-by-tags/#dynamic-visualizations","title":"Dynamic visualizations","text":"<p>You can easily customize resulting visualizations by using From, Aggregated by and Split by options. Metrics explorer visualizations are dynamic, so any new tagged resource automatically appears in the explorer widget.</p>"},{"location":"recipes/recipes/metrics-explorer-filter-by-tags/#reference","title":"Reference","text":"<p>For more information on Metrics explorer please refer to the following article: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metrics-Explorer.html</p>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/","title":"Monitoring hybrid environments using Amazon Managed Service for Grafana","text":"<p>In this recipe we show you how to visualize metrics from an Azure Cloud environment to Amazon Managed Service for Grafana (AMG) and create alert notifications in AMG to be sent to Amazon Simple Notification Service and Slack.</p> <p>As part of the implementation, we will create an AMG workspace, configure the Azure Monitor plugin as the data source for AMG and configure the Grafana dashboard. We will be creating two notification channels: one for Amazon SNS and one for slack.We will also configure alerts in the AMG dashboard to be sent to the notification channels.</p> <p>Note</p> <p>This guide will take approximately 30 minutes to complete.</p>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/#infrastructure","title":"Infrastructure","text":"<p>In the following section we will be setting up the infrastructure for this recipe. </p>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/#prerequisites","title":"Prerequisites","text":"<ul> <li>The AWS CLI is installed and configured in your environment.</li> <li>You need to enable AWS-SSO</li> </ul>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/#architecture","title":"Architecture","text":"<p>First, create an AMG workspace to visualize the metrics from Azure Monitor. Follow the steps in the Getting Started with Amazon Managed Service for Grafana blog post. After you create the workspace, you can assign access to the Grafana workspace to an individual user or a user group. By default, the user has a user type of viewer. Change the user type based on the user role.</p> <p>Note</p> <p>You must assign an Admin role to at least one user in the workspace.</p> <p>In Figure 1, the user name is grafana-admin. The user type is Admin. On the Data sources tab, choose the required data source. Review the configuration, and then choose Create workspace. </p>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/#configure-the-data-source-and-custom-dashboard","title":"Configure the data source and custom dashboard","text":"<p>Now, under Data sources, configure the Azure Monitor plugin to start querying and visualizing the metrics from the Azure environment. Choose Data sources to add a data source. </p> <p>In Add data source, search for Azure Monitor and then configure the parameters from the app registration console in the Azure environment. </p> <p>To configure the Azure Monitor plugin, you need the directory (tenant) ID and the application (client) ID. For instructions, see the article about creating an Azure AD application and service principal. It explains how to register the app and grant access to Grafana to query the data.</p> <p></p> <p>After the data source is configured, import a custom dashboard to analyze the Azure metrics. In the left pane, choose the + icon, and then choose Import.</p> <p>In Import via grafana.com, enter the dashboard ID, 10532.</p> <p></p> <p>This will import the Azure Virtual Machine dashboard where you can start analyzing the Azure Monitor metrics. In my setup, I have a virtual machine running in the Azure environment.</p> <p></p>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/#configure-the-notification-channels-on-amg","title":"Configure the notification channels on AMG","text":"<p>In this section, you\u2019ll configure two notifications channels and then send alerts.</p> <p>Use the following command to create an SNS topic named grafana-notification and subscribe an email address.</p> <p><pre><code>aws sns create-topic --name grafana-notification\naws sns subscribe --topic-arn arn:aws:sns:&lt;region&gt;:&lt;account-id&gt;:grafana-notification --protocol email --notification-endpoint &lt;email-id&gt;\n</code></pre> In the left pane, choose the bell icon to add a new notification channel. Now configure the grafana-notification notification channel. On Edit notification channel, for Type, choose AWS SNS. For Topic, use the ARN of the SNS topic you just created. For Auth Provider, choose the workspace IAM role.</p> <p></p>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/#slack-notification-channel","title":"Slack notification channel","text":"<p>To configure a Slack notification channel, create a Slack workspace or use an existing one. Enable Incoming Webhooks as described in Sending messages using Incoming Webhooks.</p> <p>After you\u2019ve configured the workspace, you should be able to get a webhook URL that will be used in the Grafana dashboard.</p> <p></p>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/#configure-alerts-in-amg","title":"Configure alerts in AMG","text":"<p>You can configure Grafana alerts when the metric increases beyond the threshold. With AMG, you can configure how often the alert must be evaluated in the dashboard and send the notification. In this example, configure an alert for CPU utilization for an Azure virtual machine. When the utilization exceeds a threshold, configure AMG to send notifications to both channels.</p> <p>In the dashboard, choose CPU utilization from the dropdown, and then choose Edit. On the Alert tab of the graph panel, configure how often the alert rule should be evaluated and the conditions that must be met for the alert to change state and initiate its notifications.</p> <p>In the following configuration, an alert is created if the CPU utilization exceeds 50%. Notifications will be sent to the grafana-alert-notification and slack-alert-notification channels.</p> <p></p> <p>Now, you can sign in to the Azure virtual machine and initiate stress testing using tools like stress. When the CPU utilization exceeds the threshold, you will receive notifications on both channels.</p> <p>Now configure alerts for CPU utilization with the right threshold to simulate an alert that is sent to the Slack channel.</p>"},{"location":"recipes/recipes/monitoring-hybridenv-amg/#conclusion","title":"Conclusion","text":"<p>In the recipe, we showed you how to deploy the AMG workspace, configure notification channels, collect metrics from Azure Cloud, and configure alerts on the AMG dashboard. Because AMG is a fully managed, serverless solution, you can spend your time on the applications that transform your business and leave the heavy lifting of managing Grafana to AWS.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/","title":"Using Amazon Managed Service for Prometheus to monitor App Mesh environment configured on EKS","text":"<p>In this recipe we show you how to ingest App Mesh Envoy  metrics in an Amazon Elastic Kubernetes Service (EKS) cluster  to Amazon Managed Service for Prometheus (AMP) and create a custom dashboard on Amazon Managed Grafana  (AMG) to monitor the health and performance of microservices.</p> <p>As part of the implementation, we will create an AMP workspace, install the App Mesh  Controller for Kubernetes and inject the Envoy container into the pods. We will be  collecting the Envoy metrics using Grafana Agent  configured in the EKS cluster and write them to AMP. Finally, we will be creating an AMG workspace and configure the AMP as the datasource and create a custom dashboard.</p> <p>Note</p> <p>This guide will take approximately 45 minutes to complete.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#infrastructure","title":"Infrastructure","text":"<p>In the following section we will be setting up the infrastructure for this recipe. </p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#architecture","title":"Architecture","text":"<p>The Grafana agent is configured to scrape the Envoy metrics and ingest them to AMP through the AMP remote write endpoint </p> <p>Info</p> <p>For more information on Prometheus Remote Write Exporter check out Getting Started with Prometheus Remote Write Exporter for AMP.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#prerequisites","title":"Prerequisites","text":"<ul> <li>The AWS CLI is installed and configured in your environment.</li> <li>You need to install the eksctl command in your environment.</li> <li>You need to install kubectl in your environment. </li> <li>You have Docker installed into your environment.</li> <li>You need AMP workspace configured in your AWS account.</li> <li>You need to install Helm.</li> <li>You need to enable AWS-SSO.</li> </ul>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#setup-an-eks-cluster","title":"Setup an EKS cluster","text":"<p>First, create an EKS cluster that will be enabled with App Mesh for running the sample application.  The <code>eksctl</code> CLI will be used to deploy the cluster using the eks-cluster-config.yaml. This template will create a new cluster with EKS.</p> <p>Edit the template file and set your region to one of the available regions for AMP:</p> <ul> <li><code>us-east-1</code></li> <li><code>us-east-2</code></li> <li><code>us-west-2</code></li> <li><code>eu-central-1</code></li> <li><code>eu-west-1</code></li> </ul> <p>Make sure to overwrite this region in your session, for example, in the Bash shell:</p> <pre><code>export AWS_REGION=eu-west-1\n</code></pre> <p>Create your cluster using the following command:</p> <p><pre><code>eksctl create cluster -f eks-cluster-config.yaml\n</code></pre> This creates an EKS cluster named <code>AMP-EKS-CLUSTER</code> and a service account  named <code>appmesh-controller</code> that will be used by the App Mesh controller for EKS.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#install-app-mesh-controller","title":"Install App Mesh Controller","text":"<p>Next, we will run the below commands to install the App Mesh Controller  and configure the Custom Resource Definitions (CRDs): </p> <pre><code>helm repo add eks https://aws.github.io/eks-charts\n</code></pre> <pre><code>helm upgrade -i appmesh-controller eks/appmesh-controller \\\n     --namespace appmesh-system \\\n     --set region=${AWS_REGION} \\\n     --set serviceAccount.create=false \\\n     --set serviceAccount.name=appmesh-controller\n</code></pre>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#set-up-amp","title":"Set up AMP","text":"<p>The AMP workspace is used to ingest the Prometheus metrics collected from Envoy.  A workspace is a logical Cortex server dedicated to a tenant. A workspace supports fine-grained access control for authorizing its management such as update, list,  describe, and delete, and the ingestion and querying of metrics.</p> <p>Create a workspace using the AWS CLI:</p> <pre><code>aws amp create-workspace --alias AMP-APPMESH --region $AWS_REGION\n</code></pre> <p>Add the necessary Helm repositories:</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts &amp;&amp; \\\nhelm repo add kube-state-metrics https://kubernetes.github.io/kube-state-metrics \n</code></pre> <p>For more details on AMP check out the AMP Getting started guide.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#scraping-ingesting-metrics","title":"Scraping &amp; ingesting metrics","text":"<p>AMP does not directly scrape operational metrics from containerized workloads in a Kubernetes cluster.  You must deploy and manage a Prometheus server or an OpenTelemetry agent such as the AWS Distro for OpenTelemetry Collector or the Grafana Agent to perform this task. In this receipe, we walk you through the  process of configuring the Grafana Agent to scrape the Envoy metrics and analyze them using AMP and AMG.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#configure-grafana-agent","title":"Configure Grafana Agent","text":"<p>The Grafana Agent is a lightweight alternative to running a full Prometheus server.  It keeps the necessary parts for discovering and scraping Prometheus exporters and  sending metrics to a Prometheus-compatible backend. The Grafana Agent also includes  native support for AWS Signature Version 4 (Sigv4) for AWS Identity and Access Management (IAM)  authentication.</p> <p>We now walk you through the steps to configure an IAM role to send Prometheus metrics to AMP.  We install the Grafana Agent on the EKS cluster and forward metrics to AMP.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#configure-permissions","title":"Configure permissions","text":"<p>The Grafana Agent scrapes operational metrics from containerized workloads running in the  EKS cluster and sends them to AMP. Data sent to AMP must be signed with valid AWS credentials using Sigv4 to authenticate and authorize each client request for the managed service.</p> <p>The Grafana Agent can be deployed to an EKS cluster to run under the identity of a Kubernetes service account.  With IAM roles for service accounts (IRSA), you can associate an IAM role with a Kubernetes service account and thus provide IAM permissions to any pod that uses the service account. </p> <p>Prepare the IRSA setup as follows:</p> <pre><code>kubectl create namespace grafana-agent\n\nexport WORKSPACE=$(aws amp list-workspaces | jq -r '.workspaces[] | select(.alias==\"AMP-APPMESH\").workspaceId')\nexport ROLE_ARN=$(aws iam get-role --role-name EKS-GrafanaAgent-AMP-ServiceAccount-Role --query Role.Arn --output text)\nexport NAMESPACE=\"grafana-agent\"\nexport REMOTE_WRITE_URL=\"https://aps-workspaces.$AWS_REGION.amazonaws.com/workspaces/$WORKSPACE/api/v1/remote_write\"\n</code></pre> <p>You can use the gca-permissions.sh  shell script to automate the following steps (note to replace the placeholder variable  <code>YOUR_EKS_CLUSTER_NAME</code> with the name of your EKS cluster):</p> <ul> <li>Creates an IAM role named <code>EKS-GrafanaAgent-AMP-ServiceAccount-Rol</code>e with an IAM policy that has permissions to remote-write into an AMP workspace.</li> <li>Creates a Kubernetes service account named <code>grafana-agent</code> under the <code>grafana-agent</code> namespace that is associated with the IAM role.</li> <li>Creates a trust relationship between the IAM role and the OIDC provider hosted in your Amazon EKS cluster.</li> </ul> <p>You need <code>kubectl</code> and <code>eksctl</code> CLI tools to run the <code>gca-permissions.sh</code> script.  They must be configured to access your Amazon EKS cluster.</p> <p>Now create a manifest file, grafana-agent.yaml,  with the scrape configuration to extract Envoy metrics and deploy the Grafana Agent. </p> <p>Note</p> <p>At time of writing, this solution will not work for EKS on Fargate due to the lack of support for daemon sets there.</p> <p>The example deploys a daemon set named <code>grafana-agent</code> and a deployment named  <code>grafana-agent-deployment</code>. The <code>grafana-agent</code> daemon set collects metrics  from pods on the cluster and the <code>grafana-agent-deployment</code> deployment collects metrics from services that do not live on the cluster, such as the EKS control plane.</p> <p><pre><code>kubectl apply -f grafana-agent.yaml\n</code></pre> After the <code>grafana-agent</code> is deployed, it will collect the metrics and ingest  them into the specified AMP workspace. Now deploy a sample application on the  EKS cluster and start analyzing the metrics.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#sample-application","title":"Sample application","text":"<p>To install an application and inject an Envoy container, we use the AppMesh controller for Kubernetes.</p> <p>First, install the base application by cloning the examples repo:</p> <pre><code>git clone https://github.com/aws/aws-app-mesh-examples.git\n</code></pre> <p>And now apply the resources to your cluster:</p> <pre><code>kubectl apply -f aws-app-mesh-examples/examples/apps/djapp/1_base_application\n</code></pre> <p>Check the pod status and make sure it is running:</p> <pre><code>$ kubectl -n prod get all\n\nNAME                            READY   STATUS    RESTARTS   AGE\npod/dj-cb77484d7-gx9vk          1/1     Running   0          6m8s\npod/jazz-v1-6b6b6dd4fc-xxj9s    1/1     Running   0          6m8s\npod/metal-v1-584b9ccd88-kj7kf   1/1     Running   0          6m8s\n</code></pre> <p>Next, install the App Mesh controller and meshify the deployment:</p> <pre><code>kubectl apply -f aws-app-mesh-examples/examples/apps/djapp/2_meshed_application/\nkubectl rollout restart deployment -n prod dj jazz-v1 metal-v1\n</code></pre> <p>Now we should see two containers running in each pod:</p> <pre><code>$ kubectl -n prod get all\nNAME                        READY   STATUS    RESTARTS   AGE\ndj-7948b69dff-z6djf         2/2     Running   0          57s\njazz-v1-7cdc4fc4fc-wzc5d    2/2     Running   0          57s\nmetal-v1-7f499bb988-qtx7k   2/2     Running   0          57s\n</code></pre> <p>Generate the traffic for 5 mins and we will visualize it AMG later:</p> <pre><code>dj_pod=`kubectl get pod -n prod --no-headers -l app=dj -o jsonpath='{.items[*].metadata.name}'`\n\nloop_counter=0\nwhile [ $loop_counter -le 300 ] ; do \\\nkubectl exec -n prod -it $dj_pod  -c dj \\\n-- curl jazz.prod.svc.cluster.local:9080 ; echo ; loop_counter=$[$loop_counter+1] ; \\\ndone\n</code></pre>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#create-an-amg-workspace","title":"Create an AMG workspace","text":"<p>To create an AMG workspace follow the steps in the Getting Started with AMG blog post.  To grant users access to the dashboard, you must enable AWS SSO. After you create the workspace, you can assign access to the Grafana workspace to an individual user or a user group.  By default, the user has a user type of viewer. Change the user type based on the user role. Add the AMP workspace as the data source and then start creating the dashboard.</p> <p>In this example, the user name is <code>grafana-admin</code> and the user type is <code>Admin</code>. Select the required data source. Review the configuration, and then choose <code>Create workspace</code>.</p> <p></p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#configure-amg-datasource","title":"Configure AMG datasource","text":"<p>To configure AMP as a data source in AMG, in the <code>Data sources</code> section, choose  <code>Configure in Grafana</code>, which will launch a Grafana workspace in the browser.  You can also manually launch the Grafana workspace URL in the browser.</p> <p></p> <p>As you can see from the screenshots, you can view Envoy metrics like downstream  latency, connections, response code, and more. You can use the filters shown to  drill down to the envoy metrics of a particular application.</p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#configure-amg-dashboard","title":"Configure AMG dashboard","text":"<p>After the data source is configured, import a custom dashboard to analyze the Envoy metrics.  For this we use a pre-defined dashboard, so choose <code>Import</code> (shown below), and  then enter the ID <code>11022</code>. This will import the Envoy Global dashboard so you can  start analyzing the Envoy metrics.</p> <p></p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#configure-alerts-on-amg","title":"Configure alerts on AMG","text":"<p>You can configure Grafana alerts when the metric increases beyond the intended threshold.  With AMG, you can configure how often the alert must be evaluated in the dashboard and send the notification.  Before you create alert rules, you must create a notification channel.</p> <p>In this example, configure Amazon SNS as a notification channel. The SNS topic must be  prefixed with <code>grafana</code> for notifications to be successfully published to the topic if you use the defaults, that is, the service-managed permissions.</p> <p>Use the following command to create an SNS topic named <code>grafana-notification</code>:</p> <pre><code>aws sns create-topic --name grafana-notification\n</code></pre> <p>And subscribe to it via an email address. Make sure you specify the region and Account ID in the  below command:</p> <pre><code>aws sns subscribe \\\n    --topic-arn arn:aws:sns:&lt;region&gt;:&lt;account-id&gt;:grafana-notification \\\n    --protocol email \\\n    --notification-endpoint &lt;email-id&gt;\n</code></pre> <p>Now, add a new notification channel from the Grafana dashboard. Configure the new notification channel named grafana-notification. For Type,  use AWS SNS from the drop down. For Topic, use the ARN of the SNS topic you just created.  For Auth provider, choose AWS SDK Default.</p> <p></p> <p>Now configure an alert if downstream latency exceeds five milliseconds in a one-minute period. In the dashboard, choose Downstream latency from the dropdown, and then choose Edit.  On the Alert tab of the graph panel, configure how often the alert rule should be evaluated  and the conditions that must be met for the alert to change state and initiate its notifications.</p> <p>In the following configuration, an alert is created if the downstream latency exceeds the  threshold and notification will be sent through the configured grafana-alert-notification channel to the SNS topic.</p> <p></p>"},{"location":"recipes/recipes/servicemesh-monitoring-ampamg/#cleanup","title":"Cleanup","text":"<ol> <li>Remove the resources and cluster: <pre><code>kubectl delete all --all\neksctl delete cluster --name AMP-EKS-CLUSTER\n</code></pre></li> <li>Remove the AMP workspace: <pre><code>aws amp delete-workspace --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text`\n</code></pre></li> <li>Remove the amp-iamproxy-ingest-role IAM role: <pre><code>aws delete-role --role-name amp-iamproxy-ingest-role\n</code></pre></li> <li>Remove the AMG workspace by removing it from the console. </li> </ol>"},{"location":"recipes/recipes/Workspaces-Monitoring-AMP-AMG/","title":"Index","text":"<p>Organizations have started adopting Amazon Workspaces as virtual cloud based desktop as a solution (DAAS) to replace their existing traditional desktop solution to shift the cost and effort of maintaining laptops and desktops to a cloud pay-as-you-go model. Organizations using Amazon Workspaces would need support of these managed services to monitor their workspaces environment for Day 2 operations. A cloud based managed open source monitoring solution such as Amazon Managed Service for Prometheus and Amazon Managed Grafana helps IT teams to quickly setup and operate a monitoring solution to save cost. Monitoring CPU, memory, network, or disk activity from Amazon Workspace eliminates guesswork while troubleshooting Amazon Workspaces environment.</p> <p>A managed monitoring solution on your Amazon Workspaces environments yields following organizational benefits:</p> <ul> <li>Service desk staff can quickly identify and drill down to Amazon Workspace issues that need investigation without guesswork by leveraging managed monitoring services such as Amazon Managed Service for Prometheus and Amazon Managed Grafana</li> <li>Service desks staffs can investigate Amazon Workspace issues after the event using the historical data in Amazon Managed Service for Prometheus</li> <li>Eliminates long calls that waste time questioning business users on Amazon Workspaces issues</li> </ul> <p>In this blog post, we will set up Amazon Managed Service for Prometheus, Amazon Managed Grafana, and a Prometheus server on Amazon Elastic Compute Cloud (EC2) to provide a monitoring solution for Amazon Workspaces.  We will automate the deployment of Prometheus agents on any new Amazon Workspace using Active Directory Group Policy Objects (GPO).</p> <p>Solution Architecture</p> <p>The following diagram demonstrates the solution to monitor your Amazon Workspaces environment using AWS native managed services such as Amazon Managed Service for Prometheus and Amazon Managed Grafana. This solution will deploy a Prometheus server on Amazon Elastic Compute Cloud (EC2) instance which polls prometheus agents on your Amazon Workspace periodically and remote writes metrics to Amazon Managed Service for Prometheus. We will be using Amazon Managed Grafana to query and visualize metrics on your Amazon Workspaces infrastructure. </p>"},{"location":"signals/alarms/","title":"Alarms","text":"<p>An alarm refers to the state of a probe, monitor, or change in a value over or under a given threshold. A simple example would be an alarm that sends an email when a disk is full or a web site is down. More sophisticated alarms are entirely programmatic and used to drive complex interactions such as auto-scaling or creating of entire server clusters. </p> <p>Regardless of the use case though, an alarm indicates the current state of a metric. This state can be <code>OK</code>, <code>WARNING</code>, <code>ALERT</code>, or <code>NO DATA</code>, depending on the system in question. </p> <p>Alarms reflect this state for a period of time and are built on top of a timeseries. As such, they are derived from a time series. This graph below shows two alarms: one with a warning threshold, and another that is indicative of average values across this timeseries. As the volume of traffic in this shows, the alarms for the warning threshold should be in a breach state when it dips below the defined value.</p> <p></p> <p>Info</p> <p>The purpose of an alarm can be either to trigger an action (either human or progammatic), or to be informational (that a threshold is breached). Alarms provide insight into performance of a metric.</p>"},{"location":"signals/alarms/#alert-on-things-that-are-actionable","title":"Alert on things that are actionable","text":"<p>Alarm fatigue is when people get so many alerts that they have learned to ignore them. This is not an indication of a well-monitored system! Rather this is an anti-pattern.</p> <p>Success</p> <p>Create alarms for things that are actionable, and you should always work from your objectives backwards.</p> <p>For example, if you operate a web site that requires fast response times, create an alert to be delivered when your response times are exceeding a given threshold. And if you have identified that poor performance is tied to high CPU utilization then alert on this datapoint proactively before it becomes an issue. However, there may no need to alert on all CPU utilization everywhere in your environment if it does not endanger your outcomes.</p> <p>Success</p> <p>If an alarm does not need alert you, or trigger an automated process, then there is no need to have it alert you. You should remove the notifications from alarms that are superfluous. </p>"},{"location":"signals/alarms/#beware-of-the-everything-is-ok-alarm","title":"Beware of the \"everything is OK alarm\"","text":"<p>Likewise, a common pattern is the \"everything is OK\" alarm, when operators are so used to getting constant alerts that they only notice when things suddenly go silent! This is a very dangerous mode to operate in, and a pattern that works against operational excellence.</p> <p>Warning</p> <p>The \"everything is OK alarm\" usually requries a human to interpret it! This makes patterns like self-healing applications impossible.<sup>1</sup></p>"},{"location":"signals/alarms/#fight-alarm-fatigue-with-aggregation","title":"Fight alarm fatigue with aggregation","text":"<p>Observability is a human problem, not a technology problem. And as such, your alarm strategy should focus on reducing alarms rather than creating more. As you implement  telemetry collection, it is natural to have more alerts from your environment. Be cautious though to only alert on things that are actionable. If the condition that caused the alert is not actionable then there is no need to report on it.</p> <p>This is best shown by example: if you have five web servers that use a single database for their backend, what happens to your web servers if the database is down? The answer for many people is that they get at least six alerts - five for the web servers and one for the database! </p> <p></p> <p>But there are only two alerts that make sense to deliver:</p> <ol> <li>The web site is down, and</li> <li>The database is the cause</li> </ol> <p></p> <p>Success</p> <p>Distilling your alerts into aggregates makes it easier for people to understand, and then easier to create runbooks and automation for.</p>"},{"location":"signals/alarms/#use-your-existing-itsm-and-support-processes","title":"Use your existing ITSM and support processes","text":"<p>Regardless of your monitoring and observability platform, they must integrate into your current toolchain. </p> <p>Success</p> <p>Create trouble tickets and issues using a programmatic integration from your alerts into these tools, removing human effort and streamlining processes along the way. </p> <p>This allows you to derive important operatonal data such as DORA metrics.</p> <ol> <li> <p>See https://aws.amazon.com/blogs/apn/building-self-healing-infrastructure-as-code-with-dynatrace-aws-lambda-and-aws-service-catalog/ for more about this pattern.\u00a0\u21a9</p> </li> </ol>"},{"location":"signals/anomalies/","title":"Anomalies","text":"<p>WIP</p>"},{"location":"signals/events/","title":"Events","text":""},{"location":"signals/events/#what-do-we-mean-by-events","title":"What do we mean by events?","text":"<p>Many architectures are event driven these days. In event driven architectures, events are signals from different systems which we capture and pass onto other systems. An event is typically a change in state, or an update.</p> <p>For example, in an eCommerce system you may have an event when an item is added to the cart. This event could be captured and passed on to the shopping cart part of the system to update the number of items and cost of the cart, along with the item details.</p> <p>Info</p> <p>For some customers an event may be a milestone, such as a the completion of a purchase. There is a case to be made for treating the aggregate moment of a workflow conclusion as an event, but for our purposes we do not consider a milestone itself to be an event.</p>"},{"location":"signals/events/#why-are-events-useful","title":"Why are events useful?","text":"<p>There are two main ways in which events can be useful in your Observability solution. One is to visualize events in the context of other data, and the other is to enable you to take action based on an event. </p> <p>Success</p> <p>Events are intended to give valuable information, either to people or machines, about changes and actions in your workload.</p>"},{"location":"signals/events/#visualizing-events","title":"Visualizing events","text":"<p>There are many event signals which are not directly from your application, but may have an impact on your application performance, or provide additional insight into root cause. Dashboards are the most common mechanism for visualizing your events, though some analytics or business intelligence tools also work in this context. Even email or instant messaging applications can receive visualizations readily.</p> <p>Consider a timechart of application performance, such as time to place an order on your web front end. The time chart lets you see there has been a step change in the response time a few days ago. It might be useful to know if there have been any recent deployments. Consider being able to see a timechart of recent deployments alongside, or superimposed on the same chart?</p> <p></p> <p>Tip</p> <p>Consider which events might be useful to you to understand the wider context. The events that are important to you might be code deployments, infrastructure change events, adding new data (such as publishing new items for sale, or bulk adding new users), or modifying or adding functionality (such as changing the way people add items to their cart).</p> <p>Success</p> <p>Visualize events along with other important metric data so you can correlate events.</p>"},{"location":"signals/events/#taking-action-on-events","title":"Taking action on events","text":"<p>In the Observability world, a triggered alarm is a common event. This event would likely contain an identifier for the alarm, the alarm state (such as <code>IN ALARM</code>, or <code>OK</code>), and details of what triggered this. In many cases this alarm event will be detected and an email notification sent. This is an example of an action on an alarm. </p> <p>Alarm notification is critical in observability. This is how we let the right people know there is an issue. However, when action on events mature in your observability solution, it can automatically remediate the issue without human intervention. </p>"},{"location":"signals/events/#but-what-action-to-take","title":"But what action to take?","text":"<p>We cannot automate action without first understanding what action will ease the detected issue. At the start of your Observability journey, this may often not be obvious. However, the more experience you have remediating issues, the more you can fine tune your alarms to catch areas where there is a known action. There may be built in actions in the alarm service you have, or you may need to capture the alarm event yourself and script the resolution.</p> <p>Info</p> <p>Auto-scaling systems, such as a horizontal pod autoscaling are just an implementation of this principal. Kubernetes simply abstracts this automation for you.</p> <p>Having access to data on alarm frequency and resolution will help you decide if there is a possibility for automation. Whilst wider scope alarms based on issue symptoms are great at capturing issues, you may find you need more specific criteria to link to auto remediation.</p> <p>As you do this, consider integrating this with your incident management/ticketing/ITSM tool. Many organizations track incidents, and associated resolutions and metrics such as Mean Time to Resolve (MTTR). If you do this, consider also capturing your automated resolutions in a similar manner. This lets you understand the type and proportion of issues which are automatically remediated, but also allows you to look for underlying patterns and issues. </p> <p>Tip</p> <p>Just because someone didn't have to manually fix an issue, doesn't mean you shouldn't be looking at the underlying cause. </p> <p>For example, consider a server restart every time it becomes unresponsive. The restart allows the system to continue functioning, but what is causing the unresponsiveness. How often this happens, and if there is a pattern (for example that matches with report generation, or high users, or system backups), will determine the priority and resources you put into understanding and fixing the root cause.</p> <p>Success</p> <p>Consider delivery of every event related to your key performance indicators into a message bus for consumption. And note that some observability solutions do this transparently without explicit configuration requirements.</p>"},{"location":"signals/events/#getting-your-events-into-your-observability-platform","title":"Getting your events into your Observability platform","text":"<p>Once you have identified the events which are important to you, you'll need to consider how best to get them into your Observability platform.  Your platform may have a specific way to capture events, or you may have to bring them in as logs or metric data. </p> <p>Note</p> <p>One simple way to get the information in is to write the events to a log file and ingest them in the same way as you do your other log events.</p> <p>Explore how your system will let you visualize these. Can you identify events which are related to your application? Can you combine data onto a single chart? Even if there is nothing specific, you should at least be able to create a timechart alongside your other data to visually correlate. Keep the time axis the same, and consider stacking these vertically for easy comparison.</p> <p></p>"},{"location":"signals/logs/","title":"Logs","text":"<p>Logs are a series of messages that are sent by an application, or an appliance, that are represented by one or more lines of details about an event, or sometimes about the health of that application. Typically, logs are delivered to a file, though sometimes they are sent to a collector that performs analysis and aggregation. There are many full-featured log aggregators, frameworks, and products that aim to make the task of generating, ingesting, and managing log data at any volume \u2013 from megabytes per day to terabytes per hour.</p> <p>Logs are emitted by a single application at a time and usually pertain to the scope of that one application - though developers are free to have logs be as complex and nuanced as they desire. For our purposes we consider logs to be a fundamentally different signal from traces, which are composed of events from more than one application or a service, and with context about the connection between services such as response latency, service faults, request parameters etc.</p> <p>Data in logs can also be aggregate over a period of time. For example, they may be statistical (e.g. number of requests served over the previous minute). They can be structured, free-form, verbose, and in any written language. </p> <p>The primary use cases for logging are describing,</p> <ul> <li>an event, including its status and duration, and other vital statistics</li> <li>errors or warnings related to that event (e.g. stack traces, timeouts)</li> <li>application launches, start-up and shutdown messages</li> </ul> <p>Note</p> <p>Logs are intended to be immutable, and many log management systems include mechanisms to protect against, and detect attempts, to modify log data. </p> <p>Regardless of your requirements for logging, these are the best practices that we have identified. </p>"},{"location":"signals/logs/#structured-logging-is-key-to-success","title":"Structured logging is key to success","text":"<p>Many systems will emit logs in a semi-structured format. For example, an Apache web server may write logs like this, with each line pertaining to a single web request:</p> <pre><code>192.168.2.20 - - [28/Jul/2006:10:27:10 -0300] \"GET /cgi-bin/try/ HTTP/1.0\" 200 3395\n127.0.0.1 - - [28/Jul/2006:10:22:04 -0300] \"GET / HTTP/1.0\" 200 2216\n</code></pre> <p>Whereas a Java stack trace may be a single event that spans multiple lines and is less structured:</p> <pre><code>Exception in thread \"main\" java.lang.NullPointerException\n    at com.example.myproject.Book.getTitle(Book.java:16)\n    at com.example.myproject.Author.getBookTitles(Author.java:25)\n    at com.example.myproject.Bootstrap.main(Bootstrap.java:14)\n</code></pre> <p>And a Python error log event may look like this:</p> <pre><code>Traceback (most recent call last):\n  File \"e.py\", line 7, in &lt;module&gt;\n    raise TypeError(\"Again !?!\")\nTypeError: Again !?!\n</code></pre> <p>Of these three examples, only the first one is easily parsed by both humans and a log aggregation system. Using structured logs makes it easy to process log data quickly and effectively, giving both humans and machines the data they need to immediately find what they are looking for.</p> <p>The most commonly understood log format is JSON, wherein each component to an event is represented as a key/value pair. In JSON, the python example above may be rewritten to look like this:</p> <pre><code>{\n    \"level\", \"ERROR\"\n    \"file\": \"e.py\",\n    \"line\": 7,\n    \"error\": \"TypeError(\\\"Again !?!\\\")\"\n}\n</code></pre> <p>The use of structured logs makes your data transportable from one log system to another, simplifies development, and make operational diagnosis faster (with less errors). Also, using JSON embeds the schema of the log message along with the actual data, which enables sophisticated log analysis systems to index your messages automatically.</p>"},{"location":"signals/logs/#use-log-levels-appropriately","title":"Use log levels appropriately","text":"<p>There are two types of logs: those that have a level and those that are a series of events. For those that have a level, these are a critical component to a successful logging strategy. Log levels vary slightly from one framework to another, but generally they follow this structure:</p> Level Description <code>DEBUG</code> Fine-grained informational events that are most useful to debug an application. These are usually of value to devlopers and are very verbose. <code>INFO</code> Informational messages that highlight the progress of the application at coarse-grained level. <code>WARN</code> Potentially harmful situations that indicate a risk to an application. These can trigger an alarm in an applicaiton. <code>ERROR</code> Error events that might still allow the application to continue running. These are likely to trigger an alarm that requires attention. <code>FATAL</code> Very severe error events that will presumably cause an application to abort. <p>Info</p> <p>Implicitly logs that have no explicit level may be considered as <code>INFO</code>, though this behaviour may vary between applications.</p> <p>Other common log levels are <code>CRITICAL</code> and <code>NONE</code>, depending on your needs, programming language, and framework. <code>ALL</code> and <code>NONE</code> are also common, though not found in every application stack.</p> <p>Log levels are crucial for informing your monitoring and observability solution about the health of your environment, and log data should easily express this data using a logical value. </p> <p>Tip</p> <p>Logging too much data at <code>WARN</code> will fill your monitoring system with data that is of limited value, and then you may lose important data in the sheer volume of messages.  </p> <p></p> <p>Success</p> <p>Using a standardized log level strategy makes automation easier, and helps developers get to the root cause of issues quickly.</p> <p>Warning</p> <p>Without a standard approach to log levels, filtering your logs is a major challenge.</p>"},{"location":"signals/logs/#filter-logs-close-to-the-source","title":"Filter logs close to the source","text":"<p>Wherever possible, reduce the volume of logs as close to the source as possible. There are many reasons to follow this best practice:</p> <ul> <li>Ingesting logs always costs time, money, and resources.</li> <li>Filtering sensitive data (e.g. personally identifiable data) from downstream systems reduces risk exposure from data leakage.</li> <li>Downstream systems may not have the same operational concerns as the sources of data. For example, <code>INFO</code> logs from an application may be of no interest to a monitoring and alerting system that watches for <code>CRITCAL</code> or <code>FATAL</code> messages.</li> <li>Log systems, and networks, need not be placed under undue stress and traffic.</li> </ul> <p>Success</p> <p>Filter your log close to the source to keep your costs down, decrease risk of data exposure, and focus each component on the things that matter.</p> <p>Tip</p> <p>Depending on your architecture, you may wish to use infrastructure as code (IaC) to deploy changes to your application and environment in one operation. This approach allows you to deploy your log filter patterns along with applications, giving them the same rigor and treatment.</p>"},{"location":"signals/logs/#avoid-double-ingestion-antipatterns","title":"Avoid double-ingestion antipatterns","text":"<p>A common pattern that administrators pursue is copying all of their logging data into a single system with the goal querying all of their logs all from a single location. There are some manual workflow advantages to doing so, however this pattern introduces additional cost, complexity, points of failure, and operational overhead.</p> <p></p> <p>Success</p> <p>Where possible, use a combination of log levels and log filtering to avoid a wholesale propagation of log data from your environments.</p> <p>Info</p> <p>Some organizations or workloads require log shipping in order to meet regulatory requirements, store logs in a secure location, provide non-reputability, or achieve other objectives. This is a common use case for re-ingesting log data. Note that a proper application of log levels and log filtering is still appropriate to reduce the volume of superfluous data entering these log archives.</p>"},{"location":"signals/logs/#collect-metric-data-from-your-logs","title":"Collect metric data from your logs","text":"<p>Your logs contain metrics that are just waiting to be collected! Even ISV solutions or applications that you have not written yourself will emit valuable data into their logs that you can extract meaningful insights into overall workload health from. Common examples include:</p> <ul> <li>Slow query time from databases</li> <li>Uptime from web servers</li> <li>Transaction processing time</li> <li>Counts of <code>ERROR</code> or <code>WARNING</code> events over time</li> <li>Raw count of packages that are available for upgrade</li> </ul> <p>Tip</p> <p>This data is less useful when locked in a static log file. The best practice is to identify key metric data and then publish it into your metric system where it can be correlated with other signals.</p>"},{"location":"signals/logs/#log-to-stdout","title":"Log to <code>stdout</code>","text":"<p>Where possible, applications shouould log to <code>stdout</code> rather than to a fixed location such as a file or socket. This enables log agents to collect and route your log events based on rules that make sense for your own observability solution. While not possible for all applications, this is the best practice for containerized workloads.</p> <p>Note</p> <p>While applications should be generic and simple in their logging practices, remaining loosely coupled from logging solutions, the transmission of log data does still require a log collector to send data from <code>stdout</code> to a file. The important concept is to avoid application and business logic being dependant on your logging infrastructure - in other words, you should work to separate your concerns.</p> <p>Success</p> <p>Decoupling your application from your log management lets you adapt and evolve your solution without code changes, thereby minimizing the potential blast radius of changes made to your environment.</p>"},{"location":"signals/metrics/","title":"Metrics","text":"<p>Metrics are a series of numerical values that are kept in order with the time that they are created. They are used to track everything from the number of servers in your environment, their disk usage, number of requests they handle per second, or the latency in completing these requests.</p> <p>But metrics are not limited to infrastructure or application monitoring. Rather, they can be used for any kind of business or workload to track sales, call queues, and customer satisfaction. In fact, metrics are most useful when combining both operational data and business metrics, giving a well-rounded view and observable system.</p> <p>It might be worth looking into the OpenTelemetry documentation page that provides some additional context on Metrics.</p>"},{"location":"signals/metrics/#know-your-key-performance-indicatorskpis-and-measure-them","title":"Know your Key Performance Indicators(KPIs), and measure them!","text":"<p>The most important thing with metrics is to measure the right things. And what those are will be different for everyone. An e-commerce application may have sales per hour as a critical KPI, whereas a bakery would like be more interested in the number of croissants made per day.</p> <p>Warning</p> <p>There is no singular, entirely complete, and comprehensive source for your business KPIs. You must understand your project or application well enough to know what your output goals are. </p> <p>Your first step is to name your high-level goals, and most likely those goals are not expressed as a single metric that comes from your infrastructure alone. In the e-commerce example above, once you identify the meta goal which is measuring sales per hour, you then can backtrack to detailed metrics such as time spent to search a product before purchase, time taken to complete the checkout process, latency of product search results and so on. This will guide us to be intentional about collecting relevant information to observe the system.</p> <p>Success</p> <p>Having identified your KPIs, you can now work backwards to see what metrics in your workload impact them.</p>"},{"location":"signals/metrics/#correlate-with-operational-metric-data","title":"Correlate with operational metric data","text":"<p>If high CPU utilization on your web server causes slow response times, which in turn makes for dissatisfied customers and ultimately lower revenue, then measuring your CPU utilization has a direct impact on your business outcomes and should absolutely be measured!</p> <p>Or conversely, if you have an application that performs batch processing on ephemeral cloud resources (such as an Amazon EC2 fleet, or similar in other cloud provider environments), then you may want to have CPU as utilized as possible in order to accomplish the most cost-effective means of completing the batch. </p> <p>In either case, you need to have your operational data (e.g. CPU utilization) be in the same system as your business metrics so you can correlate the two. </p> <p>Success</p> <p>Store your business metrics and operational metrics in a system where you can correlate them together and draw conclusions based on observed impacts to both.</p>"},{"location":"signals/metrics/#know-what-good-looks-like","title":"Know what good looks like!","text":"<p>Understanding what a healthy baseline is can be challenging. Many people have to stress test their workloads to understand what healthy metrics look like. However, depending on your needs you may be able to observe existing operational metrics to draw safe conclusions about healthy thresholds.</p> <p>A healthy workload is one that has a balance of meeting your KPI objectives while remaining resilient, available, and cost-effective.</p> <p>Success</p> <p>Your KPIs must have an identified healthy range so you can create alarms when performance falls below, or above, what is required.</p>"},{"location":"signals/metrics/#use-anomaly-detection-algorithms","title":"Use anomaly detection algorithms","text":"<p>The challenge with knowing what good looks like is that it may be impractical to know the healthy thresholds for every metric in your system. A Relational Database Management System(RDBMS) can emit dozens of performance metrics, and when coupled with a microservices architecture you can potentially have hundreds of metrics that can impact your KPIs.</p> <p>Watching such a large number of datapoints and individually identifying their upper and lower thresholds may not always be practical for humans to do. But machine learning is very good at this sort of repetitive task. Leverage automation and machine learning wherever possible as it can help identify issues that you would otherwise not even know about!</p> <p>Success</p> <p>Use machine learning algorithms and anomaly detection models to automatically calculate your workload's performance thresholds. </p>"},{"location":"signals/traces/","title":"Traces","text":"<p>Traces represent an entire journey of the requests as they traverse through different components of an application.</p> <p>Unlike logs or metrics, traces are composed of events from more than one application or a service, and with context about the connection between services such as response latency, service faults, request parameters, and metadata.</p> <p>Tip</p> <p>There is conceptual similarity between logs and traces, however a trace is intended to be considered in a cross-service context, whereas logs are typically limited to the execution of a single service or application.</p> <p>Today's developers are leaning towards building modular and distributed applications. Some call these Service Oriented Architecture, others will refer to them as microservices. Regardless of the name, when something goes wrong in these loosely coupled applications, just looking at logs or events may not be sufficient to track down the root cause of an incident.  Having full visibility into request flow is essential and this is where traces add value. Through a series of causally related events that depict end-to-end request flow, traces help  you gain that visibility.</p> <p>Traces are an essential pillar of observability because they provide the basic information on the flow of the request as it comes and leaves the system.</p> <p>Tip</p> <p>Common use cases for traces include performance profiling, debugging production issues, and root cause analysis of failures.</p>"},{"location":"signals/traces/#instrument-all-of-your-integration-points","title":"Instrument all of your integration points","text":"<p>When all of your workload functionality and code is at one place, it is easy to look at the source code to see how a request is passed across different functions. At a system level you know which machine the app is running and if something goes wrong, you can find the root cause quickly. Imagine doing that in a microservices-based architecture where different components are loosely coupled and are running in an distributed environment. Logging into numerous systems to see their logs from each interconnected request would be impractical, if not impossible.</p> <p>This is where observability can help. Instrumentation is a key step towards increasing that observability. In broader terms Instrumentation is measuring the events in your application using code.</p> <p>A typical instrumentation approach would be to assign a unique trace identifier for each request entering the system and carry that trace id as it passes through different components while adding additional metadata.</p> <p>Success</p> <p>Every connection from one service to another should be instrumented to emit traces to a central collector. This approach helps you see into otherwise opaque aspects of your workload.</p> <p>Success</p> <p>Instrumenting your application can be a largely automated process when using an auto-instrumentation agent or library.</p>"},{"location":"signals/traces/#transaction-time-and-status-matters-so-measure-it","title":"Transaction time and status matters, so measure it!","text":"<p>A well instrumented application can produce end to end trace, which can be viewed aseither a waterfall graph like this:</p> <p></p> <p>Or a service map:</p> <p></p> <p>It is important that you measure the transaction times and response codes to every interaction. This will help in calculating the overall processing times and track it for compliance with your SLAs, SLOs, or business KPIs.</p> <p>Success</p> <p>Only by understanding and recording the response times and status codes of your interactions can you see the contributing factors overall request patterns and workload health.</p>"},{"location":"signals/traces/#metadata-annotations-and-labels-are-your-best-friend","title":"Metadata, annotations, and labels are your best friend","text":"<p>Traces are persisted and assigned a unique ID, with each trace broken down into spans or segments (depending on your tooling) that record each step within the request\u2019s path. A span indicates the entities with which the trace interacts, and, like the parent trace, each span is assigned a unique ID and time stamp and can include additional data and metadata as well. This information is useful for debugging because it gives you the exact time and location a problem occurred.</p> <p>This is best explained through a practical example. An e-commerce application may be divided into many domains: authentication, authorization, shipping, inventory, payment processing, fulfillment, product search, recommendations, and many more. Rather than search through traces from all of these interconnected domains though, labelling your trace with a customer ID allows you to search for only interactions that are specific to this one person. This helps you to narrow your search instantly when diagnosing an operational issue.</p> <p>Success</p> <p>While the naming convention may vary between vendors, each trace can be augmented with metadata, labels, or annotations, and these are searchable across your entire workload. Adding them does require code on your part, but greatly increases the observability of your workload.</p> <p>Warning</p> <p>Traces are not logs, so be frugal with what metadata you include in your traces. And trace data is not intended for forensics and auditing, even with a high sample rate.</p>"},{"location":"tools/adot-traces/","title":"Tracing with ADOT","text":"<p>todo</p>"},{"location":"tools/alarms/","title":"Alarms","text":"<p>Amazon CloudWatch alarms allows you to define thresholds around CloudWatch Metrics and Logs and receive notifications based on the rules configured in the CloudWatch.  </p> <p>Alarms on CloudWatch metrics:</p> <p>CloudWatch alarms allows you to define thresholds on CloudWatch metrics and receive notifications when the metrics fall outside range. Each metric can trigger multiple alarms, and each alarm can have many actions associated with it. There are two different ways you could setup metric alarms based on CloudWatch metrics.</p> <ol> <li> <p>Static threshold: A static threshold represents a hard limit that the metric should not violate. You must define the range for the static threshold like upper limit and the lower limit to understand the behaviour during the normal operations.  If the metric value falls below or above the static threshold you may configure the CloudWatch to generate the alarm.</p> </li> <li> <p>Anomaly detection: Anomaly detection is generally identified as rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well-defined notion of normal behaviour.  CloudWatch anomaly detection analyzes past metric data and creates a model of expected values. The expected values take into account the typical hourly, daily, and weekly patterns in the metric.  You can apply the anomaly detection for each metric as required and CloudWatch applies a machine-learning algorithm to define the upper limit and lower limit for each of the enabled metrics and generate an alarm only when the metrics fall out of the expected values. </p> </li> </ol> <p>Tip</p> <p>Static thresholds are best used for metrics that you have a firm understanding of, such as identified performance breakpoints in your workload, or absolute limits on infrastructure components.</p> <p>Success</p> <p>Use an anomaly detection model with your alarms when you do not have visibility into the performance of a particular metric over time, or when the metric value has not been observed under load-testing or anomalous traffic previously.</p> <p></p> <p>You can follow the instructions below on how to setup of Static and Anomaly based alarms in CloudWatch.</p> <p>Static threshold alarms</p> <p>CloudWatch anomaly Detection based alarms</p> <p>Success</p> <p>To reduce the alarm fatigue or reduce the noise from the number of alarms generated, you have two advanced methods to configure the alarms:</p> <ol> <li> <p>Composite alarms: A composite alarm includes a rule expression that takes into account the alarm states of other alarms that have been created. The composite alarm goes into <code>ALARM</code> state only if all conditions of the rule are met. The alarms specified in a composite alarm's rule expression can include metric alarms and other composite alarms. Composite alarms help to fight alarm fatigue with aggregation.</p> </li> <li> <p>Metric math based alarms: Metric math expressions can be used to build more meaningful KPIs and alarms on them. You can combine multiple metrics and create a combined utilization metric and alarm on them.</p> </li> </ol> <p>These instructions below guide you on how to setup of Composite alarms and Metric math based alarms.</p> <p>Composite Alarms</p> <p>Metric Math alarms</p> <p>Alarms on CloudWatch Logs</p> <p>You can create alarms based on the CloudWatch Logs uses CloudWatch Metric filter. Metric filters turn the log data into numerical CloudWatch metrics that you can graph or set an alarm on. Once you have setup the metrics you could use either the static or anomaly based alarms on the CloudWatch metrics generated from the CloudWatch Logs.</p> <p>You can find an example on how to setup metric filter on CloudWatch logs.</p>"},{"location":"tools/alerting_and_incident_management/","title":"Alerting and incident management","text":""},{"location":"tools/amp/","title":"Amazon Managed Service for Prometheus","text":"<p>Prometheus is a popular open source monitoring tool that provides wide ranging metrics capabilities and insights about resources such as compute nodes and application related performance data. </p> <p>Prometheus uses a pull model to collect data, where as CloudWatch uses a push model. Prometheus and CloudWatch are used for some overlapping use cases, though their operating models are very different and are priced differently.</p> <p>Amazon Managed Service for Prometheus is widely used in containerized applications hosted in Kubernetes and Amazon ECS.</p> <p>You can add Prometheus metric capabilities on your EC2 instance or ECS/EKS cluster using the CloudWatch agent or AWS Distro for OpenTelemetry. The CloudWatch agent with Prometheus support discovers and collects Prometheus metrics to monitor, troubleshoot, and alarm on application performance degradation and failures faster. This also reduces the number of monitoring tools required to improve observability.</p> <p>CloudWatch Container Insights monitoring for Prometheus automates the discovery of Prometheus metrics from containerized systems and workloads https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ ContainerInsights-Prometheus.html</p>"},{"location":"tools/cloudwatch-dashboard/","title":"CloudWatch Dashboard","text":""},{"location":"tools/cloudwatch-dashboard/#introduction","title":"Introduction","text":"<p>Getting to know the inventory details of resources in AWS accounts, the resources performance and health checks is important for a stable resource management. Amazon CloudWatch dashboards are customizable home pages in CloudWatch console that can be used to monitor your resources in a single view, even if those resources are cross-account or spread across different regions.</p> <p>Amazon CloudWatch dashboards enable customers to create reusable graphs and visualize cloud resources and applications in a unified view. Through CloudWatch dashboards customers can graph metrics and logs data side by side in a unified view to quickly get the context and move from diagnosing the problem to understanding the root cause &amp; by reducing the mean time to recover or resolve (MTTR). For example, customers can visualize current utilization of key metrics like CPU utilization &amp; memory and compare them to allocated capacity. Customers can also correlate log pattern of a specific metric and set alarms to alert on performance and operational issues. CloudWatch dashboard also helps customers display the current status of alarms to quickly visualize &amp; get their attention for action. Sharing of CloudWatch dashboards allow customers to easily share displayed dashboard information to teams and or stakeholders who are internal or external to the organizations.</p>"},{"location":"tools/cloudwatch-dashboard/#widgets","title":"Widgets","text":""},{"location":"tools/cloudwatch-dashboard/#default-widgets","title":"Default Widgets","text":"<p>Widgets form the building blocks of CloudWatch dashboards that display important information &amp; near real time details of resources and application metrics and logs in AWS environment. Customers can customize dashboards to their desired experience by adding, removing, rearranging, or resizing widgets according to their requirements.</p> <p>The types of graphs that you can add to your dashboard include Line, Number, Gauge, Stacked area, Bar and Pie.</p> <p>There are default widget types like Line, Number, Gauge, Stacked area, Bar, Pie which are of Graph type and other widgets like Text, Alarm Status, Logs table, Explorer are also available for customers to choose for adding Metrics or Logs data to build dashboards.</p> <p></p> <p>Additional References:</p> <ul> <li>AWS Observability Workshop on Metric Number Widgets</li> <li>AWS Observability Workshop on Text Widgets</li> <li>AWS Observability Workshop on Alarm Widgets</li> <li>Documentation on Creating and working with widgets on CloudWatch dashboards</li> </ul>"},{"location":"tools/cloudwatch-dashboard/#custom-widgets","title":"Custom Widgets","text":"<p>Customers can also choose to add custom widget in CloudWatch dashboards to experience custom visualizations, display information from multiple sources or add custom controls like buttons to take actions directly in a CloudWatch Dashboard. Custom Widgets are completely serverless powered by Lambda functions, enabling complete control over the content, layout and interactions. Custom Widget is an easy way to build custom data view or tool on a dashboard which doesn\u2019t need complicated web framework to learn. If you can write code in Lambda and create HTML then you can create a useful custom widget.</p> <p></p> <p>Additional References:</p> <ul> <li>AWS Observability Workshop on custom widgets</li> <li>CloudWatch Custom Widgets Samples on GitHub</li> <li>Blog: Using Amazon CloudWatch dashboards custom widgets</li> </ul>"},{"location":"tools/cloudwatch-dashboard/#automatic-dashboards","title":"Automatic Dashboards","text":"<p>Automatic Dashboards are available in all AWS public regions which provide an aggregated view of the health and performance of all AWS resources under Amazon CloudWatch. This helps customers quickly get started with monitoring, resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues. Automatic Dashboards are pre-built with AWS service recommended best practices, remain resource aware, and dynamically update to reflect the latest state of important performance metrics. Automatic service dashboards display all the standard CloudWatch metrics for a service, graph all resources used for each service metric and help customers quickly identify outlier resources across accounts that can help identify resources with high or low utilization, which can help optimize costs.</p> <p></p> <p>Additional References:</p> <ul> <li>AWS Observability Workshop on Automatic dashboards</li> <li>Monitor AWS Resources Using Amazon CloudWatch Dashboards on YouTube</li> </ul>"},{"location":"tools/cloudwatch-dashboard/#container-insights-in-automatic-dashboards","title":"Container Insights in Automatic dashboards","text":"<p>CloudWatch Container Insights collect, aggregate, and summarize metrics and logs from containerized applications and microservices. Container Insights is available for Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS), and Kubernetes platforms on Amazon EC2. Container Insights supports collecting metrics from clusters deployed on Fargate for both Amazon ECS and Amazon EKS. CloudWatch automatically collects metrics for many resources, such as CPU, memory, disk, and network &amp; also provides diagnostic information, such as container restart failures, to help isolate issues and resolve them quickly.</p> <p>CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics using embedded metric format, which are performance log events that use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. The metrics that Container Insights collects are available in CloudWatch automatic dashboards, and also viewable in the Metrics section of the CloudWatch console.</p> <p></p>"},{"location":"tools/cloudwatch-dashboard/#lambda-insights-in-automatic-dashboards","title":"Lambda Insights in Automatic dashboards","text":"<p>CloudWatch Lambda Insights is a monitoring and troubleshooting solution for serverless applications such as AWS Lambda, which creates dynamic, automatic dashboards for Lambda functions. It also collects, aggregates and summarizes system-level metrics, including CPU time, memory, disk, and network and diagnostic information such as cold starts and Lambda worker shutdowns to help isolate and quickly resolve issues with Lambda functions. Lambda Insights is a Lambda extension provided as a layer at the function level which when enabled uses embedded metric format to extract metrics from the log events and doesn\u2019t require any agents.</p> <p></p>"},{"location":"tools/cloudwatch-dashboard/#custom-dashboards","title":"Custom Dashboards","text":"<p>Customers can also create Custom Dashboards as many additional dashboards as they want with different widgets and customize it accordingly. Dashboards can be configured for cross-region &amp; cross account view and can be added to a favorites list.</p> <p></p> <p>Customers can add automatic or custom dashboards to the favorite list in the CloudWatch console so that its quick and easy to access them from the navigation pane in the console page.</p> <p>Additional References:</p> <ul> <li>AWS Observability Workshop on CloudWatch dashboard</li> <li>AWS Well-Architected Labs on Performance Efficiency for monitoring with CloudWatch Dashboards</li> </ul>"},{"location":"tools/cloudwatch-dashboard/#adding-contributor-insights-to-cloudwatch-dashboards","title":"Adding Contributor Insights to CloudWatch dashboards","text":"<p>CloudWatch provides Contributor Insights to analyze log data and create time series that display contributor data, where you can see metrics about the top-N contributors, the total number of unique contributors, and their usage. This helps you find top talkers and understand who or what is impacting system performance. For example, customers can find bad hosts, identify the heaviest network users, or find URLs that generate the most errors.</p> <p>Contributor Insights reports can be added to any new or existing Dashboards in CloudWatch console.</p> <p></p>"},{"location":"tools/cloudwatch-dashboard/#adding-application-insights-to-cloudwatch-dashboards","title":"Adding Application Insights to CloudWatch dashboards","text":"<p>CloudWatch Application Insights facilitates observability for applications hosted on AWS and their underlying AWS resources which enhances visibility into the health of applications that it provides helps reduce mean time to repair (MTTR) to troubleshoot application issues. Application Insights provides automated dashboards that show potential problems with monitored applications, which help customers quickly isolate ongoing issues with the applications and infrastructure.</p> <p>The \u2018Export to CloudWatch\u2019 option inside Application Insights as shown below adds a dashboard in CloudWatch console which helps customers easily monitor their critical application for insights.</p> <p></p>"},{"location":"tools/cloudwatch-dashboard/#adding-service-map-to-cloudwatch-dashboards","title":"Adding Service Map to CloudWatch dashboards","text":"<p>CloudWatch ServiceLens enhances the observability of services and applications by integrating traces, metrics, logs, alarms, and other resource health information in one place. ServiceLens integrates CloudWatch with AWS X-Ray to provide an end-to-end view of the application to help customers more efficiently pinpoint performance bottlenecks and identify impacted users. A service map displays service endpoints and resources as nodes and highlights the traffic, latency, and errors for each node and its connections. Each displayed node provide detailed insights about the correlated metrics, logs, and traces associated with that part of the service.</p> <p>\u2018Add to dashboard\u2019 option inside Service Map as shown below adds a new dashboard or to an existing dashboard in CloudWatch console which helps customers easily trace their application for insights.</p> <p></p>"},{"location":"tools/cloudwatch-dashboard/#adding-metrics-explorer-to-cloudwatch-dashboards","title":"Adding Metrics Explorer to CloudWatch dashboards","text":"<p>Metrics explorer in CloudWatch is a tag-based tool that enables customers to filter, aggregate and visualize metrics by tags and resource properties to enhance observability for AWS services. Metrics explorer gives flexible and dynamic troubleshooting experience, so that customers can create multiple graphs at a time and use these graphs to build application health dashboards. Metrics explorer visualizations are dynamic, so if a matching resource is created after you create a metrics explorer widget and add it to a CloudWatch dashboard, the new resource automatically appears in the explorer widget.</p> <p>\u2018Add to dashboard\u2019 option inside Metrics Explorer as shown below adds a new dashboard or to an existing dashboard in CloudWatch console which helps customers easily get more graph insights into their AWS Services and resources.</p> <p></p>"},{"location":"tools/cloudwatch-dashboard/#what-to-visualize-using-cloudwatch-dashboards","title":"What to visualize using CloudWatch dashboards","text":"<p>Customer can create dashboards at account and application-level to monitor workloads and applications across regions and accounts. Customers can quickly get started with CloudWatch automatic dashboards, which are AWS service-level dashboards preconfigured with service-specific metrics. It is recommended to create application and workload-specific dashboards that focus on key metrics and resources that are relevant and critical to the application or workload in your production environment.</p>"},{"location":"tools/cloudwatch-dashboard/#visualizing-metrics-data","title":"Visualizing metrics data","text":"<p>Metrics data can be added to CloudWatch dashboards through Graph widgets like Line, Number, Gauge, Stacked area, Bar, Pie, supported by statistics on metrics through Average, Minimum, Maximum, Sum, and SampleCount. Statistics are metric data aggregations over specified periods of time.</p> <p></p> <p>Metric math enables to query multiple CloudWatch metrics and use math expressions to create new time series based on these metrics. Customers can visualize the resulting time series on the CloudWatch console and add them to dashboards. Customers also perform metric math programmatically using the GetMetricDataAPI operation.</p> <p>Additional Reference:</p> <ul> <li>Monitoring your IoT fleet using CloudWatch</li> </ul>"},{"location":"tools/cloudwatch-dashboard/#visualizing-logs-data","title":"Visualizing logs data","text":"<p>Customers can achieve visualizations of logs data in CloudWatch dashboards using bar charts, line charts, and stacked area charts to more efficiently identify patterns. CloudWatch Logs Insights generates visualizations for queries that use the stats function and one or more aggregation functions that can produce bar charts. If the query uses bin() function to group the data by one field over time, then line charts and stacked area charts can be used for visualization.</p> <p>Time series data can be visualized using the characteristics if the query contains one or more aggregation of status functions or if the query uses the bin() function to group the data by one field.</p> <p>A sample query with count() as stats function is shown below</p> <pre><code>filter @message like /GET/\n| parse @message '_ - - _ \"GET _ HTTP/1.0\" .*.*.*' as ip, timestamp, page, status, responseTime, bytes\n| stats count() as request_count by status\n</code></pre> <p>For the above query, the results are shown below in the CloudWatch Logs Insights.</p> <p></p> <p>Visualization of the query results as a pie chart is shown below.</p> <p></p> <p>Additional Reference:</p> <ul> <li>AWS Observability Workshop on displaying log results in CloudWatch dashboard.</li> <li>Visualize AWS WAF logs with an Amazon CloudWatch dashboard</li> </ul>"},{"location":"tools/cloudwatch-dashboard/#visualizing-alarms","title":"Visualizing alarms","text":"<p>Metric alarm in CloudWatch watches a single metric or the result of a math expression based on CloudWatch metrics. The alarm performs one or more actions based on the value of the metric or expression relative to a threshold over a time period. CloudWatch dashboards can be added with a single alarm in a widget, which displays the graph of the alarm's metric and also displays the alarm status. Also, an alarm status widget can be added to CloudWatch dashboard which displays the status of multiple alarms in a grid. Only the alarm names and current status are displayed, Graphs are not displayed.</p> <p>A sample metric alarm status captured in a alarm widget inside a CloudWatch dashboard is shown below.</p> <p></p>"},{"location":"tools/cloudwatch-dashboard/#cross-account-cross-region","title":"Cross-account &amp; Cross-region","text":"<p>Customers having multiple AWS accounts can set up CloudWatch cross-account observability and then create rich cross-account dashboards in central monitoring accounts, through which they can seamlessly search, visualize, and analyze metrics, logs, and traces without account boundaries.</p> <p>Customers can also create cross-account cross-region dashboards, which summarize CloudWatch data from multiple AWS accounts and multiple regions into a single dashboard. From this high-level dashboard customers can get a unified view of the entire application, and also drill down into more specific dashboards without having to sign in &amp; out of accounts or switch between regions.</p> <p>Additional References:</p> <ul> <li>How to auto add new cross-account Amazon EC2 instances in a central Amazon CloudWatch dashboard</li> <li>Deploy Multi-Account Amazon CloudWatch Dashboards</li> <li>Create Cross Account &amp; Cross Region CloudWatch Dashboards on YouTube</li> </ul>"},{"location":"tools/cloudwatch-dashboard/#sharing-dashboards","title":"Sharing dashboards","text":"<p>CloudWatch dashboards can be shared with people across teams, with stakeholders and with people external to your organization who do not have direct access to your AWS account. These shared dashboards can even be displayed on big screens in team areas, monitoring or network operations centers (NOC) or embed them in Wikis or public webpages.</p> <p>There are three ways to share dashboards to make it easy and secure.</p> <ul> <li>a dashboard can be shared publicly so that anyone having the link can view the dashboard.</li> <li>a dashboard can be shared to specific email addresses of the people who can view the dashboard. Each of these users creates their own password that they enter to view the dashboard.</li> <li>dashboards can be shared within AWS accounts with access through a single sign-on (SSO) provider.</li> </ul> <p>Things to note while sharing dashboards publicly</p> <p>Sharing of CloudWatch dashboards publicly is not recommended if the dashboard contains any sensitive or confidential information. Whenever possible, it is recommended to make use of authentication through username/password or single sign-on (SSO) while sharing dashboards.</p> <p>When dashboards are made publicly accessible, CloudWatch generates a link to a web page which hosts the dashboard. Anyone viewing the web page will also be able to see the contents of the publicly shared dashboard. The web page provides temporary credentials through the link to call APIs to query alarms and contributor insights rules in the Dashboard which you share, and to all metrics and the names and tags of all EC2 instances in your account even if they are not shown in the Dashboard which you share. We recommend you to consider whether it is appropriate to make this information publicly available.</p> <p>Please note that when you enable sharing of dashboards publicly to the web page, the following Amazon Cognito resources will be created in your account: Cognito user pool; Cognito app client; Cognito Identity pool and IAM role.</p> <p>Things to note while sharing dashboards using credentials (Username and password protected dashboard)</p> <p>Sharing of CloudWatch dashboards is not recommended if the dashboard contains any sensitive or confidential information which you would not wish to share with the users with whom you are sharing the dashboard.</p> <p>When dashboards are enabled for sharing, CloudWatch generates a link to a web page which hosts the dashboard. The users that you specified above will be granted the following permissions: CloudWatch read-only permissions to alarms and contributor insights rules in the Dashboard which you share, and to all metrics and the names and tags of all EC2 instances in your account even if they are not shown in the Dashboard which you share. We recommend you to consider whether it is appropriate to make this information available to the users with whom you are sharing.</p> <p>Please note that when you enable sharing of dashboards for users you specify for access to the web page, the following Amazon Cognito resources will be created in your account: Cognito user pool; Cognito users; Cognito app client; Cognito Identity pool and IAM role.</p> <p>Things to note while sharing dashboards using SSO Provider</p> <p>When CloudWatch dashboards are shared using Single Sign-on (SSO), users registered with the selected SSO provider will be granted permissions to access all dashboards in the account where it is shared. Also, when the sharing of dashboards is disabled in this method, all dashboards are automatically unshared.</p> <p>Additional References:</p> <ul> <li>AWS Observability Workshop on sharing dashboards</li> <li>Blog: Share your Amazon CloudWatch Dashboards with anyone using AWS Single Sign-On</li> <li>Blog: Communicate monitoring information by sharing Amazon CloudWatch dashboards</li> </ul>"},{"location":"tools/cloudwatch-dashboard/#live-data","title":"Live data","text":"<p>CloudWatch dashboards also display live data through metric widgets if the metrics from your workloads are constantly published. Customers can choose to enable live data for a whole dashboard, or for individual widgets on a dashboard.</p> <p>If live data is turned off, only data points with an aggregation period of at least one minute in the past are shown. For example, when using 5-minute periods, the data point for 12:35 would be aggregated from 12:35 to 12:40, and displayed at 12:41.</p> <p>If live data is turned on, the most recent data point is shown as soon as any data is published in the corresponding aggregation interval. Each time you refresh the display, the most recent data point may change as new data within that aggregation period is published.</p>"},{"location":"tools/cloudwatch-dashboard/#animated-dashboard","title":"Animated Dashboard","text":"<p>Animated dashboard replays CloudWatch metric data that was captured over time, which helps customers see trends, make presentations, or analyze issues after they occur. Animated widgets in the dashboard include line widgets, stacked area widgets, number widgets, and metrics explorer widgets. Pie graphs, bar charts, text widgets, and logs widgets are displayed in the dashboard but are not animated.</p>"},{"location":"tools/cloudwatch-dashboard/#apicli-support-for-cloudwatch-dashboard","title":"API/CLI support for CloudWatch Dashboard","text":"<p>Apart from accessing CloudWatch dashboard through the AWS Management Console customers can also access the service via API, AWS command-line interface (CLI) and AWS SDKs. CloudWatch API for dashboards help in automating through AWS CLI or integrating with software/products so that you can spend less time managing or administering the resources and applications.</p> <ul> <li>ListDashboards: Returns a list of the dashboards for your account</li> <li>GetDashboard: Displays the details of the dashboard that you specify.</li> <li>DeleteDashboards: Deletes all dashboards that you specify.</li> <li>PutDashboard: Creates a dashboard if it does not already exist, or updates an existing dashboard. If you update a dashboard, the entire contents are replaced with what you specify here.</li> </ul> <p>CloudWatch API Reference for Dashboard Body Structure and Syntax</p> <p>The AWS Command Line Interface (AWS CLI) is an open source tool that enables customers to interact with AWS services using commands in command-line shell, that implement functionality equivalent to that provided by the browser-based AWS Management Console from the command prompt in terminal program.</p> <p>CLI Support:</p> <ul> <li>list-dashboards</li> <li>get-dashboard</li> <li>delete-dashboards</li> <li>put-dashboard</li> </ul> <p>Additional Reference: AWS Observability Workshop on CloudWatch dashboards and AWS CLI</p>"},{"location":"tools/cloudwatch-dashboard/#automation-of-cloudwatch-dashboard","title":"Automation of CloudWatch Dashboard","text":"<p>For automating creation of CloudWatch dashboards, customers can use Infrastructure as a Code (IaaC) tools like CloudFormation or Terraform that help set up AWS resources so that customers can spend less time managing those resources and more time focusing on applications that run in AWS.</p> <p>AWS CloudFormation supports creating dashboards through templates. The AWS::CloudWatch::Dashboard resource specifies an Amazon CloudWatch dashboard.</p> <p>Terraform also has modules which support creating CloudWatch dashboards through IaaC automation.</p> <p>Manually creating dashboards using desired widgets is straight forward. However, it can require some effort to update the resource sources if the content is based on the dynamic information, such as EC2 instances that are created or removed during scale-out and scale-in events in the Auto Scaling group. Please refer to the blog post if you wish to automatically create and update your Amazon CloudWatch dashboards using Amazon EventBridge and AWS Lambda.</p> <p>Additional Reference Blogs:</p> <ul> <li>Automating Amazon CloudWatch dashboard creation for Amazon EBS volume KPIs</li> <li>Automate creation of Amazon CloudWatch alarms and dashboards with AWS Systems Manager and Ansible</li> <li>Deploying an automated Amazon CloudWatch dashboard for AWS Outposts using AWS CDK</li> </ul> <p>Product FAQs on CloudWatch dashboard</p>"},{"location":"tools/cloudwatch_agent/","title":"CloudWatch Agent","text":""},{"location":"tools/cloudwatch_agent/#deploying-the-cloudwatch-agent","title":"Deploying the CloudWatch agent","text":"<p>The CloudWatch agent can be deployed as a single installation, using a distributed configuration file, layering multiple configuration files, and entirely though automation. Which approach is appropriate for you depends on your needs. <sup>1</sup></p> <p>Success</p> <p>Deployment to Windows and Linux hosts both have the capability to store and retrieve their configurations into Systems Manager Parameter Store. Treating the deployment of CloudWatch agent configuration through this automated mechanism is a best practice. </p> <p>Tip</p> <p>Alternatively, the configuration files for the CloudWatch agent can be deployed through the automation tool of your choice (Ansible, Puppet, etc.). The use of Systems Manager Parameter Store is not required, though it does simplify management.</p>"},{"location":"tools/cloudwatch_agent/#deployment-outside-of-aws","title":"Deployment outside of AWS","text":"<p>The use of the CloudWatch agent is not limited to within AWS, and is supported both on-premises and in other cloud environments. There are two additional considerations that must be heeded when using the CloudWatch agent outside of AWS though:</p> <ol> <li>Setting up IAM credentials<sup>2</sup> to allow agent to make required API calls. Even in EC2 there is no unauthenticated access to the CloudWatch APIs<sup>5</sup>.</li> <li>Ensure agent has connectivity to CloudWatch, CloudWatch Logs, and other AWS endpoints<sup>3</sup> using a route that meets your requirements. This can be either through the Internet, using AWS Direct Connect, or through a private endpoint (typically called a VPC endpoint).</li> </ol> <p>Info</p> <p>Transport between your environment(s) and CloudWatch needs to match your governance and security requirements. Broadly speaking, using private endpoints for workloads outside of AWS meets the needs of customers in even the most strictly regulated industries. However, the majority of customers will be served well through our public endpoints.</p>"},{"location":"tools/cloudwatch_agent/#use-of-private-endpoints","title":"Use of private endpoints","text":"<p>In order to push metrics and logs, the CloudWatch agent must have connectivity to the CloudWatch, and CloudWatch Logs endpoints. There are several ways to achieve this based on where the agent is installed.</p>"},{"location":"tools/cloudwatch_agent/#from-a-vpc","title":"From a VPC","text":"<p>a. You can make use of VPC Endpoints (for CloudWatch and CloudWatch Logs) in order to establish fully private and secure connection between your VPC and CloudWatch for the agent running on EC2. With this approach, agent traffic never traverses the internet.</p> <p>b. Another alternative is to have a public NAT gateway through which private subnets can connect to the internet, but cannot receive unsolicited inbound connections from the internet. </p> <p>Note</p> <p>Please note with this approach agent traffic will be logically routed via internet.</p> <p>c. If you don\u2019t have requirement to establish private or secure connectivity beyond the existing TLS and Sigv4 mechanisms, the easiest option is to have Internet Gateway to provide connectivity to our endpoints.</p>"},{"location":"tools/cloudwatch_agent/#from-on-premises-or-other-cloud-environments","title":"From on-premises or other cloud environments","text":"<p>a. Agents running outside of AWS can establish connectivity to CloudWatch public endpoints over the internet(via their own network setup) or Direct Connect Public VIF.</p> <p>b. If you require that agent traffic not route through the internet you can leverage VPC Interface endpoints, powered by AWS PrivateLink, to extend the private connectivity all the way to your on-premises network using Direct Connect Private VIF or VPN. Your traffic is not exposed to the internet, eliminating threat vectors. </p> <p>Success</p> <p>You can add ephemeral AWS access tokens for use by the CloudWatch agent by using credentials obtained from the AWS Systems Manager agent.</p> <ol> <li> <p>See Getting started with open source Amazon CloudWatch Agent for a blog that gives guidance for CloudWatch agent use and deployment.\u00a0\u21a9</p> </li> <li> <p>Guidance on setting credentials for agents running on-premises and in other cloud environments \u21a9</p> </li> <li> <p>How to verify connectivity to the CloudWatch endpoints \u21a9</p> </li> <li> <p>A blog for on-premises, private connectivity \u21a9</p> </li> <li> <p>Use of all AWS APIs related to observability is typically accomplished by an instance profile - a mechanism to grant temporary access credentials to instances and containers running in AWS.\u00a0\u21a9</p> </li> </ol>"},{"location":"tools/dashboards/","title":"Dashboards","text":"<p>Dashboards are an important part of your Observability soluution. They enable you to produce a curated visualization of your data. They enable you see a history of your data, and see it alongside other related data. They also allow you to provide context. They help you understand the bigger picture.</p> <p>Often people gather their data and create alarms, and then stop. However, alarms only show a point in time, and usually for a single metric, or small set of data. Dashboards help you see the behaviour over time.</p> <p></p>"},{"location":"tools/dashboards/#a-practical-example-consider-an-alarm-for-high-cpu","title":"A practical example: consider an alarm for high CPU","text":"<p>You know the machine is running with higher than desired CPU. Do you need to act, and how quickly? What might help you decide?</p> <ul> <li>What does normal CPU look like for this instance/application? </li> <li>Is this a spike, or a trend of increasing CPU? </li> <li>Is it impacting performance? If not, how long before it will does? </li> <li>Is this a regular occurrance? And does it usually recover on its own?</li> </ul>"},{"location":"tools/dashboards/#see-the-history-of-the-data","title":"See the history of the data","text":"<p>Now consider a dashboard, with a historic timechart of the CPU. Even with only this single metric, you can see if this is a spike, or an upward trend. You can also see how quickly it is trending upwards, and so make some decisions on the priority for action.</p>"},{"location":"tools/dashboards/#see-the-impact-on-the-workflow","title":"See the impact on the workflow","text":"<p>But what does this machine do? How important is this in our overall context? Imagine we now add a visualization of the workflow  performance, be it response time, throughput, errors, or some other measure. Now we can see if the high CPU is having an impact on the workflow or users this instance is supporting.</p>"},{"location":"tools/dashboards/#see-the-history-of-the-alarm","title":"See the history of the alarm","text":"<p>Consider adding a visualization which shows how often the alarm has triggered in the last month, and combining that with looking further back to see if this is a regular occurrance. For example, is a backup job triggering the spike? Knowing the pattern of reoccurance can help you understand the underlying issue, and make longer term decisions on how to stop the alarm reoccurring altogether.</p>"},{"location":"tools/dashboards/#add-context","title":"Add context","text":"<p>Finally, add some context to the dashboard. Include a brief description of the reason this dashboard exists, the workflow it relates to, what to do when there is an issue, links to documentation, and who to contact.</p> <p>Info</p> <p>Now we have a story, which helps the dashboard user to see what is happening, understand the impact, and make appropriate data driven decisions on what action and the urgency of it.</p>"},{"location":"tools/dashboards/#dont-try-to-visualize-everything-all-at-once","title":"Don't try to visualize everything all at once","text":"<p>We often talk about alarm fatigue. Too many alarms, without identifiable actions and priorities, can overload your team and lead to inefficiencies. Alarms should be for things which are important to you, and actionable.</p> <p>Dashboards are more flexible here. They don't demand your attention in the same way, so you have more freedom to visualize things that you may not be certain are important yet, or that support your exploration. Still, don't over do it! Everything can suffer from too much of a good thing.</p> <p>Dashboards should provide a picture of something that is important to you. In the same was as deciding what data to ingest, you need to think about what matters to you for dashboards.  For your dashboards, think about</p> <ul> <li>Who will be viewing this?<ul> <li>What is their background and knowledge? </li> <li>How much context do they need? </li> </ul> </li> <li>What questions are they trying to answer?</li> <li>What actions will they be taking as a result of seeing this data?</li> </ul> <p>Tip</p> <p>Sometimes it can be hard to know what your dashboard story should be, and how much to include. So where could you start to design your dashboard? Lets look at two ways: KPI driven, or incident driven.</p>"},{"location":"tools/dashboards/#design-your-dashboard-kpi-driven","title":"Design your dashboard: KPI driven","text":"<p>One way to understand this is to work back from your KPIs. This is usually a very user driven approach. For layout, typically we are working top down, getting to more detail as we move further down a dashboard, or navigate to lower level dashboards. </p> <p>First, understand your KPIs. What they mean. This wil help you decide how you want to visualize these.  Many KPIs are shown as a single number. For example, what percentage of customers are successfully completing a specific workflow, and in what time? But over what time period? You may well meet your KPI if you average over a week, but still have smaller periods of time within this that breach your standards. Are these breaches important to you? Do they impact your customer experience. If so, you may consider different periods and time charts to see your KPIs. And maybe not everyone needs to see the detail, so perhaps you move the breakdown of KPIs to a separate dashboard, for a separate audience.</p> <p>Next, what contribute to those KPIs? What workflows need to be running in order for those actions to happen? Can you measure these?</p> <p>Identify the main components and add visualizations of their performance. When a KPI breeches, you should be able to quickly look and see where in the workflow the main impact is.</p> <p>And you can keep going down - what impacts the perfomance of those workflows? Remember your audience as you decide the level of depth. </p> <p>Consider the example of an e-commerce system with a KPI for the number of order placed. For an order to be placed, users must be able to perform the following action: search for products, add them to their cart, add their delivery details, and pay for the order. For each of these workflows, you might consider checking key components are functioning. For example by using RUM or Synthetics to get data on action success and see if the user is being impacted by an issue. You might consider a measurement of throughput, latency, failed action percentages to see if the performance of each action is as expected. You might consider measurements of the underlying infrastructure to see what might be impacting performance.</p> <p>However, don't put all of your information on the same dashboard. Again, consider your user audience.</p> <p>Success</p> <p>Create layers of dashboards that allow drilldown and provide the right context for the right users.</p>"},{"location":"tools/dashboards/#design-your-dashboard-incident-driven","title":"Design your dashboard: Incident driven","text":"<p>For many people, incident resolution is a key driver for observability. You have been alerted to an issue, by a user, or by an Observability alarm, and you need to quickly find a fix and potentially a root cause of the issue.</p> <p>Success</p> <p>Start by looking at your recent incidents. Are there common patterns? Which were the most impactful for your company? Which ones repeat?</p> <p>In this case, we're designing a dashboard for those trying to understand the severity, identify the root cause and fix the incident.</p> <p>Think back to the specific indcident. </p> <ul> <li>How did you verify the incident was as reported?<ul> <li>What did you check? Endpoints? Errors? </li> </ul> </li> <li>How did you understand the impact, and therefore priority of the issue?</li> <li>What did you look at for cause of the issue?  </li> </ul> <p>Application Performance Monitoring (APM) can help here, with Synthetics for regular baseline and testing of endpoints and workflows, and RUM for the actual customer experience. You can use this data to quickly visualize which workflows are impacted, and by how much.</p> <p>Visualizations which show the error count over time, and the top # errors, can help you to focus on the right area, and show you specific details of errors. This is where we are often using log data, and dynamic visualizations of error codes and reasons.</p> <p>It can be very useful here to have some kind of filtering or drilldown, to get to the specifics as quickly as possible. Think about ways to implement this without too much overhead. For example, having a single dashboard which you can filter to get closer to the details.</p>"},{"location":"tools/dashboards/#layout","title":"Layout","text":"<p>The layout of your dashboard is also important. </p> <p>Success</p> <p>Typically the most significant visualizations for your user want to be top left, or otherwise aligned with a natural beginning of page navigation.</p> <p>You can use layout to help tell the story. For example, you may use a top-down layout, where the further down you scroll, the more details you see. Or perhaps a left-right display would be useful with higher level services on the left, and their dependencies as you move to the right.</p>"},{"location":"tools/dashboards/#create-dynamic-content","title":"Create dynamic content","text":"<p>Many of your workloads will be designed to grow or shrink as demand dictates, and your dashboards need to take this into account. For example you may have your instances in an autoscaling group, and when you hit a certain load, additional instances are added.</p> <p>Success</p> <p>A dashboard showing data from specific instances, specified by some kind of ID, will not allow the data from those new instances to be seen. Add metadata to your resources and data, so you can create your visualizations to capture all instances with a specific metadata value. This way they will reflect the actual state.</p> <p>Another example of dynamic visualizations might be being able to find the top 10 errors occurring now, and how they have behaved over recent history. You want to be able to see a table, or a chart, without knowledge of which errors might occur.</p>"},{"location":"tools/dashboards/#think-about-symptoms-first-over-causes","title":"Think about symptoms first over causes","text":"<p>When you observe symptoms, you are considering about the impact this has on your users and systems. Many underlying causes might give the same symptoms. This enables you to capture more issues, including unknown issues. As you understand causes, your lower level dashboards may be more specific to these to help you quickly diagnose and fix issues.</p> <p>Tip</p> <p>Don't capture the specific JavaScript error that impacted the users last week. Capture the impact on the workflow it disrupted, and then show the top count of JavaScript errors over recent history, or which have dramatically increased in recent history.</p>"},{"location":"tools/dashboards/#use-topbottom-n","title":"Use top/bottom N","text":"<p>Most of the time there is no need to visualize all of your operational metrics at the same time. A large fleet of EC2 instances is a good example of this: there is no need or value in having the disk IOPS or CPU utilization for an entire farm of hundreds of servers displayed simultaneously. This creates an anti-pattern where you can spend more time trying to dig-through your metrics than seeing the best (or worst) performing resources.</p> <p>Success</p> <p>Use your dashboards to show the ten or 20 of any given metric, and then focus on the symptoms this reveals. </p> <p>CloudWatch metrics allows you to search for the top N for any time series. For example, this query will return the busiest 20 EC2 instances by CPU utilization:</p> <pre><code>SORT(SEARCH('{AWS/EC2,InstanceId} MetricName=\"CPUUtilization\"', 'Average', 300), SUM, DESC, 10)\n</code></pre> <p>Use this approach, or similar with CloudWatch Metric Insights to identify the top or bottom performing metrics in your dashboards.</p>"},{"location":"tools/dashboards/#show-kpis-with-thresholds-visually","title":"Show KPIs with thresholds visually","text":"<p>Your KPIs should have a warning or error threshold, and dashboards can show this using a horizontal annotation. This will appear as a high water mark on a widget. Showing this visually can give human operators a forewarning if business outcomes or infrastructure are in jeopardy.</p> <p></p> <p>Success</p> <p>Horizontal annotations are a critical part of a well-developed dashboard.</p>"},{"location":"tools/dashboards/#the-importance-of-context","title":"The importance of context","text":"<p>People can easily misinterpret data. Their background and current context will colour how they view the data.</p> <p>So make sure you include text within your dashboard. What is this data for, and who? What does it mean? Link to documentation on the application, who supports it, the troubleshooting docs. You can also uses text displays to divide your dashboard display. se them on the left to set left-right context. Use them as full horizontal displays to divide your dashboard vertically.</p> <p>Success</p> <p>Having links to IT support, operations on-call, or business owners can give teams a fast path to contact people who can help support when issues occur.</p> <p>Tip</p> <p>Hyperlinks to ticketing systems is also a very useful addition for dashboards.</p>"},{"location":"tools/emf/","title":"Embedded Metric Format","text":"<p>The CloudWatch embedded metric format(EMF) is a JSON specification used to instruct CloudWatch Logs to automatically extract metric values embedded in structured log events. You can use CloudWatch to graph and create alarms on the extracted metric values. With EMF, you can push the metric related data in terms of CloudWatch logs which gets discovered as metric in CloudWatch.</p> <p>Below is a sample EMG for mat encamp and JSON schema :</p> <pre><code>{\n  \"_aws\": {\n    \"Timestamp\": 1574109732004,\n    \"CloudWatchMetrics\": [\n      {\n        \"Namespace\": \"lambda-function-metrics\",\n        \"Dimensions\": [\n          [\n            \"functionVersion\"\n          ]\n        ],\n        \"Metrics\": [\n          {\n            \"Name\": \"time\",\n            \"Unit\": \"Milliseconds\"\n          }\n        ]\n      }\n    ]\n  },\n  \"functionVersion\": \"$LATEST\",\n  \"time\": 100,\n  \"requestId\": \"989ffbf8-9ace-4817-a57c-e4dd734019ee\"\n}\n</code></pre> <p>Thus, with help of EMF you can send high cardinality metrics without the need of making manual PutMetricData API calls.</p>"},{"location":"tools/internet_monitor/","title":"Internet Monitor","text":"<p>Warning</p> <p>As of this writing, Internet Monitor is available in preview in the CloudWatch console. The scope of features for general availability may change from what you experience today.</p> <p>Collecting telemetry from all tiers of your workload is a best practice, and one that can be a challenge. But what are the tiers of your workload? For some it may be web, application, and database servers. Other people might view their workload as front end and back end. And those operating web applications can use Real User Monitoring(RUM) to observe the health of these apps as experienced by end users. </p> <p>But what about the traffic between the client and the datacenter or cloud services provider? And for applications that are not served as web pages and therefore cannot use RUM?</p> <p></p> <p>Internet Monitor works at the networking level and evaluates the health of observed traffic, correlated against AWS existing knowledge of known Internet issues. In short, if there is an Internet Service Provider (ISP) that has a performance or availability issue and if your application has traffic that uses this ISP for client/server communication, then Internet Monitor can proactively inform you about this impact to your workload. Additionally, it can make recommendations to you based on your selected hosting region and use of CloudFront as a Content Delivery Network<sup>1</sup>.</p> <p>Tip</p> <p>Internet Monitor only evaluates traffic from networks that your workloads traverse. For example, if an ISP in another country is impacted, but your users do not use that carrier, then you will not have visibility into that issue.</p>"},{"location":"tools/internet_monitor/#create-monitors-for-applications-that-traverse-the-internet","title":"Create monitors for applications that traverse the Internet","text":"<p>The way that Internet Monitor operates is by watching for traffic that comes either into your CloudFront distributions or to your VPCs from impacted ISPs. This allows you to make decisions about application behaviour, routing, or user notification that helps offset business issues that arise as a result of network problems that are outside of your control.</p> <p></p> <p>Success</p> <p>Only create monitors that watch traffic which traverses the Internet. Private traffic, such as between two hosts in a private network (RFC1918) cannot be monitored using Internet Monitor.</p> <p>Success</p> <p>Prioritize traffic from mobile applications where applicable. Customers roaming between providers, or in remote geographical locations, may have different or unexpected experiences that you should be aware of.</p>"},{"location":"tools/internet_monitor/#enable-actions-through-eventbridge-and-cloudwatch","title":"Enable actions through EventBridge and CloudWatch","text":"<p>Observed issues will be published through EventBridge using a schema that contains the souce identified as <code>aws.internetmonitor</code>. EventBridge can be used to automatically create issues in your ticket management system, page your support teams, or even trigger automation that can alter your workload to mitigate some scenarios.</p> <pre><code>{\n  \"source\": [\"aws.internetmonitor\"]\n}\n</code></pre> <p>Likewise, extensive details of traffic are available in CloudWatch Logs for observed cities, countries, metros, and subdivisions. This allows you to create highly-targeted actions which can notify impacted customers proactively about issues local to them. Here is an example of a country-level observation about a single provider:</p> <pre><code>{\n    \"version\": 1,\n    \"timestamp\": 1669659900,\n    \"clientLocation\": {\n        \"latitude\": 0,\n        \"longitude\": 0,\n        \"country\": \"United States\",\n        \"subdivision\": \"\",\n        \"metro\": \"\",\n        \"city\": \"\",\n        \"countryCode\": \"US\",\n        \"subdivisionCode\": \"\",\n        \"asn\": 00000,\n        \"networkName\": \"MY-AWESOME-ASN\"\n    },\n    \"serviceLocation\": \"us-east-1\",\n    \"percentageOfTotalTraffic\": 0.36,\n    \"bytesIn\": 23,\n    \"bytesOut\": 0,\n    \"clientConnectionCount\": 0,\n    \"internetHealth\": {\n        \"availability\": {\n            \"experienceScore\": 100,\n            \"percentageOfTotalTrafficImpacted\": 0,\n            \"percentageOfClientLocationImpacted\": 0\n        },\n        \"performance\": {\n            \"experienceScore\": 100,\n            \"percentageOfTotalTrafficImpacted\": 0,\n            \"percentageOfClientLocationImpacted\": 0,\n            \"roundTripTime\": {\n                \"p50\": 71,\n                \"p90\": 72,\n                \"p95\": 73\n            }\n        }\n    },\n    \"trafficInsights\": {\n        \"timeToFirstByte\": {\n            \"currentExperience\": {\n                \"serviceName\": \"VPC\",\n                \"serviceLocation\": \"us-east-1\",\n                \"value\": 48\n            },\n            \"ec2\": {\n                \"serviceName\": \"EC2\",\n                \"serviceLocation\": \"us-east-1\",\n                \"value\": 48\n            }\n        }\n    }\n}\n</code></pre> <p>Success</p> <p>Values such as <code>percentageOfTotalTraffic</code> can reveal powerful insights about where your customers access your workloads from and can be used for advanced analytics.</p> <p>Warning</p> <p>Note that log groups created by Internet Monitor will have a default retention period set to never expire. AWS does not delete your data without your consent, so be sure to set a retention period that makes sense for your needs.</p> <p>Success</p> <p>Each monitor will create at least 10 discrete CloudWatch metrics. These should be used for creating alarms just as you would with any other operational metric.</p>"},{"location":"tools/internet_monitor/#utilize-traffic-optimization-suggestions","title":"Utilize traffic optimization suggestions","text":"<p>Internet Monitor features traffic optimization recommendations that can advise you on where to best place your workloads so as to have the best customer experiences. For those workloads that are global, or have global customers, this feature is particularly valuable. </p> <p></p> <p>Success</p> <p>Pay close attention to the current, predicted, and lowest time-to-first-byte (TTFB) values in the traffic optimization suggestions view as these can indicate potentially poor end-user experiences that are otherwise difficult to observe.</p> <ol> <li> <p>See https://aws.amazon.com/blogs/aws/cloudwatch-internet-monitor-end-to-end-visibility-into-internet-performance-for-your-applications/ for our launch blog about this new feature.\u00a0\u21a9</p> </li> </ol>"},{"location":"tools/metrics/","title":"Metrics","text":"<p>Metrics are data about the performance of your system. Having all the metrics related to system or the resources available in a centralised place grants you the ability to compare metrics, analyse performance, and make better strategic decisions like scaling-up or scaling-in resources. Metrics are also important for the knowing the health of the resources and take proactive measures.</p> <p>Metric data is foundational and used to drive alarms, anomaly detection, events, dashboards and more.</p>"},{"location":"tools/metrics/#vended-metrics","title":"Vended metrics","text":"<p>CloudWatch metrics collects data about the performance of your systems. By default, most AWS services provide free metrics for their resources. This includes  Amazon EC2 instances, Amazon RDS, Amazon S3 buckets, and many more. </p> <p>We refer to these metrics as vended metrics. There is no charge for the collection of vended metrics in your AWS account.</p> <p>Info</p> <p>For a complete list of AWS services that emit metrics to CloudWatch see this page.</p>"},{"location":"tools/metrics/#querying-metrics","title":"Querying metrics","text":"<p>You can utilise the metric math feature in CloudWatch to query multiple metrics and use math expressions to analyse the metrics for more granularity. For example, you can write a metric math expression to find out the Lambda error rate by query as:</p> <pre><code>Errors/Requests\n</code></pre> <p>Below you see an example of how this can appear in the CloudWatch console:</p> <p></p> <p>Success</p> <p>Use metric math to get the most value from your data and derive values from the performance of separate data sources.</p> <p>CloudWatch also supports conditional statements. For example, to return a value of <code>1</code> for each timeseries where latency is over a specific threshold, and <code>0</code> for all other data points, a query would resemble this:</p> <pre><code>IF(latency&gt;threshold, 1, 0)\n</code></pre> <p>In the CloudWatch console we can use this logic to create boolean values, which in turn can trigger CloudWatch alarms or other actions. This can enable automatic actions from derived datapoints. An example from the CloudWatch console is below:</p> <p></p> <p>Success</p> <p>Use conditional statements to trigger alarms and notifications when performance exceeds thresholds for derived values. </p> <p>You can also use a <code>SEARCH</code> function to show the top <code>n</code> for any metric. When visualizing the best or worst performing metrics across a large number timeseries (e.g. thousands of servers) this approach allows you to see only the data that matters most. Here is an example of a search returning the top two CPU-consuming EC2 instances, averaged over the last five minutes:</p> <pre><code>SLICE(SORT(SEARCH('{AWS/EC2,InstanceId} MetricName=\"CPUUtilization\"', 'Average', 300), MAX, DESC),0, 2)\n</code></pre> <p>And a view of the same in the CloudWatch console:</p> <p></p> <p>Success</p> <p>Use the <code>SEARCH</code> approach to rapidly display the valuable or worst performing resources in your environment, and then display these in dashboards.</p>"},{"location":"tools/metrics/#collecting-metrics","title":"Collecting metrics","text":"<p>If you would like to have additional metrics like memory or disk space utilization for your EC2 instances, you use the CloudWatch agent to push this data to CloudWatch on your behalf. Or if you have custom processing data which needs to be visualised in graphical manner, and you want this data to be present as CloudWatch metric, then you can use <code>PutMetricData</code> API to publish custom metrics to CloudWatch.</p> <p>Success</p> <p>Use one of the AWS SDKs to push metric data to CloudWatch rather than the bare API.</p> <p><code>PutMetricData</code> API calls are charges on number of queries. The best practise to use the <code>PutMetricData</code> API optimally. Using the Values and Counts method in this API, enables you to publish up to 150 values per metric with one <code>PutMetricData</code> request, and supports retrieving percentile statistics on this data. Thus, instead of making separate API calls for each of the datapoint, you should group all your datapoints together and then push to CloudWatch in a single <code>PutMetricData</code> API call. This approach benefits the user in two ways:</p> <ol> <li>CloudWatch pricing</li> <li><code>PutMetricData</code> API throttling can be prevented</li> </ol> <p>Success</p> <p>When using <code>PutMetricData</code>, the best practice is to batch your data into single <code>PUT</code> operations whenever possible.</p> <p>Success</p> <p>If large volumes of metrics are emitted into CloudWatch then consider using Embedded Metric Format as an alternative approach. Note that Embedded Metric Format does not use, nor charge, for the use of <code>PutMetricData</code>, though it does incur billing from the use of CloudWatch Logs.</p>"},{"location":"tools/metrics/#anomaly-detection","title":"Anomaly detection","text":"<p>CloudWatch has an anomaly detection feature that augments your observability strategy by learning what normal is based on recorded metrics. The use of anomaly detection is a best practice for any metric signal collection system.</p> <p>Anomaly detection builds a model over a two-week period of time. </p> <p>Warning</p> <p>Anomaly detection only builds its model from the time of creation forward. It does not project backwards in time to find previous outliers.</p> <p>Warning</p> <p>Anomaly detection does not know what good is for a metric, only what normal is based on standard deviation.</p> <p>Success</p> <p>The best practice is to train your anomaly detection models to only analyze the times of day that normal behaviour is expected. You can define time periods to exclude from training (such as nights, weekends, or holidays). </p> <p>An example of an anomaly detection band can be seen here, with the band in grey.</p> <p></p> <p>Setting exclusion windows for anomaly detection can be done with the CloudWatch console, CloudFormation, or using one of the AWS SDKs.</p>"},{"location":"tools/observability_accelerator/","title":"AWS Observability Accelerator","text":"<p>The AWS Observability Accelerator is a set of opinionated modules to help you set up observability for your AWS environments with AWS Native services and AWS-managed observability services such as Amazon Managed Service for Prometheus,Amazon Managed Grafana, AWS Distro for OpenTelemetry (ADOT) and Amazon CloudWatch.</p> <p>We provide curated metrics, logs, traces collection, cloudwatch dashboard, alerting rules and Grafana dashboards for your EKS infrastructure, Java/JMX, NGINX based workloads and your custom applications.</p> <p>AWS Observability Accelerator provide shared artifacts (documentation, dashboards, alerting rules) for the Terraform and CDK projects.</p> <p>Checkout the project documentation for Terraform and CDK projects for more information.</p>"},{"location":"tools/rum/","title":"Real User Monitoring","text":"<p>With CloudWatch RUM, you can perform real user monitoring to collect and view client-side data about your web application performance from actual user sessions in near real time. The data that you can visualize and analyze includes page load times, client-side errors, and user behavior. When you view this data, you can see it all aggregated together, and also see breakdowns by the browsers and devices that your customers use.</p> <p></p>"},{"location":"tools/rum/#web-client","title":"Web client","text":"<p>The CloudWatch RUM web client is developed and built using Node.js version 16 or higher. The code is publicly available on GitHub. You can use the client with Angular and React applications.</p> <p>CloudWatch RUM is designed to create no perceptible impact to your application\u2019s load time, performance, and unload time.</p> <p>Note</p> <p>End user data that you collect for CloudWatch RUM is retained for 30 days and then automatically deleted. If you want to keep the RUM events for a longer time, you can choose to have the app monitor send copies of the events to CloudWatch Logs in your account.</p> <p>Tip</p> <p>If avoiding potential interruption by ad blockers is a concern for your web application then you may wish to host the web client on your own content delivery network, or even inside your own web site. Our documentation on GitHub provides guidance on hosting the web client from your own origin domain.</p>"},{"location":"tools/rum/#authorize-your-application","title":"Authorize Your Application","text":"<p>To use CloudWatch RUM, your application must have authorization through one of three options.</p> <ol> <li>Use authentication from an existing identity provider that you have already set up.</li> <li>Use an existing Amazon Cognito identity pool</li> <li>Let CloudWatch RUM create a new Amazon Cognito identity pool for the application</li> </ol> <p>Success</p> <p>Letting CloudWatch RUM create a new Amazon Cognito identity pool for the application requires the least effort to set up. It's the default option.</p> <p>Tip</p> <p>CloudWatch RUM can configured to separate unauthenticated users from authenticated users. See this blog post for details. </p>"},{"location":"tools/rum/#data-protection-privacy","title":"Data Protection &amp; Privacy","text":"<p>The CloudWatch RUM client can use cookies to help collect end user data. This is useful for the user journey feature, but is not required. See our detailed documentation for privacy related information.<sup>1</sup></p> <p>Tip</p> <p>While the collection of web application telemetry using RUM is safe and does not expose personally identifiable information (PII) to you through the console or CloudWatch Logs, be mindful that you can collect custom attribute through the web client. Be careful not to expose sensitive data using this mechanism.</p>"},{"location":"tools/rum/#client-code-snippet","title":"Client Code Snippet","text":"<p>While the code snippet for the CloudWatch RUM web client will be automatically generated, you can also manually modify the code snippet to configure the client to your requirements. </p> <p>Success</p> <p>Use a cookie consent mechanism to dynamically enable cookie creation in singe page applications. See this blog post for more information.</p>"},{"location":"tools/rum/#disable-url-collection","title":"Disable URL Collection","text":"<p>Prevent the collection of resource URLs that might contain personal information.</p> <p>Success</p> <p>If your application uses URLs that contain personally identifiable information (PII), we strongly recommend that you disable the collection of resource URLs by setting <code>recordResourceUrl: false</code> in the code snippet configuration, before inserting it into your application.</p>"},{"location":"tools/rum/#enable-active-tracing","title":"Enable Active Tracing","text":"<p>Enable end-to-end tracing by setting <code>addXRayTraceIdHeader: true</code> in the web client. This causes the CloudWatch RUM web client to add an X-Ray trace header to HTTP requests.</p> <p>If you enable this optional setting, XMLHttpRequest and fetch requests made during user sessions sampled by the app monitor are traced. You can then see traces and segments from these user sessions in the RUM dashboard, the CloudWatch ServiceLens console, and the X-Ray console. </p> <p>Click the checkbox to enable active tracing when setting up your application monitor in the AWS Console to have the setting automatically enabled in your code snippet.</p> <p></p>"},{"location":"tools/rum/#inserting-the-snippet","title":"Inserting the Snippet","text":"<p>Insert the code snippet that you copied or downloaded in the previous section inside the <code>&lt;head&gt;</code> element of your application. Insert it before the <code>&lt;body&gt;</code> element or any other <code>&lt;script&gt;</code> tags.</p> <p>Success</p> <p>If your application has multiple pages, insert the code snippet in a shared header component that is included in all pages.</p> <p>Warning</p> <p>It is critical that the web client be as early in the <code>&lt;head&gt;</code> element as possible! Unlike passive web trackers that are loaded near the bottom of a page's HTML, for RUM to capture the most performance data requires it be instantiated early in the page render process.</p>"},{"location":"tools/rum/#use-custom-metadata","title":"Use Custom Metadata","text":"<p>You can add custom metadata to the CloudWatch RUM events default event metadata. Session attributes are added to all events in a user's session. Page attributes are added only to the pages specified.</p> <p>Success</p> <p>Avoid using reserved keywords noted on this page as key names for your custom attributes</p>"},{"location":"tools/rum/#use-page-groups","title":"Use Page Groups","text":"<p>Success</p> <p>Use page groups to associate different pages in your application with each other so that you can see aggregated analytics for groups of pages. For example, you might want to see the aggregated page load times of all of your pages by type and language.</p> <pre><code>awsRum.recordPageView({ pageId: '/home', pageTags: ['en', 'landing']})\n</code></pre>"},{"location":"tools/rum/#use-extended-metrics","title":"Use Extended Metrics","text":"<p>There is a default set of metrics automatically collected by CloudWatch RUM that are published in the metric namespace named <code>AWS/RUM</code>. These are free, vended metrics that RUM creates on your behalf.</p> <p>Success</p> <p>Send any of the CloudWatch RUM metrics to CloudWatch with additional dimensions so that the metrics give you a more fine-grained view.</p> <p>The following dimensions are supported for extended metrics:</p> <ul> <li>BrowserName</li> <li>CountryCode - ISO-3166 format (two-letter code)</li> <li>DeviceType</li> <li>FileType</li> <li>OSName</li> <li>PageId</li> </ul> <p>However, you can create your own metrics and alarms based on them using our guidance from this page. This approach allows you to monitor performance for any datapoint, URI, or other component that you need.</p> <ol> <li> <p>See our blog post discussing the considerations when using cookies with CloudWatch RUM.\u00a0\u21a9</p> </li> </ol>"},{"location":"tools/slos/","title":"Service Level Objectives (SLOs)","text":"<p>Are highly available and resilient applications an active business driver for your company? If the answer is \u2018yes\u2019, continue reading. </p> <p>Failures are a given and everything will eventually fail over time! This becomes an even more important lesson when you are building applications that need to scale. Here comes the importance of SLOs.</p> <p>SLOs measure an agreed-upon target for service availability based on critical end-user journeys. That agreed-upon target should be crafted around what matters to your customer / end-user. To build such a resilient eco-system, you should measure performance objectively and report reliability accurately using meaningful, realistic, and actionable SLOs. Now, let us get familiarized with key service level terminologies.</p>"},{"location":"tools/slos/#service-level-terminology","title":"Service Level Terminology","text":"<ul> <li> <p>SLI is service level indicator: a carefully defined quantitative measure of some aspect of the level of service that is provided.</p> </li> <li> <p>SLO is service level objective: a target value or range of values for a service level that is measured by an SLI, over a period of time.</p> </li> <li> <p>SLA is service level agreement: an agreement with your customers that includes consequences of missing the SLOs they contain.</p> </li> </ul> <p>The following diagram illustrates that SLA is a \u2018promise/agreement\u2019, SLO is a \u2018goal/target value\u2019, and SLI is a measurement of \u2018how did the service do?\u2019.  </p> <p></p>"},{"location":"tools/slos/#is-there-an-aws-tool-to-monitor-all-of-this","title":"Is there an AWS tool to monitor all of this?","text":"<p>The answer is \u2018yes\u2019! </p> <p>Amazon CloudWatch Application Signals is a new capability that makes it easy to automatically instrument and operate applications on AWS. Application Signals instruments your applications on AWS so that you can monitor the health of your application and track performance against your business objectives. Application Signals provides you with a unified, application-centric view of your applications, services, and dependencies, and helps you monitor and triage application health. Application Signals is supported and tested on Amazon EKS, Amazon ECS, and Amazon EC2 and at the time of writing this, it supports only Java applications!</p> <p>Application Signals helps you set SLOs on your key performance metrics. You can use Application Signals to create service level objectives for the services for your critical business operations. By creating SLOs on these services, you will be able to track them on the SLO dashboard, giving you an at-a-glance view of your most important operations. To speed up root cause identification, Application Signals provides a comprehensive view of application performance, integrating additional performance signals from CloudWatch Synthetics, which monitors critical APIs and user interactions, and CloudWatch RUM, which monitors real user performance.</p> <p>Application Signals automatically collects latency and availability metrics for every service and operation that it discovers, and these metrics are often ideal to use as SLIs. At the same time, Application Signals gives you the flexibility to use any CloudWatch metric or metric expression as an SLI! </p> <p>Application Signals automatically instruments applications based on best practices for application performance and correlates telemetry across metrics, traces, logs, real user monitoring, and synthetic monitoring for applications running on Amazon EKS. Read this blog for more details.</p> <p>Check this blog to learn how to set up an SLO in CloudWatch Application Signals to monitor the reliability of a service. </p> <p>Observability is a foundational element for establishing a reliable service, thereby putting your organization well on its way to operating effectively at scale. We believe, Amazon CloudWatch Application Signals will be an awesome tool to help you achieve that goal.</p>"},{"location":"tools/synthetics/","title":"Synthetic testing","text":"<p>Amazon CloudWatch Synthetics allows you to monitor applications from the perspective of your customer, even in the absence of actual users. By continuously testing your APIs and website experiences, you can gain visibility into intermittent issues that occur even when there is no user traffic.</p> <p>Canaries are configurable scripts, that you can run on a schedule to continually test your APIs and website experiences 24x7. They follow the same code paths and network routes as real-users, and can notify you of unexpected behavior including latency, page load errors, broken or dead links, and broken user workflows.</p> <p></p> <p>Important</p> <p>Ensure that you use Synthetics canaries to monitor only endpoints and APIs where you have ownership or permissions. Depending on the canary frequency settings, these endpoints might experience increased traffic.</p>"},{"location":"tools/synthetics/#getting-started","title":"Getting Started","text":""},{"location":"tools/synthetics/#full-coverage","title":"Full Coverage","text":"<p>Tip</p> <p>When developing your testing strategy, consider both public and private internal endpoints within your Amazon VPC.</p>"},{"location":"tools/synthetics/#recording-new-canaries","title":"Recording New Canaries","text":"<p>The CloudWatch Synthetics Recorder Chrome browser plugin allows you to quickly build new canary test scripts with complex workflows from scratch. The type and click actions taken during recording are converted into a Node.js script that you can use to create a canary. The known limitations of the CloudWatch Synthetics Recorder are noted on this page.</p>"},{"location":"tools/synthetics/#viewing-aggregate-metrics","title":"Viewing Aggregate Metrics","text":"<p>Take advantage of the out-of-the-box reporting on aggregate metrics collected from your fleet of canary scripts. CloudWatch Automatic Dashboard</p> <p></p>"},{"location":"tools/synthetics/#building-canaries","title":"Building Canaries","text":""},{"location":"tools/synthetics/#blueprints","title":"Blueprints","text":"<p>Use canary blueprints to simplify the setup process for multiple canary types.</p> <p></p> <p>Info</p> <p>Blueprints are a convenient way to start writing canaries and simple use cases can be covered with no code.</p>"},{"location":"tools/synthetics/#maintainability","title":"Maintainability","text":"<p>When you write your own canaries, they are tied to a runtime version. This will be a specific version of either Python with Selenium, or JavaScript with Puppeteer. See [this page] for a list of our currently-supported runtime versions and those that are deprecated. </p> <p>Success</p> <p>Improve the maintainability of your scripts by using environment variables to share data that can be accessed during the canary's execution.</p> <p>Success</p> <p>Upgrade your canaries to the latest runtime version when available. </p>"},{"location":"tools/synthetics/#string-secrets","title":"String Secrets","text":"<p>You can code your canaries to pull secrets (such as login credentials) from a secure system outside of your canary or its environment variables. Any system that can be reached by AWS Lambda can potentially provide secrets to your canaries at runtime.</p> <p>Success</p> <p>Execute your tests and secure sensitive data by storing secrets like database connection details, API keys, and application credentials using AWS Secrets Manager.</p>"},{"location":"tools/synthetics/#managing-canaries-at-scale","title":"Managing Canaries at Scale","text":""},{"location":"tools/synthetics/#check-for-broken-links","title":"Check for Broken Links","text":"<p>Success</p> <p>If your website contains a high-volume of dynamic content and links, you can use CloudWatch Synthetics to crawl your website, detect broken links, and find the reason for failure. Then use a failure threshold to optionally create a CloudWatch Alarm when a failure threshold has been violated.</p>"},{"location":"tools/synthetics/#multiple-heartbeat-urls","title":"Multiple Heartbeat URLs","text":"<p>Success</p> <p>Simplify your testing and optimize costs by batching multiple URLs in a single heartbeat monitoring canary test. You can then see the status, duration, associated screenshots, and failure reason for each URL in the step summary of the canary run report.</p>"},{"location":"tools/synthetics/#organize-in-groups","title":"Organize in Groups","text":"<p>Success</p> <p>Organize and track your canaries in groups to view aggregated metrics and more easily isolate and drill in to failures.</p> <p></p> <p>Warning</p> <p>Note that groups will require the exact name of the canary if you are creating a cross-region group.</p>"},{"location":"tools/synthetics/#runtime-options","title":"Runtime Options","text":""},{"location":"tools/synthetics/#versions-and-support","title":"Versions and Support","text":"<p>CloudWatch Synthetics currently supports runtimes that use Node.js for scripts and the Puppeteer framework, and runtimes that use Python for scripting and Selenium WebDriver for the framework.</p> <p>Success</p> <p>Always use the most recent runtime version for your canaries, to be able to use the latest features and updates made to the Synthetics library.</p> <p>CloudWatch Synthetics notifies you by email if you have canaries that use runtimes that are scheduled to be deprecated in the next 60 days.</p>"},{"location":"tools/synthetics/#code-samples","title":"Code Samples","text":"<p>Get started with code samples for both Node.js and Puppeteer and Python and Selenium.</p>"},{"location":"tools/synthetics/#import-for-selenium","title":"Import for Selenium","text":"<p>Create canaries in Python and Selenium from scratch or by importing existing scripts with minimal changes.</p>"},{"location":"tools/xray/","title":"AWS X-Ray","text":""},{"location":"tools/xray/#sampling-rules","title":"Sampling rules","text":"<p>Sampling rules using X-Ray can be configured in the AWS Console, through local configuration file, or both. The local configuration will override those set in the console. </p> <p>Success</p> <p>Use the X-Ray console, API, or CloudFormation whenever possible. This allows you to change the sampling behaviour of an application at runtime.</p> <p>You can set sample rates separately for each of these criteria:</p> <ul> <li>Service name (e.g. billing, payments)</li> <li>Service type (e.g. EC2, Container)</li> <li>HTTP method</li> <li>URL path</li> <li>Resource ARN</li> <li>Host (e.g. www.example.com)</li> </ul> <p>The best practice is to set a sample rate that collects enough data to diagnose issues and understand performance profiles, while not collecting so much data that it is unmanageable. For example, sampling 1% of traffic to a landing page, but 10% of requests to a payment page, would align well with a strong observability practice.</p> <p>Some transactions you may wish to capture 100% of. Be cautious though as traces are not intended for forensic audits of access to your workload!</p> <p>Warning</p> <p>As traces are not intended to be used for auditing or forensic analysis, avoid sample rates of 100%. This can set a false expectation that X-Ray (by default using a UDP emitter) will never lose a transaction trace.</p> <p>As a rule, capturing transaction traces should never create an onerous load on your staff, or your AWS bill. Add traces to your environment slowly while you learn the volume of data that your workload emits.</p> <p>Info</p> <p>By default, the X-Ray SDK records the first request each second, and five percent of any additional requests.</p> <p>Success</p> <p>Always set a reservoir size that you can tolerate. The reservoir size determines the maximum number of requests per second that you will capture. This protects you from malicious attack, unwanted charges, and configuration errors.</p>"},{"location":"tools/xray/#daemon-configuration","title":"Daemon configuration","text":"<p>The X-Ray daemon is intended to offload the effort of sending telemetry to the X-Ray dataplane for analysis. As such, it should not consume too many resources on the server, container, or instance on which the source application runs.</p> <p>Success</p> <p>The best practice is to run the X-Ray daemon on another instance or container, thereby enforcing the separation of concerns and allowing your source system to be unencumbered. </p> <p>Success</p> <p>In a container orchestration pattern, such as Kubernetes, operating your X-Ray daemon as a sidecar is a common practice.</p> <p>The daemon has safe default settings and can operate in EC2, ECS, EKS, or Fargate environments without futher configuration in most instances. For hybrid and other cloud environments though, you may with to adjust the <code>Endpoint</code> to reflect a VPC endpoint if you are using a Direct Connect or VPN to integrate your remote environments.</p> <p>Tip</p> <p>If you must run the X-Ray daemon on the same instance or virtual machine as the source application, consider setting the <code>TotalBufferSizeMB</code> setting to ensure X-ray does not consume more system resources than you can afford.</p>"},{"location":"tools/xray/#annotations","title":"Annotations","text":"<p>AWS X-Ray supports arbitrary metadata to be sent along with your traces. These are called annotations. They are a powerful feature that allows you to group your traces logically. Annotations are indexed as well, making for an easy way to find traces that pertain to a single entity.</p> <p>When you use auto-instrumentation SDKs for X-Ray, annotations may not appear automatically. You need to add them to your code, which greatly enriches your traces and creates ways for you to generate X-Ray Insights, metrics based off of your annotations, alarms and anomaly detection models from your system behaviour, and automate ticketing and remediation when a component impacting your users is observed.</p> <p>Success</p> <p>Use annotations to understand the flow of data in your environment.</p> <p>Success</p> <p>Create alarms based on the performance and results of your annotated traces.</p>"},{"location":"tools/logs/","title":"Logging","text":"<p>The selection of logging tools is tied to your requirements for data transmission, filtering, retention, capture, and integration with the applications that generate your data. When using Amazon Web Services for observability (regardless whether you host on-premises or in another cloud environment), you can leverage the CloudWatch agent or another tool such as Fluentd to emit logging data for analysis.</p> <p>Here we will expand on the best practices for implementing the CloudWatch agent for logging, and the use of CloudWatch Logs within the AWS console or APIs.</p> <p>Info</p> <p>The CloudWatch agent can also be used for delivery of metric data to CloudWatch. See the metrics page for implementation details. It can also be used to collect traces from OpenTelemetry or X-Ray client SDKs, and send them to AWS X-Ray.</p>"},{"location":"tools/logs/#collecting-logs-with-the-cloudwatch-agent","title":"Collecting logs with the CloudWatch agent","text":""},{"location":"tools/logs/#forwarding","title":"Forwarding","text":"<p>When taking a cloud first approach to observability, as a rule, if you need to log into a machine to get its logs, you then have an anti-pattern. Your workloads should emit their logging data outside of their confines in near real time to a log analysis system, and latency between that transmission and the original event represents a potential loss of point-in-time information should a disaster befall your workload.</p> <p>As an architect you will have to determine what your acceptable loss for logging data is and adjust the CloudWatch agent's <code>force_flush_interval</code> to accommodate this.</p> <p>The <code>force_flush_interval</code> instructs the agent to send logging data to the data plane at a regular cadence, unless the buffer size is reached, in which case it will send all buffered logs immediately.</p> <p>Tip</p> <p>Edge devices may have very different requirements from low-latency, in-AWS workloads, and may need to have much longer <code>force_flush_interval</code> settings. For example, an IoT device on a low-bandwidth Internet connection may only need to flush logs every 15 minutes.</p> <p>Success</p> <p>Containerized or stateless workloads may be especially sensitive to log flush requirements. Consider a stateless Kubernetes application or EC2 fleet that can be scaled-in at any moment. Loss of logs may take place when these resources are suddenly terminated, leaving no way to extract logs from them in the future. The standard <code>force_flush_interval</code> is usually appropriate for these scenarios, but can be lowered if required.</p>"},{"location":"tools/logs/#log-groups","title":"Log groups","text":"<p>Within CloudWatch Logs, each collection of logs that logically applies to an application should be delivered to a single log group. Within that log group you want to have commonality among the source systems that create the log streams within.</p> <p>Consider a LAMP stack: the logs from Apache, MySQL, your PHP application, and hosting Linux operating system would each belong to a separate log group.</p> <p>This grouping is vital as it allows you to treat groups with the same retention period, encryption key, metric filters, subscription filters, and Contributor Insights rules.</p> <p>Success</p> <p>There is no limitation on the number of log streams in a log group, and you can search through the entire compliment of logs for your application in a single CloudWatch Logs Insights query. Having a separate log stream for each pod in a Kubernetes service, or for every EC2 instance in your fleet, is a standard pattern.</p> <p>Success</p> <p>The default retention period for a log group is indefinite. The best practice is to set the retention period at the time of creating the log group.</p> <p>While you can set this in the CloudWatch console at any time, the best practice is to do so either in-tandem with the log group creation using infrastructure as code (CloudFormation, Cloud Development Kit, etc.) or using the <code>retention_in_days</code> setting inside of the CloudWatch agent configuration.</p> <p>Either approach lets you set the log retention period proactively, and aligned with your project's data retention requirements.</p> <p>Success</p> <p>Log group data is always encrypted in CloudWatch Logs. By default, CloudWatch Logs uses <code>server-side</code> encryption for the log data at rest. As an alternative, you can use AWS Key Management Service for this encryption. Encryption using AWS KMS is enabled at the log group level, by associating a KMS key with a log group, either when you create the log group or after it exists. This can be configured using infrastructure as code (CloudFormation, Cloud Development Kit, etc.).</p> <p>Using AWS Key Management Service to manage keys for CloudWatch Logs requires additional configuration and granting permissions to the keys for your users.<sup>1</sup></p>"},{"location":"tools/logs/#log-formatting","title":"Log formatting","text":"<p>CloudWatch Logs has the capability to automatically discover log fields and index JSON data upon ingestion. This feature facilitates ad hoc queries and filtering, enhancing the usability of log data. However, it's important to note that automatic indexing is only applicable to structured data. Unstructured logging data won't be automatically indexed but can still be delivered to CloudWatch Logs.</p> <p>Unstructured logs can still be searched or queried using a regular expression with <code>parse</code> command.</p> <p>Success</p> <p>The two best practices for log formats when using CloudWatch Logs:</p> <ol> <li>Use a structured log formatter such as Log4j, <code>python-json-logger</code>, or your framework's native JSON emitter.</li> <li>Send a single line of logging per event to your log destination.</li> </ol> <p>Note that when sending multiple lines of JSON logging, each line will be interpreted as a single event.</p>"},{"location":"tools/logs/#handling-stdout","title":"Handling <code>stdout</code>","text":"<p>As discussed in our log signals page, the best practice is to decouple logging systems from their generating applications. However to send data from <code>stdout</code> to a file is a common pattern for many (if not most) platforms. Container orchestration systems such as Kubernetes or Amazon Elastic Container Service manage this delivery of <code>stdout</code> to a log file automatically, allowing for collection of each log from a collector. The CloudWatch agent then reads this file in real time and forwards the data to a log group on your behalf.</p> <p>Success</p> <p>Use the pattern of simplified application logging to <code>stdout</code>, with collection by an agent, as much as possible.</p>"},{"location":"tools/logs/#filtering-logs","title":"Filtering logs","text":"<p>There are many reasons to filter your logs such as preventing the persistent storage of personal data, or only capturing data that is of a particular log level. In any event, the best practice is to perform this filtering as close to the originating system as possible. In the case of CloudWatch, this will mean before data is delivered into CloudWatch Logs for analysis. The CloudWatch agent can perform this filtering for you.</p> <p>Success</p> <p>Use the <code>filters</code> feature to <code>include</code> log levels that you want and <code>exclude</code> patterns that are known not to be desirable, e.g. credit card numbers, phone numbers, etc.</p> <p>Tip</p> <p>Filtering out certain forms of known data that can potentially leak into your logs can be time-consuming and error prone. However, for workloads that handle specific types of known undesirable data (e.g. credit card numbers, Social Security numbers), having a filter for these records can prevent a potentially damaging compliance issue in the future. For example, dropping all records that contain a Social Security number can be as simple as this configuration:</p> <pre><code>\"filters\": [\n  {\n    \"type\": \"exclude\",\n    \"expression\": \"\\b(?!000|666|9\\d{2})([0-8]\\d{2}|7([0-6]\\d))([-]?|\\s{1})(?!00)\\d\\d\\2(?!0000)\\d{4}\\b\"\n  }\n]\n</code></pre>"},{"location":"tools/logs/#multi-line-logging","title":"Multi-line logging","text":"<p>The best practice for all logging is to use structured logging with a single line emitted for every discrete log event. However, there are many legacy and ISV-supported applications that do not have this option. For these workloads, CloudWatch Logs will interpret each line as a unique event unless they are emitted using a multi-line-aware protocol. The CloudWatch agent can perform this with the <code>multi_line_start_pattern</code> directive.</p> <p>Success</p> <p>Use the <code>multi_line_start_pattern</code> directive to ease the burden of ingesting muli-line logging into CloudWatch Logs.</p>"},{"location":"tools/logs/#configuring-logging-class","title":"Configuring logging class","text":"<p>CloudWatch Logs offers two classes of log groups:</p> <ul> <li> <p>The CloudWatch Logs Standard log class is a full-featured option for logs that require real-time monitoring or logs that you access frequently.</p> </li> <li> <p>The CloudWatch Logs Infrequent Access log class is a new log class that you can use to cost-effectively consolidate your logs. This log class offers a subset of CloudWatch Logs capabilities including managed ingestion, storage, cross-account log analytics, and encryption with a lower ingestion price per GB. The Infrequent Access log class is ideal for ad-hoc querying and after-the-fact forensic analysis on infrequently accessed logs.</p> </li> </ul> <p>Success</p> <p>Use the <code>log_group_class</code> directive to specify which log group class to use for the new log group. Valid values are STANDARD and INFREQUENT_ACCESS. If you omit this field, the default of STANDARD is used by the agent.</p>"},{"location":"tools/logs/#search-with-cloudwatch-logs","title":"Search with CloudWatch Logs","text":""},{"location":"tools/logs/#manage-costs-with-query-scoping","title":"Manage costs with query scoping","text":"<p>With data delivered into CloudWatch Logs, you can now search through it as required. Be aware that CloudWatch Logs charges per gigabyte of data scanned. There are strategies for keeping your query scope under control, which will result in reduced data scanned.</p> <p>Success</p> <p>When searching your logs ensure that your time and date range is appropriate. CloudWatch Logs allows you to set relative or absolute time ranges for scans. If you are only looking for entries from the day before, then there is no need to include scans of logs from today!</p> <p>Success</p> <p>You can search multiple log groups in a single query, but doing so will cause more data to be scanned. When you have identified the log group(s) you need to target, reduce your query scope to match.</p> <p>Tip</p> <p>You can see how much data each query actually scans directly from the CloudWatch console. This approach can help you create queries that are efficient.</p> <p></p>"},{"location":"tools/logs/#share-successful-queries-with-others","title":"Share Successful Queries with Others","text":"<p>While the CloudWatch Logs query syntax is not complex, writing certain queries from scratch can still be time-consuming. Sharing well-written queries with other users within the same AWS account can streamline the investigation of application logs. This can be achieved directly from the AWS Management Console or programmatically using CloudFormation or AWS CDK. Doing so reduces the amount of rework required for others who need to analyze log data.</p> <p>Success</p> <p>Save queries that are often repeated into CloudWatch Logs so they can be prepopulated for your users.</p> <p></p>"},{"location":"tools/logs/#pattern-analysis","title":"Pattern analysis","text":"<p>CloudWatch Logs Insights uses machine learning algorithms to find patterns when you query your logs. A pattern is a shared text structure that recurs among your log fields. Patterns are useful for analyzing large log sets because a large number of log events can often be compressed into a few patterns.<sup>2</sup></p> <p>Success</p> <p>Use pattern to automatically cluster your log data into patterns.</p> <p></p>"},{"location":"tools/logs/#compare-diff-with-previous-time-ranges","title":"Compare (diff) with previous time ranges","text":"<p>CloudWatch Logs Insights enables comparison of log event changes over time, aiding in error detection and trend identification. Comparison queries reveal patterns, facilitating quick trend analysis, with the ability to examine sample raw log events for deeper investigation. Queries are analyzed against two time periods: the selected period and an equal-length comparison period.<sup>3</sup></p> <p>Success</p> <p>Compare changes in your log events over time using <code>diff</code> command.</p> <p></p> <ol> <li> <p>See How to search through your AWS Systems Manager Session Manager console logs \u2013 Part 1 for a practical example of CloudWatch Logs log group encryption with access privileges.\u00a0\u21a9</p> </li> <li> <p>See CloudWatch Logs Insights Pattern Analysis for more detailed insights.\u00a0\u21a9</p> </li> <li> <p>See CloudWatch Logs Insigts Compare(diff) with previous ranges for more information.\u00a0\u21a9</p> </li> </ol>"},{"location":"tools/logs/logs-insights-examples/","title":"CloudWatch Logs Insights Example Queries","text":"<p>CloudWatch Logs Insights provides a powerful platform for analyzing and querying CloudWatch log data. It allows you interactively search through your log data using a SQL like query language with a few simple but powerful commands. </p> <p>CloudWatch Logs insights provides out of the box example queries for the following categories:</p> <ul> <li>Lambda</li> <li>VPC Flow Logs</li> <li>CloudTrail</li> <li>Common Queries</li> <li>Route 53</li> <li>AWS AppSync</li> <li>NAT Gateway</li> </ul> <p>In this section of the best practices guide we provide some example queries for other types of logs that are not currently included in the out of the box examples. This list will evolve and change over time and you can submit your own examples for review by leaving an issue on the git hub.</p>"},{"location":"tools/logs/logs-insights-examples/#api-gateway","title":"API Gateway","text":""},{"location":"tools/logs/logs-insights-examples/#last-20-messages-containing-an-http-method-type","title":"Last 20 Messages containing an HTTP Method Type","text":"<pre><code>filter @message like /$METHOD/ \n| fields @timestamp, @message\n| sort @timestamp desc\n| limit 20\n</code></pre> <p>This query will return the last 20 log messages containing a specific HTTP method sorted in descending timestamp order. Substitute METHOD for the method you are querying for. Here is an example of how to use this query:</p> <pre><code>filter @message like /POST/ \n| fields @timestamp, @message\n| sort @timestamp desc\n| limit 20\n</code></pre> <p>Tip</p> <p>You can change the $limit value in order to return a different amount of messages.</p>"},{"location":"tools/logs/logs-insights-examples/#top-20-talkers-sorted-by-ip","title":"Top 20 Talkers Sorted by IP","text":"<pre><code>fields @timestamp, @message\n| stats count() by ip\n| sort ip asc\n| limit 20\n</code></pre> <p>This query will return the top 20 talkers sorted by IP. This can be useful for detecting malicious activity against your API.</p> <p>As a next step you could then add an additional filter for method type. For example, this query would show the top talkers by IP but only the \"PUT\" method call:</p> <pre><code>fields @timestamp, @message\n| filter @message like /PUT/\n| stats count() by ip\n| sort ip asc\n| limit 20\n</code></pre>"},{"location":"tools/logs/logs-insights-examples/#cloudtrail-logs","title":"CloudTrail Logs","text":""},{"location":"tools/logs/logs-insights-examples/#api-throttling-errors-grouped-by-error-category","title":"API throttling errors grouped by error category","text":"<pre><code>stats count(errorCode) as eventCount by eventSource, eventName, awsRegion, userAgent, errorCode\n| filter errorCode = 'ThrottlingException' \n| sort eventCount desc\n</code></pre> <p>This query allows you to see API throttling errors grouped by category and displayed in descending order.</p> <p>Tip</p> <p>In order to use this query you would first need to ensure you are sending CloudTrail logs to CloudWatch.</p>"},{"location":"tools/logs/logs-insights-examples/#root-account-activity-in-line-graph","title":"Root account activity in line graph","text":"<pre><code>fields @timestamp, @message, userIdentity.type \n| filter userIdentity.type='Root' \n| stats count() as RootActivity by bin(5m)\n</code></pre> <p>With this query you can visualize root account activity in a line graph. This query aggregates the root activity over time, counting the occurrences of root activity within each 5-minute interval.</p> <p>Tip</p> <p>Visualize log data in graphs</p>"},{"location":"tools/logs/logs-insights-examples/#vpc-flow-logs","title":"VPC Flow Logs","text":""},{"location":"tools/logs/logs-insights-examples/#filtering-flow-logs-for-selected-source-ip-address-with-action-as-reject","title":"Filtering flow logs for selected source IP address with action as REJECT.","text":"<pre><code>fields @timestamp, @message, @logStream, @log  | filter srcAddr like '$SOURCEIP' and action = 'REJECT'\n| sort @timestamp desc\n| limit 20\n</code></pre> <p>This query will return the last 20 log messages containing a 'REJECT' from the $SOURCEIP. This can be used to detect if traffic is explicitly rejected, or if the issue is some type of client side network configuration problem.</p> <p>Tip</p> <p>Ensure that you substitute the value of the IP address you are interested in for '$SOURCEIP'</p> <pre><code>fields @timestamp, @message, @logStream, @log  | filter srcAddr like '10.0.0.5' and action = 'REJECT'\n| sort @timestamp desc\n| limit 20\n</code></pre>"},{"location":"tools/logs/logs-insights-examples/#grouping-network-traffic-by-availability-zones","title":"Grouping network traffic by Availability Zones","text":"<pre><code>stats sum(bytes / 1048576) as Traffic_MB by azId as AZ_ID \n| sort Traffic_MB desc\n</code></pre> <p>This query retrieves network traffic data grouped by Availability Zone (AZ). It calculates the total traffic in megabytes (MB) by summing the bytes and converting them to MB. The results are then sorted in descending order based on the traffic volume in each AZ.</p>"},{"location":"tools/logs/logs-insights-examples/#grouping-network-traffic-by-flow-direction","title":"Grouping network traffic by flow direction","text":"<pre><code>stats sum(bytes / 1048576) as Traffic_MB by flowDirection as Flow_Direction \n| sort by Bytes_MB desc\n</code></pre> <p>This query is designed to analyze network traffic grouped by flow direction. (Ingress or Egress) </p>"},{"location":"tools/logs/logs-insights-examples/#top-10-data-transfers-by-source-and-destination-ip-addresses","title":"Top 10 data transfers by source and destination IP addresses","text":"<pre><code>stats sum(bytes / 1048576) as Data_Transferred_MB by srcAddr as Source_IP, dstAddr as Destination_IP \n| sort Data_Transferred_MB desc \n| limit 10\n</code></pre> <p>This query retrieves the top 10 data transfers by source and destination IP addresses. This query allows for identifying the most significant data transfers between specific source and destination IP addresses.</p>"},{"location":"tools/logs/dataprotection/data-protection-policies/","title":"CloudWatch Logs Data Protection Policies for SLG/EDU","text":"<p>Although logging data is beneficial in general, however, masking them is useful for organizations who have strict regulations such as the Health Insurance Portability and Accountability Act (HIPAA), General Data Privacy Regulation (GDPR), Payment Card Industry Data Security Standard (PCI-DSS), and Federal Risk and Authorization Management Program (FedRAMP).</p> <p>Data Protection policies in CloudWatch Logs enables customers to define and apply data protection policies that scan log data-in-transit for sensitive data and mask sensitive data that is detected.</p> <p>These policies leverage pattern matching and machine learning models to detect sensitive data and helps you audit and mask those data that appears in events ingested by CloudWatch log groups in your account.</p> <p>The techniques and criteria used to select sensitive data are referred to as matching data identifiers. Using these managed data identifiers, CloudWatch Logs can detect:</p> <ul> <li>Credentials such as private keys or AWS secret access keys</li> <li>Device identifiers such as IP addresses or MAC addresses</li> <li>Financial information such as bank account number, credit card numbers or credit card verification code</li> <li>Protected Health Information (PHI) such as Health Insurance Card Number (EHIC) or Personal health Number</li> <li>Personally Identifiable Information (PII) such as driver\u2019s licenses, social security numbers or taxpayer identification numbers</li> </ul> <p>Important</p> <p>Sensitive data is detected and masked when it is ingested into the log group. When you set a data protection policy, log events ingested to the log group before that time are not masked.</p> <p>Let us expand on some of the data types mentioned above and see some examples:</p>"},{"location":"tools/logs/dataprotection/data-protection-policies/#data-types","title":"Data Types","text":""},{"location":"tools/logs/dataprotection/data-protection-policies/#credentials","title":"Credentials","text":"<p>Credentials are sensitive data types which are used to verify who you are and whether you have permission to access the resources that you are requesting. AWS uses these credentials like private keys and secret access keys to authenticate and authorize your requests.</p> <p>Using CloudWatch Logs Data Protection policies, sensitive data that matches the data identifiers you have selected is masked. (We will see a masked example at the end of the section).</p> <p></p> <p></p> <p>Tip</p> <p>Data classification best practices start with clearly defined data classification tiers and requirements, which meet your organizational, legal, and compliance standards.</p> <p>As a best practice, use tags on AWS resources based on the data classification framework to implement compliance in accordance with your organization data governance policies. </p> <p>Tip</p> <p>To avoid sensitive data in your log events, best practice is to exclude them in your code in the first place and log only necessary information.</p>"},{"location":"tools/logs/dataprotection/data-protection-policies/#financial-information","title":"Financial Information","text":"<p>As defined by the Payment Card Industry Data Security Standard (PCI DSS), bank account, routing numbers, debit and credit card numbers, credit card magnetic strip data are considered as sensitive financial information.</p> <p>To detect sensitive data, CloudWatch Logs scans for the data identifiers that you specify regardless of the geo-location the log group is located once you set a data protection policy.</p> <p></p> <p>Info</p> <p>Check the full list of financial data types and data identifiers</p>"},{"location":"tools/logs/dataprotection/data-protection-policies/#protected-health-information-phi","title":"Protected Health Information (PHI)","text":"<p>PHI includes a very wide set of personally identifiable health and health-related data, including insurance and billing information, diagnosis data, clinical care data like medical records and data sets and lab results such as images and test results.</p> <p>CloudWatch Logs scan and detect the health information from the chosen log group and mask that data.</p> <p></p> <p>Info</p> <p>Check the full list of phi data types and data identifiers</p>"},{"location":"tools/logs/dataprotection/data-protection-policies/#personally-identifiable-information-pii","title":"Personally Identifiable Information (PII)","text":"<p>PII is a textual reference to personal data that could be used to identify an individual. PII examples include addresses, bank account numbers, and phone numbers.</p> <p></p> <p>Info</p> <p>Check the full list of pii data types and data identifiers</p>"},{"location":"tools/logs/dataprotection/data-protection-policies/#masked-logs","title":"Masked Logs","text":"<p>Now if you go to your log group where you set your data protection policy, you will see that data protection is <code>On</code> and the console also displays a count of sensitive data.</p> <p></p> <p>Now, clicking on <code>View in Log Insights</code> will take you to the Log Insights console. Running the below query to check the logs events in a log stream will give you a list of all the logs.</p> <pre><code>fields @timestamp, @message\n| sort @timestamp desc\n| limit 20\n</code></pre> <p>Once you expand a query, you will see the masked results as shown below:</p> <p></p> <p>Important</p> <p>When you create a data protection policy, then by default, sensitive data that matches the data identifiers you've selected is masked. Only users who have the <code>logs:Unmask</code> IAM permission can view unmasked data.</p> <p>Tip</p> <p>Use AWS IAM and Access Management(IAM) to administer and restrict access to sensitive data in CloudWatch.</p> <p>Tip</p> <p>Regular monitoring and auditing of your cloud environment are equally important in safeguarding sensitive data. It becomes a critical aspect when applications generate a large volume of data and manual and thereby, it is recommended not to log an excessive amount of data. Read this AWS Prescriptive Guidance for Logging Best Practices</p> <p>Tip</p> <p>Log Group Data is always encrypted in CloudWatch Logs. Alternatively, you can also use AWS Key Management Service to encrypt your log data.</p> <p>Tip</p> <p>For resiliency and scalability, set up CloudWatch alarms and automate remediation using AWS Amazon EventBridge and AWS Systems Manager. </p> <ol> <li> <p>Check our AWS blog Protect Sensitive Data with Amazon CloudWatch Logs to get started.\u00a0\u21a9</p> </li> </ol>"}]}